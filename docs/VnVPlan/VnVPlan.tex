\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}

%Includes "References" in the table of contents
\usepackage[nottoc]{tocbibind}
\usepackage[toc,page]{appendix}
\usepackage[square,numbers,compress]{natbib}
\bibliographystyle{abbrvnat}

\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[letterpaper, portrait, margin=1in]{geometry}
\usepackage[dvipsnames]{xcolor}

\input{../Comments}
\input{../Common}

\newcommand{\SRS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/SRS/SRS.pdf}{SRS}}
\newcommand{\MG}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}}
\newcommand{\MIS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \cite{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

The software being tested is called EcoOptimizer. EcoOptimizer is a python refactoring library that focuses on optimizing code in a way that reduces its energy consumption. The system will be capable to analyze python code in order to spot inefficiencies (code smells) within, measuring the energy efficiency of the inputted code and, of course, apply appropriate refactorings that preserve the initial function of the source code. \\

Furthermore, peripheral tools such as a Visual Studio Code (VS Code) extension and GitHub Action are also to be tested. The extension will integrate the library with Visual Studio Code for a more efficient development process and the GitHub Action will allow a proper integration of the library into continuous integration (CI) workflows.

\subsection{Objectives}

The primary objective of this project is to build confidence in the \textbf{correctness} and \textbf{energy efficiency} of the refactoring library, ensuring that it performs as expected in improving code efficiency while maintaining functionality. Usability is also emphasized, particularly in the user interfaces provided through the \textbf{VS Code extension} and \textbf{GitHub Action} integrations, as ease of use is critical for adoption by software developers. These qualities—correctness, energy efficiency, and usability—are central to the project’s success, as they directly impact user experience, performance, and the sustainable benefits of the tool.\\

Certain objectives are intentionally left out-of-scope due to resource constraints. We will not independently verify external libraries or dependencies; instead, we assume they have been validated by their respective development teams. 

\subsection{Challenge Level and Extras}

Our project, set at a \textbf{general} challenge level, includes two additional focuses: \textbf{user documentation} and \textbf{usability testing}. The user documentation aims to provide clear, accessible guidance for developers, making it easy to understand the tool’s setup, functionality, and integration into existing workflows. Usability testing will ensure that the tool is intuitive and meets user needs effectively, offering insights to refine the user interface and optimize interactions with its features.

\subsection{Relevant Documentation}

The Verification and Validation (VnV) plan relies on three key documents to guide testing and assessment: 
\begin{itemize}
  \item[] \textbf{Software Requirements Specification (\SRS)\cite{SRS}:} The foundation for the VnV plan, as it defines the functional and non-functional requirements the software must meet; aligning tests with these requirements ensures that the software performs as expected in terms of correctness, performance, and usability.
  
  \item[] \textbf{Module Interface Specification (\MG)\cite{MGDoc}:} Provides detailed information about each module's interfaces, which is crucial for integration testing to verify that all modules interact correctly within the system.
  
  \item[] \textbf{Module Guide (\MIS)\cite{MISDoc}:} Outlines the system's architectural design and module structure, ensuring the design of tests that align with the intended flow and dependencies within the system.
\end{itemize}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification Plan}

\textbf{Function \& Non-Functional Requirements:}
\begin{itemize}
    \item A comprehensive test suite that covers all requirements specified in the SRS will be created.
    \item Each requirement will be mapped to specific test cases to ensure maximum coverage.
    \item Automated and manual testing will be conducted to verify that the implemented system meets each functional requirement.
    \item Usability testing with representative users will be carried out to validate user experience requirements and other non-functional requirements.
    \item Performance tests will be conducted to verify that the system meets specified performance requirements.
\end{itemize}

\textbf{Traceability Matrix:}
\begin{itemize}
    \item We will create a requirements traceability matrix that links each SRS requirement to its corresponding implementation, test cases, and test results.
    \item This matrix will help identify any requirements that may have been overlooked during development.
\end{itemize}

\textbf{Supervisor Review:}
\begin{itemize}
    \item After the implementation of the system, we will conduct a formal review session with key stakeholders such as our project supervisor, Dr. Istvan David.
    \item The stakeholders will be asked to verify that each requirement in the SRS is mapped out to specific expectations of the project. 
    \item Prior to meeting, we will provide a summary of key requirements and design decisions and prepare a list specific questions or areas where we seek guidance.
    \item During the meeting, we will present an overview of the SRS using tables and other visual aids. We will conduct a walk through of critical section. Finally, we will discuss any potential risks or challenges identified.
\end{itemize}

\textbf{User Acceptance Testing (UAT):}
\begin{itemize}
    \item We will involve potential end-users in testing the system to ensure it meets real-world usage scenarios.
    \item Feedback from UAT will be used to identify any discrepancies between the SRS and user expectations.
\end{itemize}

\textbf{Continuous Verification:}
\begin{itemize}
    \item Throughout the development process, we will regularly review and update the SRS to ensure it remains aligned with the evolving system.
    \item Any changes to requirements will be documented and their impact on the system assessed.
\end{itemize}

\textbf{\textit{\\Checklist for SRS Verification Plan}}
\begin{itemize}
    \item[$\square$] Create comprehensive test suite covering all SRS requirements
    \item[$\square$] Map each requirement to specific test cases
    \item[$\square$] Conduct automated testing for functional requirements
    \item[$\square$] Perform manual testing for functional requirements
    \item[$\square$] Carry out usability testing with representative users
    \item[$\square$] Conduct performance tests to verify system meets requirements
    \item[$\square$] Create requirements traceability matrix
    \item[$\square$] Link each SRS requirement to implementation in traceability matrix
    \item[$\square$] Link each SRS requirement to test cases in traceability matrix
    \item[$\square$] Link each SRS requirement to test results in traceability matrix
    \item[$\square$] Schedule formal review session with project supervisor
    \item[$\square$] Prepare summary of key requirements and design decisions for supervisor review
    \item[$\square$] Prepare list of specific questions for supervisor review
    \item[$\square$] Create visual aids for SRS overview presentation
    \item[$\square$] Conduct walkthrough of critical SRS sections during review
    \item[$\square$] Discuss potential risks and challenges with supervisor
    \item[$\square$] Organize User Acceptance Testing (UAT) with potential end-users
    \item[$\square$] Collect and analyze UAT feedback
    \item[$\square$] Identify discrepancies between SRS and user expectations from UAT
    \item[$\square$] Establish process for regular SRS review and updates
    \item[$\square$] Document any changes to requirements
    \item[$\square$] Assess impact of requirement changes on the system
\end{itemize}

\subsection{Design Verification Plan}

\textbf{Peer Review Plan:}
\begin{itemize}
    \item Each team member along with other classmates will thoroughly review the entire Design Document.
    \item A checklist-based approach will be used to ensure all key elements are covered.
    \item Feedback will be collected and discussed in a dedicated team meeting.
\end{itemize}

\textbf{Supervisor Review:}
\begin{itemize}
    \item A structured review meeting will be scheduled with our project supervisor, Dr. Istvan David.
    \item We will present an overview of the design using visual aids (e.g., diagrams, tables).
    \item We will conduct a walkthrough of critical sections.
    \item We will use our project's issue tracker to document and follow up on any action items or changes resulting from this review.
\end{itemize}

\begin{itemize}
  \item[$\square$] All functional requirements are mapped to specific design elements 
  \item[$\square$] Each functional requirement is fully addressed by the design 
  \item[$\square$] No functional requirements are overlooked or partially implemented 
  \item[$\square$] Performance requirements are met by the design 
  \item[$\square$] Scalability considerations are incorporated 
  \item[$\square$] Reliability and availability requirements are satisfied 
  \item[$\square$] Usability requirements are reflected in the user interface design
  \item[$\square$] High-level architecture is clearly defined 
  \item[$\square$] Architectural decisions are justified with rationale 
  \item[$\square$] Architecture aligns with project constraints and goals 
  \item[$\square$] All major components are identified and described 
  \item[$\square$] Interactions between components are clearly specified 
  \item[$\square$] Component responsibilities are well-defined 
  \item[$\square$] Appropriate data structures are chosen for each task 
  \item[$\square$] Efficient algorithms are selected for critical operations 
  \item[$\square$] Rationale for data structure and algorithm choices is provided
  \item[$\square$] UI design is consistent with usability requirements 
  \item[$\square$] User flow is logical and efficient 
  \item[$\square$] Accessibility considerations are incorporated 
  \item[$\square$] All external interfaces are properly specified 
  \item[$\square$] Interface protocols and data formats are defined 
  \item[$\square$] Error handling for external interfaces is addressed 
  \item[$\square$] Comprehensive error handling strategy is in place
  \item[$\square$] Exception scenarios are identified and managed 
  \item[$\square$] Error messages are clear and actionable 
  \item[$\square$] Authentication and authorization mechanisms are described 
  \item[$\square$] Data encryption methods are specified where necessary 
  \item[$\square$] Security best practices are followed in the design
  \item[$\square$] Design allows for future expansion and feature additions 
  \item[$\square$] Code modularity and reusability are considered 
  \item[$\square$] Documentation standards are established for maintainability 
  \item[$\square$] Performance bottlenecks are identified and addressed 
  \item[$\square$] Resource utilization is optimized 
  \item[$\square$] Performance testing strategies are outlined 
  \item[$\square$] Design adheres to established coding standards 
  \item[$\square$] Industry best practices are followed 
  \item[$\square$] Design patterns are appropriately applied
  \item[$\square$] All major design decisions are justified 
  \item[$\square$] Trade-offs are explained with pros and cons 
  \item[$\square$] Alternative approaches considered are documented 
  \item[$\square$] Documents is clear, concise, and free of ambiguities 
  \item[$\square$] Documents follows a logical structure 
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

The section will cover system tests for the non-functional requirements (NFR) listed in the \SRS \hspace{1pt} document\cite{SRS}. The goal for these tests is to address the fit criteria for the requirements. Each test will be linked back to a specific NFR that can be observed in section \ref{trace-sys}.

\noindent
\rule{\linewidth}{2pt}

\subsubsection{Look and Feel}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Look and Feel requirements listed in the SRS \cite{SRS}. They seek to validate that the system is modern, visually appealing, and supporting of a calm and focused user experience. 
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-LF-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Side-by-side code comparison in IDE plugin} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open in VS Code, with a sample code file loaded \\
    \textbf{Input/Condition:} The user initiates a refactoring operation \\
    \textbf{Output/Result:} The plugin displays the original and refactored code side by side\\[2mm]
    \textbf{How test will be performed:} The tester will open a sample code file within the IDE plugin and apply a refactoring operation. After refactoring, they will verify that the original code appears on one side of the interface and the refactored code on the other, with clear options to accept or reject each change. The tester will interact with the accept/reject buttons to ensure functionality and usability, confirming that users can seamlessly make refactoring decisions with both versions displayed side by side.

  \item \textbf{Theme adaptation in VS Code} \\[2mm]
    \textbf{Type:} Non-functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open in VS Code with either light or dark theme enabled \\
    \textbf{Input/Condition:} The user switches between light and dark themes in VS Code \\
    \textbf{Output/Result:} The plugin’s interface adjusts automatically to match the theme \\[2mm]
    \textbf{How test will be performed:} The tester will open the plugin in both light and dark themes within VS Code by toggling the theme settings in the IDE. They will observe the plugin interface each time the theme is switched, ensuring that the plugin automatically adjusts to match the selected theme without any manual adjustments required. 

  \item \textbf{Colour-coded refactoring indicators for energy savings} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with sample code loaded \\
    \textbf{Input/Condition:} The plugin displays refactoring suggestions based on energy savings \\
    \textbf{Output/Result:} Refactoring suggestions are colour-coded according to energy-saving potential (e.g., yellow for minor savings, red for major savings) \\[2mm]
    \textbf{How test will be performed:} The tester will load sample code with multiple refactoring suggestions based on energy-saving potential and activate the plugin’s analysis feature. The tester will then review each suggestion, confirming that they are visually differentiated by colour codes (e.g., yellow for minor savings and red for major savings). They will interact with each coloured indicator to ensure that it is responsive and accurately represents the suggested energy savings levels.

  \item \textbf{Visual alerts in GitHub Action for significant energy savings} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} GitHub Action enabled for a pull request (PR) \\
    \textbf{Input/Condition:} PR analysis indicates energy savings exceeding a predefined threshold. \\
    \textbf{Output/Result:} A success icon or green label appears in the PR summary \\[2mm]
    \textbf{How test will be performed:} The tester will set up a pull request (PR) with changes that yield significant energy savings. When the GitHub Action completes the analysis, the tester will check the PR summary to confirm that a green label or success icon appears, indicating substantial energy savings. The tester will repeat the test with a PR that does not meet the threshold to ensure no alert is displayed. 

  \item \textbf{Minimalist design of the refactoring interface} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give an answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} After a testing session, developers fill out the survey found in \ref{A.2} evaluating their experience with the plugin.

  \item \textbf{Professional and authoritative appearance of the plugin} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give an answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} After a testing session, developers fill out the survey found in \ref{A.2} evaluating the professionalism and trustworthiness of the plugin interface.

  \item \textbf{Calm and focused atmosphere of plugin interface} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give an answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} Following a testing session, developers complete the survey found in \ref{A.2} evaluating the plugin’s atmosphere and its impact on focus.

  \item \textbf{Modern, visually appealing design} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give a answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} After initial interaction, developers complete the survey found in \ref{A.2} on the tool’s design aesthetics.
\end{enumerate}

\noindent
\rule{\linewidth}{2pt}
    
\subsubsection{Usability \& Humanity}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Usability \& Humanity requirements listed in the SRS \cite{SRS}. They seek to validate that the system is accessible, user-centred, intuitive and easy to navigate.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-UH-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Customizable settings for refactoring preferences} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with settings panel accessible \\
    \textbf{Input/Condition:} User customizes refactoring style and detection sensitivity \\
    \textbf{Output/Result:} Custom configurations save and load successfully \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the settings menu within the tool and adjust various options, including refactoring style, colour-coded indicators, and unit preferences (metric vs. imperial). After each adjustment, the tester will observe if the interface and refactoring suggestions reflect the changes made. 

  \item \textbf{Multilingual support in user guide} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} Bilingual user navigates to system documentation \\
    \textbf{Input/Condition:} User accesses guide in both English and French \\
    \textbf{Output/Result:} The guide is accessible in both languages \\[2mm]
    \textbf{How test will be performed:} The tester will set the tool’s language to French and access the user guide, reviewing each section to ensure accurate translation and readability. After verifying the French version, they will switch the language to English, confirming consistency in content, layout, and clarity between both versions.

  \item \textbf{YouTube installation tutorial availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} User access documentation resources \\
    \textbf{Input/Condition:} User follows the provided link to a YouTube tutorial \\
    \textbf{Output/Result:} Installation tutorial is available and accessible on YouTube, and user successfully installs the system. \\[2mm]
    \textbf{How test will be performed:} The tester will start with the installation instructions provided in the user guide and follow the link to the YouTube installation tutorial. They will watch the video and proceed with each installation step as demonstrated. Throughout the process, the tester will note the clarity and pacing of the instructions, any gaps between the video and the actual steps, and if the video effectively guides them to a successful installation. 

  \item \textbf{High-Contrast Theme Accessibility Check} \\[2mm]
    \textbf{Objective:} Evaluate the high-contrast themes in the refactoring tool for compliance with accessibility standards to ensure usability for visually impaired users. \\
    \textbf{Scope:} Focus on UI components that utilize high-contrast themes, including text, buttons, and backgrounds. \\
    \textbf{Methodology:} Static Analysis \\
    \textbf{Process:} 
    \begin{itemize}
      \item Identify all colour codes used in the system and categorize them by their role in the UI (i.e. background, foreground text, buttons, etc.).
      \item Use tools to measure color contrast ratios against WCAG thresholds (4.5:1 for normal text, 3:1 for large text)\cite{WCAG}.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Developers implement themes that pass the testing process. \\[2mm]
    \textbf{Tools and Resources:} WebAIM Color Contrast Checker, WCAG guidelines documentation, internal coding standards. \\[2mm]
    \textbf{Acceptance Criteria:} All UI elements must meet WCAG contrast ratios; documentation must accurately reflect theme usage.

  \item \textbf{Audio cues for important actions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with audio cues enabled \\
    \textbf{Input/Condition:} User performs actions triggering audio cues \\
    \textbf{Output/Result:} The system emits an audible attention catching sound. \\[2mm]
    \textbf{How test will be performed:} The tester will enable audio cues in the tool's settings, then perform a series of tasks, such as running code analysis, applying refactorings, and saving changes. Each action should trigger an audio cue indicating task completion or user feedback. The tester will evaluate the volume, timing, and appropriateness of each cue and document whether the cues enhance the user experience or cause any distractions. 

  \item \textbf{Intuitive user interface for core functionality} \\[2mm]
    \textbf{Type:} Non-Functional, User Testing, Dynamic \\
    \textbf{Initial State:} IDE plugin open with code loaded \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} Users can access core functions within three clicks or less \\[2mm]
    \textbf{How test will be performed:} After a testing session, developers fill out the survey found in \ref{A.2} evaluating their experience with the plugin.

  \item \textbf{Clear and concise user prompts} \\[2mm]
    \textbf{Type:} Non-Functional, User Survey, Dynamic \\
    \textbf{Initial State:} IDE plugin prompts user for input \\
    \textbf{Input/Condition:} Users follow on-screen instructions \\
    \textbf{Output/Result:} 90\% of users report the prompts are straightforward and effective \\[2mm]
    \textbf{How test will be performed:} Users complete tasks requiring prompts and answer the survey found in \ref{A.2} on the clarity of guidance provided.

  \item \textbf{Context-sensitive help based on user actions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with help function enabled \\
    \textbf{Input/Condition:} User engages in various actions, requiring guidance \\
    \textbf{Output/Result:} Help resources are accessible within 1-3 clicks \\[2mm]
    \textbf{How test will be performed:} The tester will perform a series of tasks within the tool, such as initiating a code analysis, applying a refactoring, and adjusting settings. At each step, they will access the context-sensitive help option to confirm that the information provided is relevant to the current task. The tester will evaluate the ease of accessing help, the relevance and clarity of guidance, and whether the help content effectively supports task completion.

  \item \textbf{Clear and constructive error messaging} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with possible error scenarios triggered \\
    \textbf{Input/Condition:} User encounters an error during use \\
    \textbf{Output/Result:} 80\% of users report that error messages are helpful and courteous \\[2mm]
    \textbf{How test will be performed:} After receiving error messages, users fill out the survey found in \ref{A.2} on their clarity and constructiveness.

  % \item \textbf{Usability and Design Acceptance} \\
  %   \textbf{Type:} Non-Functional, Manual, Dynamic \\
  %   \textbf{Initial State:} User’s IDE environment is prepared without the refactoring tool installed \\
  %   \textbf{Input/Condition:} User installs the refactoring tool, explores its key features, and completes a survey to provide feedback on usability, intuitiveness, and design quality \\
  %   \textbf{Output/Result:} User installs the tool successfully, completes each feature exploration task without issues, and fills out a survey on the tool’s usability and design \\
  %   \textbf{How test will be performed:} The tester will start from a clean slate and perform the following steps:
  %   \begin{itemize}
  %       \item \textbf{Installation:} The tester will install the refactoring tool from the PIP package manager by running \texttt{pip install ecooptimizer}. After installation, they will open the tool within the IDE, authenticate as prompted, and confirm successful setup by verifying the tool’s presence in the IDE interface.
  %       \item \textbf{Code Analysis:} The tester will load a sample code file and initiate the code analysis feature. They will review the output, focusing on the clarity of refactoring suggestions and ease of accessing this feature within the interface.
  %       \item \textbf{Refactoring Options and Side-by-Side Comparison:} The tester will apply a refactoring operation and evaluate the side-by-side comparison of original and refactored code. They will interact with the accept/reject buttons, assessing the ease of making refactoring decisions while viewing both code versions.
  %       \item \textbf{Settings and Customisation:} The tester will navigate to the settings menu to explore customisation options, such as refactoring styles, colour-coded indicators, and measurement unit toggles between metric and imperial. They will adjust various settings and evaluate the intuitiveness of the settings menu and customisation process.
  %       \item \textbf{Help and Error Handling:} The tester will simulate a common error (e.g., loading an incompatible file type) to trigger an error message, reviewing the clarity and guidance provided. They will also explore help resources, assessing their relevance, accessibility, and ease of use.
  %       \item \textbf{Metrics Reporting and Export Functionality:} The tester will explore the metrics reporting feature to view data on energy savings or emissions and attempt to export this data in formats such as JSON and XML. They will assess the readability of metrics, usability of export options, and format accuracy.
  %   \end{itemize}
  %   After completing the walkthrough, the tester will fill out a survey to provide structured feedback on each feature, including the tool’s minimalist design, professional appearance, intuitive UI, clarity of prompts, error handling, and overall usability. This feedback will be used to identify areas for improvement in user experience and interface quality.
\end{enumerate}

\noindent
\rule{\linewidth}{2pt}

\subsubsection{Performance}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Performance requirements listed in the SRS \cite{SRS}. These tests validate the tool’s efficiency and responsiveness under varying workloads, including code analysis, refactoring, and data reporting.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-PF-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Performance and capacity validation for analysis and refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} IDE open with multiple python projects of varying sizes ready (1,000, 5,000, 10,000, 100,000 lines of code). \\
    \textbf{Input/Condition:} Initiate the refactoring process for each project sequentially \\
    \textbf{Output/Result:} Process completes within 15 seconds for projects up to 5,000 lines of code, 20 seconds for 10,000 lines of code and within 2 minutes for 100,000 lines of code. \\[2mm]
    \textbf{How test will be performed:} The tester will use four python projects of different sizes: small (1,000 lines), medium (5,000 and 10,000 lines), and large (100,000 lines). For each project, start the refactoring process while running a timer. The scope of the test ends when the system presents the user with the completed refactoring proposal. The time taken for each project is checked against the expected result.

  \item \textbf{Integrity of refactored code against runtime errors} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Refactoring tool ready, with user-provided code and test suite loaded \\
    \textbf{Input/Condition:} User initiates refactoring on the input code \\
    \textbf{Output/Result:} Refactored code passes all tests in the user-provided suite without runtime errors and adheres to Python syntax standards \\[2mm]
    \textbf{How test will be performed:} The refactoring tool will first apply the refactoring to the user-provided code. After refactoring, an automated test suite will run, confirming that all original tests pass, indicating no loss of functionality. The refactored code will then be validated by an automatic linter to ensure compliance with Python syntax standards.

  \item \textbf{Functionality preservation post-refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Python file ready for refactoring with proper configurations for the system \\
    \textbf{Input/Condition:} User initiates refactoring on the code file \\
    \textbf{Output/Result:} The refactored code should pass 100\% of user-provided tests \\[2mm]
    \textbf{How test will be performed:} see test \ref{tfr-?}

  \item \textbf{Accuracy of code smell detection} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Python file containing pre-determined code smells ready for refactoring with proper configurations for the system \\
    \textbf{Input/Condition:} User initiates refactoring on the code file \\
    \textbf{Output/Result:} All code smells determined prior to the test are detected. \\[2mm]
    \textbf{How test will be performed:} see test \ref{tfr-?}

  \item \textbf{Valid syntax and structure in refactored code} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} A refactored code file is present in the user's workspace \\
    \textbf{Input/Condition:} A python linter is run on the refactored python file \\
    \textbf{Output/Result:} Refactored code meets Python syntax and structural standards \\[2mm]
    \textbf{How test will be performed:} see test \ref{tfr-?}

  \item \textbf{Handling unexpected inputs} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE open and ready with various non-standard and invalid input files \\
    \textbf{Input/Condition:} User attempts to refactor invalid code files and non-Python files \\
    \textbf{Output/Result:} Tool detects invalid input, displays a clear error message, and does not crash \\[2mm]
    \textbf{How test will be performed:} The tester will sequentially give any of the following invalid files as input to the system :
    \begin{itemize}
        \item Non-Python files (e.g., .txt, .java, .cpp, .js)
        \item Invalid Python files with syntax errors (e.g., unmatched brackets, improper indentation)
        \item Corrupted files that contain random symbols or partially deleted code
    \end{itemize}
    For each file type, the tester will initiate the refactoring process and observe the tool's response. The tool should detect each invalid input, display an error message describing the issue, and recover from the error without crashing. 

  \item \textbf{Fallback options for failed refactoring attempts} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Static \\ %TODO: Change test format
    \textbf{Initial State:} IDE open with sample code that will cause a refactoring failure (e.g., complex code with unsupported patterns) \\
    \textbf{Input/Condition:} User initiates a refactoring operation on code that is incompatible with the refactoring tool’s capabilities \\
    \textbf{Output/Result:} Tool logs the refactoring error, displays a notification to the user, and proposes alternative refactoring options without stopping the process \\[2mm]
    \textbf{How test will be performed:} The tester will load code with patterns known to cause refactoring issues or require specific handling. Upon starting the refactoring, the tool should detect the failure, log the error internally, and present an error notification to the user. The tool should then offer alternative refactoring options (e.g., a simpler refactoring method or partial refactoring). The tester will verify that the process continues smoothly without requiring a restart and document each fallback option suggested by the tool.
\end{enumerate}

\noindent
\rule{\linewidth}{2pt}

\subsubsection{Operational \& Environmental}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Operational and Environmental requirements listed in the SRS \cite{SRS}. Testing includes adherence to emissions standards, integration with environmental metrics, and adaptability to diverse operational settings.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-OPE-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Emissions Standards Compliance} \\[2mm]
    \textbf{Objective:} Ensure that the tool’s emissions metrics and reports align with widely used standards (e.g., GRI 305, GHG, ISO 14064) to support users in environmental compliance and sustainability tracking. \\[2mm]
    \textbf{Scope:} This test applies to the tool's metrics and reporting components, including data format and labelling in the emissions report. \\[2mm]
    \textbf{Methodology:} Static analysis and documentation walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review emissions metrics in the tool’s documentation and compare them with requirements from GRI 305, GHG, and ISO 14064 standards.
      \item Verify that all required emissions metrics from these standards are present in the tool’s reports, with proper format and units.
      \item Confirm that all emissions categories and labels align with standard definitions to ensure consistency and accuracy.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team and project supervisor will conduct the documentation review and patch any discrepencies. \\[2mm]
    \textbf{Tools and Resources:} Tool’s user guide, sample emissions reports, GRI 305, GHG, and ISO 14064 standards documentation \\[2mm]
    \textbf{Acceptance Criteria:} The tool’s emissions metrics meet or exceed the coverage required by GRI 305, GHG, and ISO 14064 standards. All labels and units are accurate, consistent, and aligned with these standards.


  \item \textbf{Integration with GitHub Actions for automated refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} GitHub repository with access to the refactoring library in GitHub Actions \\
    \textbf{Input/Condition:} User sets up a GitHub Actions workflow that calls the refactoring library \\
    \textbf{Output/Result:} GitHub Actions successfully initiates refactoring processes through the library as part of a continuous integration workflow \\[2mm]
    \textbf{How test will be performed:} The tester will configure a GitHub Actions workflow in a test repository, specifying steps to call the refactoring library. After committing a sample code change, the workflow should trigger automatically. The tester will verify that the refactoring library runs within GitHub Actions, completes the refactoring process, and provides feedback in the workflow logs. Successful integration will be confirmed by viewing refactoring results directly within the GitHub Actions logs.

  \item \textbf{VSCode compatibility for refactoring library extension} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} VS Code IDE open and library installed\\
    \textbf{Input/Condition:} User installs and opens the refactoring library extension in VS Code \\
    \textbf{Output/Result:} The refactoring library extension installs successfully and runs within VS Code \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the VS Code marketplace, search for the refactoring library extension, and install it. Once installed, the tester will open the extension and perform a basic refactoring task to ensure the tool operates correctly within the VS Code environment and has access to the system library.

  \item \textbf{Import and export capabilities for codebases and metrics} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with the option to import/export codebases and metrics \\
    \textbf{Input/Condition:} User imports an existing codebase and exports refactored code and metrics reports \\
    \textbf{Output/Result:} The tool successfully imports codebases, refactors them, and exports both code and metrics reports \\[2mm]
    \textbf{How test will be performed:} The tester will load an existing codebase into the tool, initiate refactoring, and select the option to export the refactored code and metrics report. The export should generate files in the selected format. The tester will verify the file formats, check for correct data structure, and validate that the content accurately reflects the refactoring and metrics generated by the tool.

  \item \textbf{PIP package installation availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Python environment ready without the refactoring library installed \\
    \textbf{Input/Condition:} User installs the refactoring library using the command \texttt{pip install ecooptimizer} \\
    \textbf{Output/Result:} The library installs successfully without errors and is available for use in Python scripts \\[2mm]
    \textbf{How test will be performed:} The tester will open a new Python environment and enter the command to install the refactoring library via PIP. Once installed, the tester will import the library in a Python script and execute a basic function to confirm successful installation and functionality. The test verifies the library’s availability and ease of installation for end users.
\end{enumerate}

\noindent
\rule{\linewidth}{2pt}

\subsubsection{Maintenance and Support}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Maintenance and Support requirements listed in the SRS \cite{SRS}. These tests focus on rollback capabilities, compatibility with external libraries, automated testing, and extensibility for adding new code smells and refactoring functions.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-MS-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Extensibility for New Code Smells and Refactorings} \\[2mm]
    \textbf{Objective:} Confirm that the tool’s architecture allows for the addition of new code smell detections and refactoring techniques with minimal code changes and disruption to existing functionality. \\[2mm]
    \textbf{Scope:} This test applies to the tool’s extensibility, including modularity of code structure, ease of integration for new detection methods, and support for customization. \\[2mm]
    \textbf{Methodology:} Code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a code walkthrough focusing on the modularity and structure of the code smell detection and refactoring components.
      \item Add a sample code smell detection and refactoring function to validate the ease of integration within the existing architecture.
      \item Verify that the new function integrates seamlessly without altering existing features and that it is accessible through the tool’s main interface.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Once the system is complete, the development team will perform the code walkthrough and integration. They will review and approve any structural changes required. \\[2mm]
    \textbf{Tools and Resources:} Code editor, tool’s developer documentation, sample code smell and refactoring patterns \\[2mm]
    \textbf{Acceptance Criteria:} New code smells and refactoring functions can be added within the existing modular structure, requiring minimal changes. The new function does not impact the performance or functionality of existing features.


    \item \textbf{Maintainable and Adaptable Codebase} \\[2mm]
    \textbf{Objective:} Ensure that the codebase is modular, well-documented, and maintainable, supporting future updates and adaptations for new Python versions and standards. \\[2mm]
    \textbf{Scope:} This test covers the maintainability of the codebase, including structure, documentation, and modularity of key components. \\[2mm]
    \textbf{Methodology:} Static analysis and documentation walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase to verify the modular organization and clear separation of concerns between components.
      \item Examine documentation for code clarity and completeness, especially around key functions and configuration files.
      \item Assess code comments and the quality of function/method naming conventions, ensuring readability and consistency for future maintenance.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Once the system is complete, the development team will conduct the code review, to identify areas for improvement. If necessary, they will also ensure to improve the quality of the documentation. \\[2mm]
    \textbf{Tools and Resources:} Code editor, documentation templates, code commenting standards, Python development guides \\[2mm]
    \textbf{Acceptance Criteria:} The codebase is modular and maintainable, with sufficient documentation to support future development. All major components are organized to allow for easy updates with minimal impact on existing functionality.
  
  \item \textbf{Easy rollback of updates in case of errors} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Latest version of the tool installed with the ability to apply and revert updates \\
    \textbf{Input/Condition:} User applies a simulated new update and initiates a rollback \\
    \textbf{Output/Result:} The system reverts to the previous stable state without any errors \\[2mm]
    \textbf{How test will be performed:} The tester will apply a simulated update. Following this, they will initiate the rollback function, which should restore the tool to its previous stable version. The tester will verify that all features function as expected post-rollback and document the time taken to complete the rollback process
\end{enumerate}

\noindent
\rule{\linewidth}{2pt}

\subsubsection{Security}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Security requirements listed in the SRS \cite{SRS}. These tests seek to validate that the tool is protected against unauthorized access, data breaches, and external threats.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-SRT-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{User authentication before accessing tool features} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} System installed, user unauthenticated \\
    \textbf{Input/Condition:} User attempts to submit code or view refactoring reports \\
    \textbf{Output/Result:} Access is denied \\[2mm]
    \textbf{How test will be performed:} The tester will first attempt to submit code and access refactored reports without logging in, verifying that access is denied. The tester will then log in using valid company credentials and repeat the actions to confirm access is granted only after successful authentication.
  
  \item \textbf{Internal-Only Communication with Energy and Reinforcement Learning Tools} \\[2mm]
    \textbf{Objective:} Ensure that the refactoring tool communicates exclusively with the internal energy consumption tool and reinforcement learning model, without exposing any public API endpoints. \\[2mm]
    \textbf{Scope:} This test applies to all network and API interactions between the refactoring tool and internal services, ensuring no direct access is available to users or external applications. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a code walkthrough of the network and API components, focusing on the access control configurations for the energy consumption tool and reinforcement learning model.
      \item Inspect the code for any exposed API endpoints or network configurations that might allow external access.
      \item Attempt to access the internal tools directly from an external environment, ensuring that all external attempts are blocked.
      \item Verify that the tool’s communication is contained within internal environments and restricted to authorized system components.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review and testing, ensuring secure access protocols. \\[2mm]
    \textbf{Tools and Resources:} Access to the codebase, network configuration files, and security audit tools \\[2mm]
    \textbf{Acceptance Criteria:} No public or external API endpoints exist for the internal tools, and only the refactoring tool can access the energy consumption and reinforcement learning models.
  
    \item \textbf{Preventing Unauthorized Changes to Refactored Code and Reports} \\[2mm]
    \textbf{Objective:} Ensure the tool’s refactored code and energy reports are protected from any unauthorized external modifications, maintaining data integrity and user trust. \\[2mm]
    \textbf{Scope:} This test applies to the data security of refactored code and energy report storage layers, verifying that access is restricted to authorized users and processes only. \\[2mm]
    \textbf{Methodology:} Static analysis and code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase and database configurations to verify the implementation of access controls and data security measures.
      \item Confirm that the tool’s security settings prevent any unauthorized external modifications, maintaining data integrity across all storage layers.
      \item Document any vulnerabilities found and evaluate with the development team to ensure improvements are made where necessary.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review while the project supervisor will oversee the test results and approve any necessary security enhancements. \\[2mm]
    \textbf{Tools and Resources:} Access to security configuration files, code editor \\[2mm]
    \textbf{Acceptance Criteria:} The review attendees find no egregious faults within the system that might allow unauthorized external access or modifications to refactored code and energy report data.

  \item \textbf{Notification and consent for data handling (SR-PR 1)} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} System idle \\
    \textbf{Input/Condition:} User initiates refactoring on their source code \\
    \textbf{Output/Result:} Tool displays data handling notice and requests explicit consent before data collection \\[2mm]
    \textbf{How test will be performed:} The tester will begin the refactoring process, and the tool should present a notice explaining data collection, storage, and processing practices, in compliance with PIPEDA. The user must provide explicit consent before proceeding. The tester will confirm that no data collection occurs until consent is granted.
  
  \item \textbf{Confidential Handling of User Data in Compliance with PIPEDA} \\[2mm]
    \textbf{Objective:} Ensure that all user-submitted data, energy reports, and refactored code are treated as confidential, encrypted during storage and transmission, and managed according to PIPEDA. \\[2mm]
    \textbf{Scope:} This test applies to the tool’s data handling practices, specifically the encryption protocols for transmission and storage, and data modification options for user compliance requests. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the encryption settings in the codebase to confirm that all data related to user submissions, energy reports, and refactored code is encrypted during transmission and storage.
      \item Verify that an option is available for users to request modifications to their personal data as per PIPEDA requirements.
      \item Document any gaps in data security or user request handling, and collaborate with the development team to implement improvements as needed.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review and implement any necessary improvements, with the project supervisor overseeing the compliance with PIPEDA standards. \\[2mm]
    \textbf{Tools and Resources:} Access to encryption libraries, security configuration files \\[2mm]
    \textbf{Acceptance Criteria:} All user data is encrypted during storage and transmission, and users have a reliable method for requesting data modifications as per PIPEDA specifications.

  \item \textbf{Audit Logs for User Actions} \\[2mm]
    \textbf{Objective:} Ensure the tool maintains tamper-proof logs of key user actions, including code submissions, login events, and access to refactored code and reports, to ensure accountability and traceability. \\[2mm]
    \textbf{Scope:} This test applies to the logging mechanisms for user actions, focusing on the security and tamper-proof nature of logs. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the logging mechanisms within the codebase to confirm that events such as logins, code submissions, and report accesses are properly recorded with timestamps and user identifiers.
      \item Document the integrity of the logs and any vulnerabilities found, and collaborate with the development team on any necessary improvements.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review, with oversight by the project supervisor to verify that logging mechanisms meet security requirements. \\[2mm]
    \textbf{Tools and Resources:} Access to log files, logging library documentation, security testing tools \\[2mm]
    \textbf{Acceptance Criteria:} Logs are tamper-proof, recording all critical user actions with integrity, and resistant to unauthorized modifications.

  \item \textbf{Audit Logs for Refactoring Processes} \\[2mm]
    \textbf{Objective:} Ensure that the tool maintains a secure, tamper-proof log of all refactoring processes, including pattern analysis, energy analysis, and report generation, for accountability in refactoring events. \\[2mm]
    \textbf{Scope:} This test covers the logging of refactoring events, ensuring logs are complete and tamper-proof for future auditing needs. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase to confirm that each refactoring event (e.g., pattern analysis, energy analysis, report generation) is logged with details such as timestamps and event descriptions.
      \item Document any logging gaps or security vulnerabilities, and consult with the development team to implement enhancements.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will review and test the logging mechanisms, with the project supervisor ensuring alignment with auditing requirements. \\[2mm]
    \textbf{Tools and Resources:} Access to logging components, tamper-proof logging tools \\[2mm]
    \textbf{Acceptance Criteria:} All refactoring processes are logged in a secure, tamper-proof manner, ensuring complete traceability for future audits.
\end{enumerate}

\noindent
\rule{\linewidth}{2pt}

\subsubsection{Cultural}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Cultural requirements listed in the SRS \cite{SRS}. These test are to ensure that the tool is accessible and appropriate for a global audience, avoiding any culturally sensitive or inappropriate elements. 

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-CULT-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Cultural sensitivity of icons and colours} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give an answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} Users complete tasks requiring prompts and answer the survey found in \ref{A.2} on cultural sensitivity of the interface design.

  \item \textbf{Support for metric and imperial units (CULT 2)} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Tool ready with energy consumption metrics displayed in the default unit system \\
    \textbf{Input/Condition:} User toggles the measurement units between metric and imperial \\
    \textbf{Output/Result:} Energy consumption measurements update correctly between metric and imperial units \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the settings, locate the measurement unit toggle, and switch between metric and imperial units. After each toggle, the displayed energy consumption data should reflect the correct measurement units. The tester will validate accuracy by comparing values against known conversions to ensure the toggle functions accurately and smoothly.

  \item \textbf{Cultural sensitivity of content} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give an answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} Users complete tasks requiring prompts and answer the survey found in \ref{A.2} on cultural sensitivity of the content of the system.
\end{enumerate}

\noindent
\rule{\linewidth}{2pt}

\subsubsection{Compliance}
\rule{\linewidth}{2pt}

\medskip

\noindent
The following subsection tests cover all Compliance requirements listed in the SRS \cite{SRS}. The tests focus on adherence to PIPEDA, CASL, and ISO 9001, as well as SSADM standards, ensuring the tool complies with relevant regulations and aligns with professional development practices.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-CPL-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Compliance with PIPEDA and CASL} \\[2mm]
    \textbf{Objective:} Ensure the tool’s data collection, usage, storage, and communication practices are fully compliant with the Personal Information Protection and Electronic Documents Act (PIPEDA) and Canada’s Anti-Spam Legislation (CASL), to avoid legal penalties and enhance user trust. \\[2mm]
    \textbf{Scope:} This test applies to all processes related to data handling, storage, and user communication to verify compliance with PIPEDA and CASL. \\[2mm]
    \textbf{Methodology:} Documentation walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the tool’s data handling and storage protocols to confirm compliance with PIPEDA, particularly focusing on secure storage, data usage transparency, and privacy rights.
      \item Verify the presence of a user consent mechanism that informs users of data collection and provides options for managing their data.
      \item Inspect communication practices to ensure compliance with CASL, confirming that the tool provides users with notification and opt-in options for all communications.
      \item Document any gaps in compliance and consult with the development team for required adjustments.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the compliance review and implement any necessary updates. \\[2mm]
    \textbf{Tools and Resources:} Access to documentation on PIPEDA and CASL requirements, tool’s data handling and communication protocols, test user accounts for opt-in verification \\[2mm]
    \textbf{Acceptance Criteria:} The tool complies with all PIPEDA and CASL requirements, with secure data handling, user consent options, and compliant communication practices.

\item \textbf{Compliance with ISO 9001 and SSADM Standards} \\[2mm]
    \textbf{Objective:} Ensure the tool’s quality management and software development processes align with ISO 9001 for quality management and SSADM (Structured Systems Analysis and Design Method) standards for software development, building stakeholder trust and market acceptance. \\[2mm]
    \textbf{Scope:} This test covers the tool’s adherence to ISO 9001 quality management practices and SSADM methodologies for software development processes. \\[2mm]
    \textbf{Methodology:} Documentation walkthrough and code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a review of the tool’s quality management procedures to verify alignment with ISO 9001 standards, including documentation, testing, and feedback mechanisms.
      \item Examine software development workflows to confirm adherence to SSADM standards, focusing on design, analysis, and structured development practices.
      \item Identify any deviations from ISO 9001 and SSADM requirements, document these findings, and discuss necessary adjustments with the development team.
      \item Validate improvements in quality management and software development after implementing recommendations.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the standards compliance review, and the project supervisor will oversee the review process. \\[2mm]
    \textbf{Tools and Resources:} Access to ISO 9001 and SSADM standards documentation, project quality management records, and development workflows \\[2mm]
    \textbf{Acceptance Criteria:} The tool’s quality management and software development processes fully adhere to ISO 9001 and SSADM standards, supporting a high-quality, structured approach to development.
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements} \label{trace-sys}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
			

\bibliography{../../refs/References}

\newpage

\begin{appendices}

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?} \label{A.2}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section{Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{appendices}

\end{document}