\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{paralist}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newcommand{\SRS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/SRS/SRS.pdf}{SRS}}
\newcommand{\MG}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}}
\newcommand{\MIS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}}

\newcommand{\colorrule}{\textcolor{BlueViolet}{\rule{\linewidth}{2pt}}}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{4cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
November 4th, 2024 & 0.0 & Created initial revision of VnV Plan\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

% \section{Symbols, Abbreviations, and Acronyms}

% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{l l} 
%   \toprule		
%   \textbf{symbol} & \textbf{description}\\
%   \midrule 
%   T & Test\\
%   \bottomrule
% \end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \cite{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

% \newpage

\pagenumbering{arabic}

This document outlines the process and methods to ensure that the software meets its requirements and functions as intended. This document provides a structured approach to evaluating the product, incorporating both verification (to confirm that the software is built correctly) and validation (to confirm that the correct software has been built). By systematically identifying and mitigating potential issues, the V\&V process aims to enhance quality, reduce risks, and ensure compliance with both functional and non-functional requirements.\\

The following sections will go over the approach for verification and validation, including the team structure, verification strategies at various stages and tools to be employed. Furthermore, a detailed list of system and unit tests are also included in this document.

\section{General Information}

\subsection{Summary}

The software being tested is called EcoOptimizer. EcoOptimizer is a python refactoring library that focuses on optimizing code in a way that reduces its energy consumption. The system will be capable to analyze python code in order to spot inefficiencies (code smells) within, measuring the energy efficiency of the inputted code and, of course, apply appropriate refactorings that preserve the initial function of the source code. \\

Furthermore, peripheral tools such as a Visual Studio Code (VS Code) extension and GitHub Action are also to be tested. The extension will integrate the library with Visual Studio Code for a more efficient development process and the GitHub Action will allow a proper integration of the library into continuous integration (CI) workflows.

\subsection{Objectives}

The primary objective of this project is to build confidence in the \textbf{correctness} and \textbf{energy efficiency} of the refactoring library, ensuring that it performs as expected in improving code efficiency while maintaining functionality. Usability is also emphasized, particularly in the user interfaces provided through the \textbf{VS Code extension} and \textbf{GitHub Action} integrations, as ease of use is critical for adoption by software developers. These qualities—correctness, energy efficiency, and usability—are central to the project’s success, as they directly impact user experience, performance, and the sustainable benefits of the tool.\\

Certain objectives are intentionally left out-of-scope due to resource constraints. We will not independently verify external libraries or dependencies; instead, we assume they have been validated by their respective development teams. 

\subsection{Challenge Level and Extras}

Our project, set at a \textbf{general} challenge level, includes two additional focuses: \textbf{user documentation} and \textbf{usability testing}. The user documentation aims to provide clear, accessible guidance for developers, making it easy to understand the tool’s setup, functionality, and integration into existing workflows. Usability testing will ensure that the tool is intuitive and meets user needs effectively, offering insights to refine the user interface and optimize interactions with its features.

\subsection{Relevant Documentation}

The Verification and Validation (VnV) plan relies on three key documents to guide testing and assessment: 
\begin{itemize}
  \item[] \textbf{Software Requirements Specification (\SRS)\cite{SRS}:} The foundation for the VnV plan, as it defines the functional and non-functional requirements the software must meet; aligning tests with these requirements ensures that the software performs as expected in terms of correctness, performance, and usability.
  
  \item[] \textbf{Module Interface Specification (\MG)\cite{MGDoc}:} Provides detailed information about each module's interfaces, which is crucial for integration testing to verify that all modules interact correctly within the system.
  
  \item[] \textbf{Module Guide (\MIS)\cite{MISDoc}:} Outlines the system's architectural design and module structure, ensuring the design of tests that align with the intended flow and dependencies within the system.
\end{itemize}

\section{Plan}

The following section outlines the comprehensive Verification and Validation (VnV) strategy, detailing the team structure, specific plans for verifying the Software Requirements Specification (SRS), design, implementation, and overall VnV process, as well as the automated tools employed and the approach to software validation.

\subsection{Verification and Validation Team}

The Verification and Validation (VnV) Team for the Source Code Optimizer project consists of the following members and their specific roles:

\begin{itemize}
    \item \textbf{Sevhena Walker}: Lead Tester. Oversees and coordinates the testing process, ensuring all feedback is applied and all project goals are met.
    \item \textbf{Mya Hussain}: Functional Requirements Tester. Tests the software to verify that it meets all specified functional requirements.
    \item \textbf{Ayushi Amin}: Integration Tester. Focuses on testing the connection between the various components of the Python package, the VSCode plugin, and the GitHub Action to ensure seamless integration.
    \item \textbf{Tanveer Brar}: Non-Functional Requirements Tester. Assesses performance/security compliance with project standards.
    \item \textbf{Nivetha Kuruparan}: Non-Functional Requirements Tester. Ensures that the final product meets user expectations regarding user experience and interface intuitiveness.
    \item \textbf{Istvan David} (supervisor): Supervises the overall VnV process, providing feedback and guidance based on industry standards and practices.
\end{itemize}

\subsection{SRS Verification Plan}

\textbf{Function \& Non-Functional Requirements:}
\begin{itemize}
    \item A comprehensive test suite that covers all requirements specified in the SRS will be created.
    \item Each requirement will be mapped to specific test cases to ensure maximum coverage.
    \item Automated and manual testing will be conducted to verify that the implemented system meets each functional requirement.
    \item Usability testing with representative users will be carried out to validate user experience requirements and other non-functional requirements.
    \item Performance tests will be conducted to verify that the system meets specified performance requirements.
\end{itemize}

\textbf{Traceability Matrix:}
\begin{itemize}
    \item We will create a requirements traceability matrix that links each SRS requirement to its corresponding implementation, test cases, and test results.
    \item This matrix will help identify any requirements that may have been overlooked during development.
\end{itemize}

\textbf{Supervisor Review:}
\begin{itemize}
    \item After the implementation of the system, we will conduct a formal review session with key stakeholders such as our project supervisor, Dr. Istvan David.
    \item The stakeholders will be asked to verify that each requirement in the SRS is mapped out to specific expectations of the project. 
    \item Prior to meeting, we will provide a summary of key requirements and design decisions and prepare a list specific questions or areas where we seek guidance.
    \item During the meeting, we will present an overview of the SRS using tables and other visual aids. We will conduct a walk through of critical section. Finally, we will discuss any potential risks or challenges identified.
\end{itemize}

\textbf{User Acceptance Testing (UAT):}
\begin{itemize}
    \item We will involve potential end-users in testing the system to ensure it meets real-world usage scenarios.
    \item Feedback from UAT will be used to identify any discrepancies between the SRS and user expectations.
\end{itemize}

\textbf{Continuous Verification:}
\begin{itemize}
    \item Throughout the development process, we will regularly review and update the SRS to ensure it remains aligned with the evolving system.
    \item Any changes to requirements will be documented and their impact on the system assessed.
\end{itemize}

\textbf{\textit{\\Checklist for SRS Verification Plan}}
\begin{itemize}
    \item[$\square$] Create comprehensive test suite covering all SRS requirements
    \item[$\square$] Map each requirement to specific test cases
    \item[$\square$] Conduct automated testing for functional requirements
    \item[$\square$] Perform manual testing for functional requirements
    \item[$\square$] Carry out usability testing with representative users
    \item[$\square$] Conduct performance tests to verify system meets requirements
    \item[$\square$] Create requirements traceability matrix
    \item[$\square$] Link each SRS requirement to implementation in traceability matrix
    \item[$\square$] Link each SRS requirement to test cases in traceability matrix
    \item[$\square$] Link each SRS requirement to test results in traceability matrix
    \item[$\square$] Schedule formal review session with project supervisor
    \item[$\square$] Prepare summary of key requirements and design decisions for supervisor review
    \item[$\square$] Prepare list of specific questions for supervisor review
    \item[$\square$] Create visual aids for SRS overview presentation
    \item[$\square$] Conduct walkthrough of critical SRS sections during review
    \item[$\square$] Discuss potential risks and challenges with supervisor
    \item[$\square$] Organize User Acceptance Testing (UAT) with potential end-users
    \item[$\square$] Collect and analyze UAT feedback
    \item[$\square$] Identify discrepancies between SRS and user expectations from UAT
    \item[$\square$] Establish process for regular SRS review and updates
    \item[$\square$] Document any changes to requirements
    \item[$\square$] Assess impact of requirement changes on the system
\end{itemize}

\subsection{Design Verification Plan}

\textbf{Peer Review Plan:}
\begin{itemize}
    \item Each team member along with other classmates will thoroughly review the entire Design Document.
    \item A checklist-based approach will be used to ensure all key elements are covered.
    \item Feedback will be collected and discussed in a dedicated team meeting.
\end{itemize}

\textbf{Supervisor Review:}
\begin{itemize}
    \item A structured review meeting will be scheduled with our project supervisor, Dr. Istvan David.
    \item We will present an overview of the design using visual aids (e.g., diagrams, tables).
    \item We will conduct a walkthrough of critical sections.
    \item We will use our project's issue tracker to document and follow up on any action items or changes resulting from this review.
\end{itemize}

\begin{itemize}
  \item[$\square$] All functional requirements are mapped to specific design elements 
  \item[$\square$] Each functional requirement is fully addressed by the design 
  \item[$\square$] No functional requirements are overlooked or partially implemented 
  \item[$\square$] Performance requirements are met by the design 
  \item[$\square$] Scalability considerations are incorporated 
  \item[$\square$] Reliability and availability requirements are satisfied 
  \item[$\square$] Usability requirements are reflected in the user interface design
  \item[$\square$] High-level architecture is clearly defined 
  \item[$\square$] Architectural decisions are justified with rationale 
  \item[$\square$] Architecture aligns with project constraints and goals 
  \item[$\square$] All major components are identified and described 
  \item[$\square$] Interactions between components are clearly specified 
  \item[$\square$] Component responsibilities are well-defined 
  \item[$\square$] Appropriate data structures are chosen for each task 
  \item[$\square$] Efficient algorithms are selected for critical operations 
  \item[$\square$] Rationale for data structure and algorithm choices is provided
  \item[$\square$] UI design is consistent with usability requirements 
  \item[$\square$] User flow is logical and efficient 
  \item[$\square$] Accessibility considerations are incorporated 
  \item[$\square$] All external interfaces are properly specified 
  \item[$\square$] Interface protocols and data formats are defined 
  \item[$\square$] Error handling for external interfaces is addressed 
  \item[$\square$] Comprehensive error handling strategy is in place
  \item[$\square$] Exception scenarios are identified and managed 
  \item[$\square$] Error messages are clear and actionable 
  \item[$\square$] Authentication and authorization mechanisms are described 
  \item[$\square$] Data encryption methods are specified where necessary 
  \item[$\square$] Security best practices are followed in the design
  \item[$\square$] Design allows for future expansion and feature additions 
  \item[$\square$] Code modularity and reusability are considered 
  \item[$\square$] Documentation standards are established for maintainability 
  \item[$\square$] Performance bottlenecks are identified and addressed 
  \item[$\square$] Resource utilization is optimized 
  \item[$\square$] Performance testing strategies are outlined 
  \item[$\square$] Design adheres to established coding standards 
  \item[$\square$] Industry best practices are followed 
  \item[$\square$] Design patterns are appropriately applied
  \item[$\square$] All major design decisions are justified 
  \item[$\square$] Trade-offs are explained with pros and cons 
  \item[$\square$] Alternative approaches considered are documented 
  \item[$\square$] Documents is clear, concise, and free of ambiguities 
  \item[$\square$] Documents follows a logical structure 
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation (V\&V) Plan for the Source Code Optimizer project serves as a critical document that requires a thorough examination to confirm its validity and effectiveness. To achieve this, the following strategies will be implemented:

\begin{enumerate}
    \item \textbf{Peer Review}: Team members and peers will conduct a detailed review of the V\&V plan. This process aims to uncover any gaps or areas that could benefit from enhancement, leveraging the collective insights of the group to strengthen the overall plan.
    
    \item \textbf{Fault Injection Testing}: We will utilize mutation testing to assess the capability of our test cases to identify intentionally introduced faults. By generating variations of the original code, we can evaluate whether our testing strategies are robust enough to catch these discrepancies, hence enhancing the reliability of our verification process.
    
    \item \textbf{Feedback Loop Integration}: Continuous feedback from review sessions and testing activities will be systematically integrated to refine the V\&V plan. This ongoing process ensures the plan evolves based on insights gained from practical testing and peer input.
\end{enumerate}


\noindent To comprehensively verify the V\&V plan, we will utilize the following checklist:

\begin{itemize}
    \item[$\square$] Does the V\&V plan include all necessary aspects of software verification and validation?
    \item[$\square$] Are the roles and responsibilities clearly outlined within the V\&V framework?
    \item[$\square$] Is there a diversity of testing methodologies included (e.g., unit testing, integration testing, system testing)?
    \item[$\square$] Does the plan have a clear process for incorporating feedback and gaining continuous improvement?
    \item[$\square$] Are success criteria established for each phase of testing?
    \item[$\square$] Is mutation testing considered to evaluate the effectiveness of the test cases?
    \item[$\square$] Are mechanisms in place to monitor and address any identified issues during the V\&V process?
    \item[$\square$] Does the V\&V plan align with the project timeline, available resources, and other constraints?
\end{itemize}

\subsection{Implementation Verification Plan}

The Implementation Verification Plan for the Source Code Optimizer project aims to ensure that the software implementation adheres to the requirements and design specifications defined in the SRS. Key components of this plan include:

\begin{itemize}
    \item \textbf{Unit Testing}: A comprehensive suite of unit tests will be established to validate the functionality of individual components within the optimizer. These tests will specifically focus on the effectiveness of the code refactoring methods employed by the optimizer, utilizing \texttt{pytest} for writing and executing these tests.
    
    \item \textbf{Static Code Analysis}: To maintain high code quality, static analysis tools such as \texttt{Pylint} and \texttt{Flake8} will be employed. These tools will help identify potential bugs, security vulnerabilities, and adherence to coding standards in the Python codebase, ensuring that the optimizer is both efficient and secure.
    
    \item \textbf{Code Walkthroughs and Reviews}: The development team will hold regular code reviews and walkthrough sessions to collaboratively evaluate the implementation of the source code optimizer. These sessions will focus on code quality, readability, and compliance with the project’s design patterns. Additionally, the final presentation will provide an opportunity for a thorough code walkthrough, allowing peers to contribute feedback on usability and functionality.
    
    \item \textbf{Continuous Integration}: The project will implement continuous integration practices using tools like GitHub Actions. This approach will automate the build and testing processes, allowing the team to verify that each change to the optimizer codebase meets the established quality criteria and integrates smoothly with the overall system.
    
    \item \textbf{Performance Testing}: The performance of the source code optimizer will be assessed to simulate various usage scenarios. This testing will focus on evaluating how effectively the optimizer processes large codebases and applies refactorings, ensuring that the tool operates efficiently under different workloads.
\end{itemize}

\subsection{Automated Testing and Verification Tools}

\textbf{Unit Testing Framework:} Pytest is chosen as the main framework for unit testing due to its \begin{inparaenum}[(i)]
                                                                                                        \item scalability
                                                                                                        \item integration with other tools(\texttt{coverage.py} for code coverage)
                                                                                                        \item extensive support for parameterized tests.
\end{inparaenum} These features make it easy to test the codebase as it grows, adapting to changes throughout the project's development \citep{pytest}.\\

\noindent\textbf{Profiling Tool:} The codebase will be evaluated based on results from both time and memory profiling to optimize computational speed and resource usage. For time profiling (recording the number of function calls, time spent in each function, and its descendants), \texttt{cProfile} will be used, as it is included within Python, making it a convenient choice for profiling. For memory profiling, \texttt{memory\_profiler} will be used, as it is easy to install and includes built-in support for visual display of output \citep{memory_profiler}.\\

\noindent\textbf{Static Analyzer:} The codebase will be statically analyzed using the PyLint tool, as it is easy to integrate with most IDEs and is actively maintained (as opposed to PySmells). PyLint provides a wide range of support, including error detection, refactoring suggestions, and code style enforcement, making it a strong choice for static analysis \citep{pylint}.\\

\noindent\textbf{Code Coverage Tools and Plan for Summary:} The code base will be analyzed to determine the percentage of code executed during tests. For granular-level coverage, \texttt{coverage.py} will be used, as it supports branch, line, and path coverage. Additionally, \texttt{coverage.py} is a test framework-independent, allowing integration with the project's unit test framework, Pytest.\\
Initially the aim is to achieve a 40\% coverage and gradually increment the level with time. Weekly reports generated from \texttt{coverage.py} will be used to track coverage trends and set goals accordingly to address any gaps in testing in the growing codebase.\\


\noindent\textbf{Test Coverage Tools:} The project will use \texttt{TestRail}, a test case management tool, to provide traceability from test cases to requirements, ensuring all requirements are covered by tests. Additionally, TestRail can help run tests and track results in integration with Pytest \citep{testrail}.\\


\noindent\textbf{Linters:} To enforce the official Python PEP 8 style guide, the team will use \texttt{PyLint}, which is also the choice for static analysis of the code.\\

\noindent\textbf{CI Plan:} As mentioned in the Development Plan, GitHub Actions will integrate the above tools within the CI pipeline. GitHub Actions will be configured to run unit tests written in \texttt{Pytest} as well as \texttt{PyLint} checks on every code push. Through automated testing, any errors and code smells will be promptly identified.\\

\subsection{Software Validation Plan}

\begin{itemize}
    \item One or more open source Python code bases will be used to test the tool on. Based on its performance in functional and non-functional tests outlined in further sections of the document, the software can be validated against defined requirements.
    \item In addition to this, the team will reach out to Dr David as well as a group of volunteer Python developers to perform usability testing on the IDE plugin workflow as well as the CI/CD workflow.
    \item The team will conduct a comprehensive review of the requirements from Dr David through the Rev 0 Demo.
\end{itemize}

\section{System Tests}

This section outlines the tests for verifying both functional and nonfunctional requirements of the software, ensuring it meets user expectations and performs reliably. This includes tests for code quality, usability, performance, security, and traceability, covering essential aspects of the software’s operation and compliance.

\subsection{Tests for Functional Requirements}

The subsections below outline tests corresponding to functional 
requirements in the \SRS \cite{SRS}. Each test is associated with a unique functional area, helping to confirm that the tool meets the specified requirements. Each functional area has its own subsection for clarity.

\noindent
\colorrule

\subsubsection{Code Input Acceptance Tests}
\colorrule

\medskip

\noindent
This section covers the tests for ensuring the system correctly accepts Python source code files, detects errors in invalid files, and provides suitable feedback (FR 1).

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-IA-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Valid Python File Acceptance} \\[2mm]
    \textbf{Control:} Automatic \\    
    \textbf{Initial State:} Tool is idle.  \\
    \textbf{Input:} A valid Python file (filename.py) with valid standard syntax. \\
    \textbf{Output:} The system accepts the file without errors.\\[2mm]
    \textbf{Test Case Derivation:} Confirming that the system correctly processes a valid Python file as per FR 1.\\[2mm]
    \textbf{How test will be performed:} Feed a syntactically valid .py file to the tool and observe if it’s accepted without issues.
            
  \item \textbf{Feedback for Python File with Bad Syntax} \\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool is idle. \\
    \textbf{Input:} A .py file (badSyntax.py) containing deliberate syntax errors that render the file unrunnable. \\
    \textbf{Output:} The system rejects the file and provides an error message detailing the syntax issue. \\[2mm]
    \textbf{Test Case Derivation:} Verifies the tool’s handling of syntactically invalid Python files to ensure user awareness of the syntax issue, meeting FR 1. \\[2mm]
    \textbf{How test will be performed:} Feed a .py file with syntax errors to the tool and check that the system identifies it as invalid and produces an appropriate error message.

  \item \textbf{Feedback for Non-Python File}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool is idle.\\
    \textbf{Input:} A non-Python file (document.txt) or a file with an incorrect extension (script.js).\\
    \textbf{Output:} The system rejects the file and provides an error message indicating the invalid file format.\\[2mm]
    \textbf{Test Case Derivation:} Ensures the tool detects unsupported file types and provides feedback, satisfying FR 1.\\[2mm]
    \textbf{How test will be performed:} Attempt to load a .txt or other non-Python file, and verify that the system rejects it with a message indicating an invalid file type.

  \item \textbf{Test for Original Code Passing the Original Test Suite}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Idle.\\
    \textbf{Input:} Python code and its associated test suite.\\
    \textbf{Output:} The original code passes 100\% of the test suite.\\[2mm]
    \textbf{Test Case Derivation:}  This test ensures that the original code is functional and compliant with the provided test suite, confirming that the input code is valid.\\[2mm]
    \textbf{How test will be performed:} 
    \begin{enumerate}
      \item The original code will be executed against its associated test suite.
      \item Verify that all tests in the original test suite pass, indicating that the original code is valid and functioning as expected.
    \end{enumerate}

  \item \textbf{Valid Python Test Suite Acceptance}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool is idle.\\
    \textbf{Input:} A valid Python test suite file (\texttt{testSuite.py}) with valid syntax and tests.\\
    \textbf{Output:} The system accepts the test suite and confirms it is ready for execution.\\[2mm]
    \textbf{Test Case Derivation:} Confirms that the tool can accept a valid test suite as input, as required by FR 2.\\[2mm]
    \textbf{How test will be performed:} Load a valid test suite \texttt{.py} file into the tool and observe that it is accepted without errors.

  \item \textbf{Feedback for Test Suite with Invalid Syntax}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool is idle.\\
    \textbf{Input:} A test suite file (\texttt{invalid\_test\_suite.py}) containing syntax errors.\\
    \textbf{Output:} The system rejects the test suite and provides an error message detailing the syntax issue.\\[2mm]
    \textbf{Test Case Derivation:} Verifies the tool's capability to identify and report errors in test suites, meeting FR 2.\\[2mm]
    \textbf{How test will be performed:} Load a test suite file with syntax errors into the tool and check for appropriate error reporting.

  \item \textbf{Test Suite with No Test Cases}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool is idle.\\
    \textbf{Input:} A valid Python file (\texttt{empty\_test\_suite.py}) that contains no test cases.\\
    \textbf{Output:} The system rejects the file and provides an error message indicating that there are no test cases present.\\[2mm]
    \textbf{Test Case Derivation:} Ensures the tool identifies test suites lacking test cases, complying with FR 2.\\[2mm]
    \textbf{How test will be performed:} Load a test suite file with no defined test cases and verify that the system produces an appropriate error message.
\end{enumerate}

\noindent
\colorrule

\subsubsection{Code Smell Detection Tests} \label{4.1.2}
\colorrule

\medskip

\noindent
This area includes tests to verify the detection of specified code 
smells that impact energy efficiency (FR 2).

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-CSD-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Detection of Large Class (LC)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file containing a class with many methods and attributes.\\
    \textbf{Input:} Python file with a class that exceeds the threshold for "Large Class" smell.\\
    \textbf{Output:} Tool identifies the "Large Class" smell and suggests refactoring options like breaking the class into smaller classes.\\[2mm]
    \textbf{Test Case Derivation:} Ensures "Large Class" code smells are identified and appropriately refactored.\\[2mm]
    \textbf{How test will be performed:} Load a file with a large class and verify detection.

  \item \textbf{Detection of Long Parameter List (LPL)}\\[2mm]
    \textbf{Control:} Automatic \\       
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file containing a method with a long parameter list.       \\
    \textbf{Input:} Python file with a method using more parameters than the threshold.\\
    \textbf{Output:} Tool flags the "Long Parameter List" smell and suggests bundling parameters into objects or reducing parameters.\\[2mm]
    \textbf{Test Case Derivation:} Ensures "Long Parameter List" code smell detection.\\[2mm]
    \textbf{How test will be performed:} Load a file with a method having a long parameter list and confirm detection.

  \item \textbf{Detection of Long Method (LM)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file with a method that exceeds the line limit threshold.\\
    \textbf{Input:} Python file containing a long method.\\
    \textbf{Output:} Tool detects "Long Method" and suggests breaking it into smaller methods.\\[2mm]
    \textbf{Test Case Derivation:} Ensures "Long Method" detection and suggestions for improving readability.\\[2mm]
    \textbf{How test will be performed:} Load a file with a long method and check for detection.

  \item \textbf{Detection of Long Message Chain (LMC)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file with a chain of method calls.\\
    \textbf{Input:} Python file containing a message chain exceeding the threshold.\\
    \textbf{Output:} Tool flags the "Long Message Chain" smell and suggests ways to simplify it, such as introducing intermediary methods.\\[2mm]
    \textbf{Test Case Derivation:} Validates "Long Message Chain" detection and suggestions for code simplification.     \\[2mm]
    \textbf{How test will be performed:} Load a file with a long chain of method calls and confirm detection.

  \item \textbf{Detection of Long Scope Chaining (LSC)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file containing deeply nested scopes.\\
    \textbf{Input:} Python file with excessive scope chaining.\\
    \textbf{Output:} Tool detects "Long Scope Chaining" and suggests reducing nesting or refactoring.\\[2mm]
    \textbf{Test Case Derivation:} Ensures tool detects deep nesting and provides ways to make code more readable.\\[2mm]
    \textbf{How test will be performed:} Load a file with nested scopes and confirm detection.

  \item \textbf{Detection of Long Base Class List (LBCL)}\\[2mm]
    \textbf{Control:} Automatic \\         
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file with a class that inherits from many base classes.\\
    \textbf{Input:} Python file containing a class with an extensive inheritance list.\\
    \textbf{Output:} Tool flags "Long Base Class List" and suggests refactoring, such as restructuring inheritance.\\[2mm]
    \textbf{Test Case Derivation:} Validates that long inheritance lists are detected, and refactoring options are provided.\\[2mm]
    \textbf{How test will be performed:} Load a file with a long base class list and confirm detection.

  \item \textbf{Detection of Useless Exception Handling (UEH)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file with empty or redundant \texttt{try-except} blocks.\\
    \textbf{Input:} Python file containing useless exception handling blocks.\\
    \textbf{Output:} Tool flags "Useless Exception Handling" and suggests meaningful handling or removal.\\[2mm]
    \textbf{Test Case Derivation:} Confirms detection of redundant exception handling and refactoring options.\\[2mm]
    \textbf{How test will be performed:} Load a file with empty \texttt{try-except} blocks and verify detection.

  \item \textbf{Detection of Long Lambda Function (LLF)}\\[2mm]
    \textbf{Control:} Automatic \\    
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file containing lambda functions that exceed the line or complexity threshold.     \\
    \textbf{Input:} Python file with a long lambda function.\\
    \textbf{Output:} Tool detects "Long Lambda Function" and suggests converting it to a named function.\\[2mm]
    \textbf{Test Case Derivation:} Validates detection of long lambda functions and refactoring suggestions for clarity.\\[2mm]
    \textbf{How test will be performed:} Load a file with a long lambda and verify detection.

  \item \textbf{Detection of Complex List Comprehension (CLC)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file with list comprehensions containing nested conditions.\\
    \textbf{Input:} Python file with a complex list comprehension.\\
    \textbf{Output:} Tool flags "Complex List Comprehension" and suggests simplifying the expression.\\[2mm]
    \textbf{Test Case Derivation:} Ensures tool detects complex list comprehensions and suggests simplifications.\\[2mm]
    \textbf{How test will be performed:} Load a file with complex list comprehension and confirm detection.

  \item \textbf{Detection of Long Element Chain (LEC)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file with a long sequence of chained elements (e.g., dictionary access).\\
    \textbf{Input:} Python file containing an element chain exceeding the length threshold.\\
    \textbf{Output:} Tool detects "Long Element Chain" and suggests restructuring the code for readability.\\[2mm]
    \textbf{Test Case Derivation:} Confirms tool detects long element chains and suggests simplification.\\[2mm]
    \textbf{How test will be performed:} Load a file with a long element chain and verify detection.

  \item \textbf{Detection of Long Ternary Conditional Expression (LTCE)}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has loaded a \texttt{.py} file with a ternary conditional expression that exceeds the line or complexity threshold.\\
    \textbf{Input:} Python file containing a long ternary conditional.\\
    \textbf{Output:} Tool flags "Long Ternary Conditional Expression" and suggests converting to a standard \texttt{if-else} block.\\[2mm]
    \textbf{Test Case Derivation:} Ensures long ternary expressions are detected, and refactoring options are provided.\\[2mm]
    \textbf{How test will be performed:} Load a file with a long ternary expression and confirm detection.


  \item \textbf{No Code Smells Detected Handling}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool is idle.\\
    \textbf{Input:} A valid Python file (filename.py) that adheres to best practices and contains no detectable code smells.\\
    \textbf{Output:} The system returns a message indicating that no code smells were found in the code.\\[2mm]
    \textbf{Test Case Derivation:} This test ensures that the tool can correctly identify when there are no code smells present, as per functional requirement FR 2.\\[2mm]
    \textbf{How test will be performed:} Provide a Python file that is well-structured and free of common code smells, and verify that the tool outputs a message confirming the absence of smells.

\end{enumerate}

\noindent
\colorrule

\subsubsection{Refactoring Suggestion (RS) Tests}
\colorrule

\medskip

\noindent
The following tests aim to validate the tool's capability to suggest appropriate refactorings in response to identified code smells, as outlined in the functional requirements (FR 5). These tests ensure that for each detected code smell, the tool provides actionable code modifications that not only enhance maintainability but also lead to measurable reductions in energy consumption.
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-RS-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Large Class (LC) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has identified a large class in the provided Python file.\\
    \textbf{Input:} A Python file containing a class with a high number of lines of code and methods.\\
    \textbf{Output:} The tool suggests splitting the large class into smaller, more manageable classes and displays the suggested modifications.\\[2mm]
    \textbf{Test Case Derivation:} Ensures that the tool provides a refactoring suggestion that reduces energy consumption while maintaining functionality as per FR 5.\\[2mm]
    \textbf{How test will be performed:} Feed a Python file with a large class to the tool and verify that it displays a refactoring suggestion to break the class into smaller parts.
  
  \item \textbf{Long Parameter List (LPL) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has detected a method with too many parameters.\\
    \textbf{Input:} A Python file with a method signature that contains an excessive number of parameters.\\
    \textbf{Output:} The tool suggests using a data structure (e.g., a dictionary or an object) to encapsulate the parameters and shows the modified method signature.\\[2mm]
    \textbf{Test Case Derivation:} Confirms that the tool can identify long parameter lists and suggest refactoring to improve code clarity and energy efficiency.\\[2mm]
    \textbf{How test will be performed:} Submit a Python file with a method featuring a long parameter list and check that the tool provides a refactoring suggestion.
  
  \item \textbf{Long Method (LM) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has identified a method that exceeds a predefined line count.\\
    \textbf{Input:} A Python file containing a method that is excessively long.\\
    \textbf{Output:} The tool suggests breaking the long method into smaller methods and displays the proposed modifications.\\[2mm]
    \textbf{Test Case Derivation:} Validates that the tool recognizes long methods and suggests refactoring to enhance maintainability and reduce energy usage.\\[2mm]
    \textbf{How test will be performed:} Provide the tool with a Python file containing a long method and observe if it suggests breaking it into smaller methods with clear modifications.
  
  \item \textbf{Long Message Chain (LMC) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has identified a long message chain in the code.\\
    \textbf{Input:} A Python file with chained method calls resulting in a long message chain.\\
    \textbf{Output:} The tool suggests simplifying the message chain by assigning intermediate results to variables and shows the refactored code.\\[2mm]
    \textbf{Test Case Derivation:} Ensures that the tool can detect long message chains and provide effective refactoring suggestions that improve energy efficiency.\\[2mm]
    \textbf{How test will be performed:} Use a Python file containing a long message chain and confirm that the tool displays a suggestion to simplify it.
  
  \item \textbf{Long Scope Chaining (LSC) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has detected a long scope chain in the provided code.\\
    \textbf{Input:} A Python file with multiple nested function calls or scope references.\\
    \textbf{Output:} The tool suggests flattening the scope chain by refactoring into clearer, standalone function calls and displays the proposed modifications.\\[2mm]
    \textbf{Test Case Derivation:} Confirms that the tool can identify long scope chaining and suggest refactoring to enhance clarity and maintainability.\\[2mm]
    \textbf{How test will be performed:} Feed a Python file with a long scope chain to the tool and check if it suggests appropriate refactoring.
  
  \item \textbf{Long Base Class List (LBCL) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has identified a class inheriting from many base classes.\\
    \textbf{Input:} A Python file with a class declaration that inherits from multiple base classes.\\
    \textbf{Output:} The tool suggests refactoring the class to reduce the number of base classes, possibly by using composition instead of inheritance, and displays the suggested changes.\\[2mm]
    \textbf{Test Case Derivation:} Validates that the tool recognizes long base class lists and provides suggestions for improving class design and energy efficiency.\\[2mm]
    \textbf{How test will be performed:} Provide the tool with a Python file containing a class with a long base class list and observe if it suggests refactoring.
  
  \item \textbf{Useless Exception Handling (UEH) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has detected unnecessary exception handling in the code.\\
    \textbf{Input:} A Python file containing try-except blocks that do not provide meaningful handling.\\
    \textbf{Output:} The tool suggests removing or modifying the exception handling and displays the modified code.\\[2mm]
    \textbf{Test Case Derivation:} Validates that the tool can identify useless exception handling and suggests actionable refactorings to enhance clarity and efficiency.\\[2mm]
    \textbf{How test will be performed:} Feed the tool a Python file with unnecessary exception handling and check if it suggests modifications.
  
  \item \textbf{Long Lambda Function (LLF) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has identified a lambda function that is too complex or lengthy.\\
    \textbf{Input:} A Python file containing a long or complex lambda function.\\
    \textbf{Output:} The tool suggests refactoring the lambda function into a named function and shows the proposed changes.\\[2mm]
    \textbf{Test Case Derivation:} Confirms that the tool recognizes long lambda functions and provides suggestions for refactoring to enhance readability and performance.\\[2mm]
    \textbf{How test will be performed:} Submit a Python file with a long lambda function to the tool and verify that it suggests converting it into a named function.
  
  \item \textbf{Complex List Comprehension (CLC) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has detected a complex list comprehension.\\
    \textbf{Input:} A Python file containing a list comprehension that is hard to read or understand.\\
    \textbf{Output:} The tool suggests breaking the list comprehension into a for loop and displays the modified code.\\[2mm]
    \textbf{Test Case Derivation:} Ensures that the tool identifies complex list comprehensions and suggests refactoring for clarity and efficiency.\\[2mm]
    \textbf{How test will be performed:} Provide a Python file with a complex list comprehension and observe if the tool offers a simpler alternative.
  
  \item \textbf{Long Element Chain (LEC) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has detected a long chain of method calls on an object.\\
    \textbf{Input:} A Python file with an object undergoing multiple chained calls.\\
    \textbf{Output:} The tool suggests breaking the chain into separate calls and displays the refactored code.\\[2mm]
    \textbf{Test Case Derivation:} Validates that the tool can recognize long element chains and provide suggestions to improve code structure and efficiency.\\[2mm]
    \textbf{How test will be performed:} Use a Python file featuring a long element chain and verify that the tool suggests breaking it apart.
  
  \item \textbf{Long Ternary Conditional Expression (LTCE) RS}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has detected a complex ternary conditional expression.\\
    \textbf{Input:} A Python file with a long ternary expression that is difficult to read.\\
    \textbf{Output:} The tool suggests refactoring the ternary expression into a standard if-else statement and shows the suggested changes.\\[2mm]
    \textbf{Test Case Derivation:} Ensures that the tool identifies long ternary conditional expressions and provides clearer refactoring alternatives.\\[2mm]
    \textbf{How test will be performed:} Submit a Python file containing a long ternary expression to the tool and confirm that it suggests refactoring it into an if-else statement.
  
  \item \textbf{Energy Consumption Measurement for Suggested Refactoring}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has suggested a refactoring for detected code smells.\\
    \textbf{Input:} Suggested refactored code.\\
    \textbf{Output:} Measurement showing improved energy consumption in joules.\\[2mm]
    \textbf{Test Case Derivation:} Confirms suggestions provide measurable energy efficiency improvement, per FR 5.\\[2mm]
    \textbf{How test will be performed:} Apply a refactoring and measure energy consumption before and after.
  
  \item \textbf{Optimal Refactoring Selection}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has identified multiple refactoring options for a single code smell.\\
    \textbf{Input:} Multiple refactoring options.\\
    \textbf{Output:} System chooses the refactoring with the lowest measured energy consumption.\\[2mm]
    \textbf{Test Case Derivation:} Ensures that the tool optimizes for energy efficiency, meeting FR 7.\\[2mm]
    \textbf{How test will be performed:} Provide multiple refactoring options and verify the tool selects the most energy-efficient one.
  
  \item \textbf{RS Not Possible Handling}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool is idle.\\
    \textbf{Input:} Code containing a complex smell that cannot be refactored due to constraints.\\
    \textbf{Output:} The system provides a message indicating that no refactoring suggestions can be made for the identified smell or given code.\\[2mm]
    \textbf{Test Case Derivation:} Ensures the tool gracefully handles situations where refactoring is too complex or not feasible.\\[2mm]
    \textbf{How test will be performed:} Provide a code example that includes a complex smell and observe the output for an appropriate message regarding the lack of suggestions.

  \item \textbf{Selection of Identical Energy Consumption Refactorings}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} The refactoring tool has analyzed a Python code file and identified multiple minimal refactorings.\\
    \textbf{Input:} Two or more refactorings that result in the same minimal energy consumption improvement.\\
    \textbf{Output:} The tool randomly selects one refactoring to apply.\\[2mm]
    \textbf{Test Case Derivation:} This test ensures that when multiple refactorings provide the same energy efficiency gain, the tool correctly implements one of them without preference, thereby fulfilling FR 5.\\[2mm]
    \textbf{How test will be performed:} The tool will be run on a Python file containing code smells. It will be observed whether it selects one of the refactorings with identical energy improvements for application.
\end{enumerate}

\noindent
\colorrule

\subsubsection{Output Validation Tests}
\colorrule

\medskip

\noindent
The following tests are designed to validate that the functionality of the original Python code remains intact after refactoring. Each test ensures that the refactored code passes the same test suite as the original code, confirming compliance with functional requirement FR 3.
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-OV-\arabic*}}, wide=0pt, font=\itshape]
  \label{itm:FR-OV-1}
  \item \textbf{Validate Refactored Code Functionality On Provided Test Suite}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} The original Python code is equipped with an existing test suite it passes.\\
    \textbf{Input:} The original Python code and its associated test suite.\\
    \textbf{Output:} The refactored code passes 100\% of the original test suite.\\[2mm]
    \textbf{Test Case Derivation:} This test confirms that the refactored code preserves the original functionality by passing all tests from the original suite, as stipulated in FR 3.\\[2mm]
    \textbf{How test will be performed:} The tool will refactor the code, and then the original test suite will be executed against the refactored code to check for passing results.
  
  \label{itm:FR-OV-2}
  \item \textbf{Verification of Valid Python Output}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Tool has processed a file with detected code smells.\\
    \textbf{Input:} Output refactored Python code.\\
    \textbf{Output:} Refactored code is syntactically correct and Python-compliant.\\[2mm]
    \textbf{Test Case Derivation:} Ensures refactored code remains valid and usable, satisfying FR 6.\\[2mm]
    \textbf{How test will be performed:} Run a linter on the output code and verify it passes without syntax errors.
  
\end{enumerate}

\newpage

\noindent
\colorrule

\subsubsection{Tests for Reporting Functionality}
\colorrule

\medskip

\noindent
The reporting functionality of the tool is crucial for providing users with comprehensive insights into the refactoring process, including detected code smells, refactorings applied, energy consumption measurements, and the results of the original test suite. This section outlines tests that ensure the reporting feature operates correctly and delivers accurate, well-structured information as specified in the functional requirements (FR 9). 
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-RP-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{A Report With All Components Is Generated}\\[2mm]
    \textbf{Control:} Manual
    \textbf{Initial State:} The tool has completed refactoring a Python code file.\\
    \textbf{Input:} The refactoring results, including detected code smells, applied refactorings, and energy consumption metrics.\\
    \textbf{Output:} A well-structured report is generated, summarizing the refactoring process.\\[2mm]
    \textbf{Test Case Derivation:} This test ensures that the tool generates a comprehensive report that includes all necessary information as required by FR 9.\\[2mm]
    \textbf{How test will be performed:} After refactoring, the tool will invoke the report generation feature and a user can validate that the output meets the structure and content specifications.

  \item \textbf{Validation of Code Smell and Refactoring Data in Report}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} The tool has identified code smells and performed refactorings.\\
    \textbf{Input:} The results of the refactoring process.\\
    \textbf{Output:} The generated report accurately lists all detected code smells and the corresponding refactorings applied.\\[2mm]
    \textbf{Test Case Derivation:} This test verifies that the report includes correct and complete information about code smells and refactorings, in compliance with FR 9.\\[2mm]
    \textbf{How test will be performed:} The tool will compare the contents of the generated report against the detected code smells and refactorings to ensure accuracy.

  \item \textbf{Energy Consumption Metrics Included in Report}\\[2mm]
    \textbf{Control:} Manual
    \textbf{Initial State:} The tool has measured energy consumption before and after refactoring.\\
    \textbf{Input:} Energy consumption metrics obtained during the refactoring process.\\
    \textbf{Output:} The report presents a clear comparison of energy usage before and after the refactorings.\\[2mm]
    \textbf{Test Case Derivation:} This test confirms that the reporting feature effectively communicates energy consumption improvements, aligning with FR 9.\\[2mm]
    \textbf{How test will be performed:} A user will analyze the energy metrics in the report to ensure they accurately reflect the measurements taken during the refactoring.

  \item \textbf{Functionality Test Results Included in Report}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} The original test suite has been executed against the refactored code.\\
    \textbf{Input:} The outcomes of the test suite execution.\\
    \textbf{Output:} The report summarizes the test results, indicating which tests passed and failed.\\[2mm]
    \textbf{Test Case Derivation:} This test ensures that the reporting functionality accurately reflects the results of the test suite as specified in FR 9.\\[2mm]
    \textbf{How test will be performed:} The tool will generate the report and validate that it contains a summary of test results consistent with the actual test outcomes.
\end{enumerate}

\noindent
\colorrule

\subsubsection{Documentation Availability Tests}
\colorrule

\medskip

\noindent
The following test is designed to ensure the availability of documentation as per FR 10.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-DA-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Test for Documentation Availability}\\[2mm]
    \textbf{Control:} Manual
    \textbf{Initial State:} The system may or may not be installed.\\
    \textbf{Input:} User attempts to access the documentation.\\
    \textbf{Output:} The documentation is available and covers installation, usage, and troubleshooting.\\[2mm]
    \textbf{Test Case Derivation:} Validates that the documentation meets user needs (FR 10).\\[2mm]
    \textbf{How test will be performed:} Review the documentation for completeness and clarity.
\end{enumerate}

\noindent
\colorrule

\subsubsection{IDE Extension Tests}
\colorrule

\medskip

\noindent
The following tests are designed to ensure that the user can integrate the tool into VS Code IDE as specified in FR 11 and that the tool works as intended as an extension.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-IE-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Installation of Extension in Visual Studio Code}\\[2mm]
    \textbf{Control:} Manual
    \textbf{Initial State:} The user has Visual Studio Code installed on their machine.\\
    \textbf{Input:} The user attempts to install the refactoring tool extension from the Visual Studio Code Marketplace.\\
    \textbf{Output:} The extension installs successfully, and the user is able to see it listed in the Extensions view.\\[2mm]
    \textbf{Test Case Derivation:} This test validates the installation process of the extension to ensure that users can easily add the tool to their development environment.\\[2mm]
    \textbf{How test will be performed:} 
    \begin{enumerate}[label=\arabic*.]
        \item Open Visual Studio Code.
        \item Navigate to the Extensions view (Ctrl+Shift+X).
        \item Search for the refactoring tool extension in the marketplace.
        \item Click on the "Install" button.
        \item After installation, verify that the extension appears in the installed extensions list.
        \item Confirm that the extension is enabled and ready for use by checking its functionality within the editor.
    \end{enumerate}

  \item \textbf{Running the Extension in Visual Studio Code}\\[2mm]
    \textbf{Control:} Manual
    \textbf{Initial State:} The user has successfully installed the refactoring tool extension in Visual Studio Code.\\
    \textbf{Input:} The user opens a Python file and activates the refactoring tool extension.\\
    \textbf{Output:} The extension runs successfully, and the user can see a list of detected code smells and suggested refactorings.\\[2mm]
    \textbf{Test Case Derivation:} This test validates that the extension can be executed within the development environment and that it correctly identifies code smells as per the functional requirements in the SRS.\\[2mm]
    \textbf{How test will be performed:}
    \begin{enumerate}[label=\arabic*.]
        \item Open Visual Studio Code.
        \item Open a valid Python file that contains known code smells.
        \item Activate the refactoring tool extension using the command palette (Ctrl+Shift+P) and selecting the extension command.
        \item Observe the output panel for the detection of code smells.
        \item Verify that the extension lists the identified code smells and provides appropriate refactoring suggestions.
        \item Confirm that the suggestions are relevant and feasible for the detected code smells.
    \end{enumerate}
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

The section will cover system tests for the non-functional requirements (NFR) listed in the \SRS \hspace{1pt} document\cite{SRS}. The goal for these tests is to address the fit criteria for the requirements. Each test will be linked back to a specific NFR that can be observed in section \ref{trace-sys}.

\noindent
\colorrule

\subsubsection{Look and Feel}

\colorrule

\medskip

\noindent
The following subsection tests cover all Look and Feel requirements listed in the SRS \cite{SRS}. They seek to validate that the system is modern, visually appealing, and supporting of a calm and focused user experience. 
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-LF-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Side-by-side code comparison in IDE plugin} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open in VS Code, with a sample code file loaded \\
    \textbf{Input/Condition:} The user initiates a refactoring operation \\
    \textbf{Output/Result:} The plugin displays the original and refactored code side by side\\[2mm]
    \textbf{How test will be performed:} The tester will open a sample code file within the IDE plugin and apply a refactoring operation. After refactoring, they will verify that the original code appears on one side of the interface and the refactored code on the other, with clear options to accept or reject each change. The tester will interact with the accept/reject buttons to ensure functionality and usability, confirming that users can seamlessly make refactoring decisions with both versions displayed side by side.

  \item \textbf{Theme adaptation in VS Code} \\[2mm]
    \textbf{Type:} Non-functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open in VS Code with either light or dark theme enabled \\
    \textbf{Input/Condition:} The user switches between light and dark themes in VS Code \\
    \textbf{Output/Result:} The plugin’s interface adjusts automatically to match the theme \\[2mm]
    \textbf{How test will be performed:} The tester will open the plugin in both light and dark themes within VS Code by toggling the theme settings in the IDE. They will observe the plugin interface each time the theme is switched, ensuring that the plugin automatically adjusts to match the selected theme without any manual adjustments required. 

  \item \textbf{Colour-coded refactoring indicators for energy savings} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with sample code loaded \\
    \textbf{Input/Condition:} The plugin displays refactoring suggestions based on energy savings \\
    \textbf{Output/Result:} Refactoring suggestions are colour-coded according to energy-saving potential (e.g., yellow for minor savings, red for major savings) \\[2mm]
    \textbf{How test will be performed:} The tester will load sample code with multiple refactoring suggestions based on energy-saving potential and activate the plugin’s analysis feature. The tester will then review each suggestion, confirming that they are visually differentiated by colour codes (e.g., yellow for minor savings and red for major savings). They will interact with each coloured indicator to ensure that it is responsive and accurately represents the suggested energy savings levels.

  \item \textbf{Visual alerts in GitHub Action for significant energy savings} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} GitHub Action enabled for a pull request (PR) \\
    \textbf{Input/Condition:} PR analysis indicates energy savings exceeding a predefined threshold. \\
    \textbf{Output/Result:} A success icon or green label appears in the PR summary \\[2mm]
    \textbf{How test will be performed:} The tester will set up a pull request (PR) with changes that yield significant energy savings. When the GitHub Action completes the analysis, the tester will check the PR summary to confirm that a green label or success icon appears, indicating substantial energy savings. The tester will repeat the test with a PR that does not meet the threshold to ensure no alert is displayed. 

  \item \textbf{Design Acceptance} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} A survey report \\[2mm]
    \textbf{How test will be performed:} After a testing session, developers fill out the survey found in \ref{A.2} evaluating their experience with the plugin.
\end{enumerate}

\noindent
\colorrule
    
\subsubsection{Usability \& Humanity}

\colorrule

\medskip

\noindent
The following subsection tests cover all Usability \& Humanity requirements listed in the SRS \cite{SRS}. They seek to validate that the system is accessible, user-centred, intuitive and easy to navigate.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-UH-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Customizable settings for refactoring preferences} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with settings panel accessible \\
    \textbf{Input/Condition:} User customizes refactoring style and detection sensitivity \\
    \textbf{Output/Result:} Custom configurations save and load successfully \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the settings menu within the tool and adjust various options, including refactoring style, colour-coded indicators, and unit preferences (metric vs. imperial). After each adjustment, the tester will observe if the interface and refactoring suggestions reflect the changes made. 

  \item \textbf{Multilingual support in user guide} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} Bilingual user navigates to system documentation \\
    \textbf{Input/Condition:} User accesses guide in both English and French \\
    \textbf{Output/Result:} The guide is accessible in both languages \\[2mm]
    \textbf{How test will be performed:} The tester will set the tool’s language to French and access the user guide, reviewing each section to ensure accurate translation and readability. After verifying the French version, they will switch the language to English, confirming consistency in content, layout, and clarity between both versions.

  \item \textbf{YouTube installation tutorial availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} User access documentation resources \\
    \textbf{Input/Condition:} User follows the provided link to a YouTube tutorial \\
    \textbf{Output/Result:} Installation tutorial is available and accessible on YouTube, and user successfully installs the system. \\[2mm]
    \textbf{How test will be performed:} The tester will start with the installation instructions provided in the user guide and follow the link to the YouTube installation tutorial. They will watch the video and proceed with each installation step as demonstrated. Throughout the process, the tester will note the clarity and pacing of the instructions, any gaps between the video and the actual steps, and if the video effectively guides them to a successful installation. 

  \item \textbf{High-Contrast Theme Accessibility Check} \\[2mm]
    \textbf{Objective:} Evaluate the high-contrast themes in the refactoring tool for compliance with accessibility standards to ensure usability for visually impaired users. \\
    \textbf{Scope:} Focus on UI components that utilize high-contrast themes, including text, buttons, and backgrounds. \\
    \textbf{Methodology:} Static Analysis \\
    \textbf{Process:} 
    \begin{itemize}
      \item Identify all colour codes used in the system and categorize them by their role in the UI (i.e. background, foreground text, buttons, etc.).
      \item Use tools to measure colour contrast ratios against WCAG thresholds (4.5:1 for normal text, 3:1 for large text)\cite{WCAG}.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Developers implement themes that pass the testing process. \\[2mm]
    \textbf{Tools and Resources:} WebAIM Color Contrast Checker, WCAG guidelines documentation, internal coding standards. \\[2mm]
    \textbf{Acceptance Criteria:} All UI elements must meet WCAG contrast ratios; documentation must accurately reflect theme usage.

  \item \textbf{Audio cues for important actions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with audio cues enabled \\
    \textbf{Input/Condition:} User performs actions triggering audio cues \\
    \textbf{Output/Result:} The system emits an audible attention catching sound. \\[2mm]
    \textbf{How test will be performed:} The tester will enable audio cues in the tool's settings, then perform a series of tasks, such as running code analysis, applying refactorings, and saving changes. Each action should trigger an audio cue indicating task completion or user feedback. The tester will evaluate the volume, timing, and appropriateness of each cue and document whether the cues enhance the user experience or cause any distractions. 

  \item \textbf{Intuitive user interface for core functionality} \\[2mm]
    \textbf{Type:} Non-Functional, User Testing, Dynamic \\
    \textbf{Initial State:} IDE plugin open with code loaded \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} Users can access core functions within three clicks or less \\[2mm]
    \textbf{How test will be performed:} After a testing session, developers fill out the survey found in \ref{A.2} evaluating their experience with the plugin.

  \item \textbf{Clear and concise user prompts} \\[2mm]
    \textbf{Type:} Non-Functional, User Survey, Dynamic \\
    \textbf{Initial State:} IDE plugin prompts user for input \\
    \textbf{Input/Condition:} Users follow on-screen instructions \\
    \textbf{Output/Result:} 90\% of users report the prompts are straightforward and effective \\[2mm]
    \textbf{How test will be performed:} Users complete tasks requiring prompts and answer the survey found in \ref{A.2} on the clarity of guidance provided.

  \item \textbf{Context-sensitive help based on user actions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with help function enabled \\
    \textbf{Input/Condition:} User engages in various actions, requiring guidance \\
    \textbf{Output/Result:} Help resources are accessible within 1-3 clicks \\[2mm]
    \textbf{How test will be performed:} The tester will perform a series of tasks within the tool, such as initiating a code analysis, applying a refactoring, and adjusting settings. At each step, they will access the context-sensitive help option to confirm that the information provided is relevant to the current task. The tester will evaluate the ease of accessing help, the relevance and clarity of guidance, and whether the help content effectively supports task completion.

  \item \textbf{Clear and constructive error messaging} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with possible error scenarios triggered \\
    \textbf{Input/Condition:} User encounters an error during use \\
    \textbf{Output/Result:} 80\% of users report that error messages are helpful and courteous \\[2mm]
    \textbf{How test will be performed:} After receiving error messages, users fill out the survey found in \ref{A.2} on their clarity and constructiveness.
\end{enumerate}

\noindent
\textcolor{Blue}{\colorrule}

\subsubsection{Performance}
\colorrule

\medskip

\noindent
The following subsection tests cover all Performance requirements listed in the SRS \cite{SRS}. These tests validate the tool’s efficiency and responsiveness under varying workloads, including code analysis, refactoring, and data reporting.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-PF-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Performance and capacity validation for analysis and refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} IDE open with multiple python projects of varying sizes ready (1,000, 5,000, 10,000, 100,000 lines of code). \\
    \textbf{Input/Condition:} Initiate the refactoring process for each project sequentially \\
    \textbf{Output/Result:} Process completes within 15 seconds for projects up to 5,000 lines of code, 20 seconds for 10,000 lines of code and within 2 minutes for 100,000 lines of code. \\[2mm]
    \textbf{How test will be performed:} The tester will use four python projects of different sizes: small (1,000 lines), medium (5,000 and 10,000 lines), and large (100,000 lines). For each project, start the refactoring process while running a timer. The scope of the test ends when the system presents the user with the completed refactoring proposal. The time taken for each project is checked against the expected result.

  \item \textbf{Integrity of refactored code against runtime errors} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Refactoring tool ready, with user-provided code and test suite loaded \\
    \textbf{Input/Condition:} User initiates refactoring on the input code \\
    \textbf{Output/Result:} Refactored code passes all tests in the user-provided suite without runtime errors and adheres to Python syntax standards \\[2mm]
    \textbf{How test will be performed:} The refactoring tool will first apply the refactoring to the user-provided code. After refactoring, an automated test suite will run, confirming that all original tests pass, indicating no loss of functionality. The refactored code will then be validated by an automatic linter to ensure compliance with Python syntax standards.

  \item \textbf{Functionality preservation post-refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Python file ready for refactoring with proper configurations for the system \\
    \textbf{Input/Condition:} User initiates refactoring on the code file \\
    \textbf{Output/Result:} The refactored code should pass 100\% of user-provided tests \\[2mm]
    \textbf{How test will be performed:} see test \hyperref[itm:FR-OV-1]{test-FR-OV-1}

  \item \textbf{Accuracy of code smell detection} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Python file containing pre-determined code smells ready for refactoring with proper configurations for the system \\
    \textbf{Input/Condition:} User initiates refactoring on the code file \\
    \textbf{Output/Result:} All code smells determined prior to the test are detected. \\[2mm]
    \textbf{How test will be performed:} see tests in the \hyperref[4.1.2]{Code Smell Detection} section.

  \item \textbf{Valid syntax and structure in refactored code} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} A refactored code file is present in the user's workspace \\
    \textbf{Input/Condition:} A python linter is run on the refactored python file \\
    \textbf{Output/Result:} Refactored code meets Python syntax and structural standards \\[2mm]
    \textbf{How test will be performed:} see test \hyperref[itm:FR-OV-2]{test-FR-OV-2}

  \item \textbf{Handling unexpected inputs} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE open and ready with various non-standard and invalid input files \\
    \textbf{Input/Condition:} User attempts to refactor invalid code files and non-Python files \\
    \textbf{Output/Result:} Tool detects invalid input, displays a clear error message, and does not crash \\[2mm]
    \textbf{How test will be performed:} The tester will sequentially give any of the following invalid files as input to the system :
    \begin{itemize}
        \item Non-Python files (e.g., .txt, .java, .cpp, .js)
        \item Invalid Python files with syntax errors (e.g., unmatched brackets, improper indentation)
        \item Corrupted files that contain random symbols or partially deleted code
    \end{itemize}
    For each file type, the tester will initiate the refactoring process and observe the tool's response. The tool should detect each invalid input, display an error message describing the issue, and recover from the error without crashing. 

  \item \textbf{Fallback Options for Failed Refactoring Attempts} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} The tool is set up in an IDE with a sample code file that includes code smells \\
    \textbf{Input/Condition:} User initiates a refactoring process on the sample code file \\
    \textbf{Output/Result:} The tool logs failed refactoring attempts, provides a clear error notification, and suggests alternative refactoring options without interrupting the overall process. \\[2mm]
    \textbf{How test will be performed:} The tester will load a sample code file into the tool that contains code smells. Upon initiating the refactoring, the tester will observe the tool’s response to any failed attempts, verifying that it logs the error. The tool should then attempt alternative refactorings without restarting the process. The tester will document the clarity of the error message, the relevance of alternative suggestions, and confirm that the tool remains functional, supporting uninterrupted refactoring of other code smells.
  

    \item \textbf{Maintainability and Adaptability of the Tool} \\[2mm]
    \textbf{Objective:} Ensure that the tool’s codebase is structured to support future updates for new Python versions and evolving coding standards, minimizing the effort required for maintenance. \\[2mm]
    \textbf{Scope:} This test applies to the tool’s code structure, documentation quality, and modularity to facilitate adaptability and maintainability over time. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a code walkthrough to evaluate the modular structure of the codebase, verifying that components are organized to allow independent updates.
      \item Review code comments, documentation, and naming conventions to ensure clarity and consistency, supporting ease of understanding for future developers.
      \item Identify any dependencies on specific Python versions and assess the ease of updating these components for compatibility with newer versions.
      \item Document any gaps in modularity or documentation and consult with the development team on improvements to support maintainability.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review and documentation assessment, with the project supervisor overseeing and validating improvements for long-term adaptability. \\[2mm]
    \textbf{Tools and Resources:} Code editor, documentation templates, Python development guidelines, and coding standards \\[2mm]
    \textbf{Acceptance Criteria:} The codebase is modular, well-documented, and adaptable, allowing for straightforward updates with minimal impact on existing functionality.

\end{enumerate}

\noindent
\colorrule

\subsubsection{Operational \& Environmental}
\colorrule

\medskip

\noindent
The following subsection tests cover all Operational and Environmental requirements listed in the SRS \cite{SRS}. Testing includes adherence to emissions standards, integration with environmental metrics, and adaptability to diverse operational settings.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-OPE-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Emissions Standards Compliance} \\[2mm]
    \textbf{Objective:} Ensure that the tool’s emissions metrics and reports align with widely used standards (e.g., GRI 305, GHG, ISO 14064) to support users in environmental compliance and sustainability tracking. \\[2mm]
    \textbf{Scope:} This test applies to the tool's metrics and reporting components, including data format and labelling in the emissions report. \\[2mm]
    \textbf{Methodology:} Static analysis and documentation walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review emissions metrics in the tool’s documentation and compare them with requirements from GRI 305, GHG, and ISO 14064 standards.
      \item Verify that all required emissions metrics from these standards are present in the tool’s reports, with proper format and units.
      \item Confirm that all emissions categories and labels align with standard definitions to ensure consistency and accuracy.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team and project supervisor will conduct the documentation review and patch any discrepancies. \\[2mm]
    \textbf{Tools and Resources:} Tool’s user guide, sample emissions reports, GRI 305, GHG, and ISO 14064 standards documentation \\[2mm]
    \textbf{Acceptance Criteria:} The tool’s emissions metrics meet or exceed the coverage required by GRI 305, GHG, and ISO 14064 standards. All labels and units are accurate, consistent, and aligned with these standards.


  \item \textbf{Integration with GitHub Actions for automated refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} GitHub repository with access to the refactoring library in GitHub Actions \\
    \textbf{Input/Condition:} User sets up a GitHub Actions workflow that calls the refactoring library \\
    \textbf{Output/Result:} GitHub Actions successfully initiates refactoring processes through the library as part of a continuous integration workflow \\[2mm]
    \textbf{How test will be performed:} The tester will configure a GitHub Actions workflow in a test repository, specifying steps to call the refactoring library. After committing a sample code change, the workflow should trigger automatically. The tester will verify that the refactoring library runs within GitHub Actions, completes the refactoring process, and provides feedback in the workflow logs. Successful integration will be confirmed by viewing refactoring results directly within the GitHub Actions logs.

  \item \textbf{VS Code compatibility for refactoring library extension} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} VS Code IDE open and library installed\\
    \textbf{Input/Condition:} User installs and opens the refactoring library extension in VS Code \\
    \textbf{Output/Result:} The refactoring library extension installs successfully and runs within VS Code \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the VS Code marketplace, search for the refactoring library extension, and install it. Once installed, the tester will open the extension and perform a basic refactoring task to ensure the tool operates correctly within the VS Code environment and has access to the system library.

  \item \textbf{Import and export capabilities for codebases and metrics} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with the option to import/export codebases and metrics \\
    \textbf{Input/Condition:} User imports an existing codebase and exports refactored code and metrics reports \\
    \textbf{Output/Result:} The tool successfully imports codebases, refactors them, and exports both code and metrics reports \\[2mm]
    \textbf{How test will be performed:} The tester will load an existing codebase into the tool, initiate refactoring, and select the option to export the refactored code and metrics report. The export should generate files in the selected format. The tester will verify the file formats, check for correct data structure, and validate that the content accurately reflects the refactoring and metrics generated by the tool.

  \item \textbf{PIP package installation availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Python environment ready without the refactoring library installed \\
    \textbf{Input/Condition:} User installs the refactoring library using the command \texttt{pip install ecooptimizer} \\
    \textbf{Output/Result:} The library installs successfully without errors and is available for use in Python scripts \\[2mm]
    \textbf{How test will be performed:} The tester will open a new Python environment and enter the command to install the refactoring library via PIP. Once installed, the tester will import the library in a Python script and execute a basic function to confirm successful installation and functionality. The test verifies the library’s availability and ease of installation for end users.

\end{enumerate}

\noindent
\colorrule

\subsubsection{Maintenance and Support}
\colorrule

\medskip

\noindent
The following subsection tests cover all Maintenance and Support requirements listed in the SRS \cite{SRS}. These tests focus on rollback capabilities, compatibility with external libraries, automated testing, and extensibility for adding new code smells and refactoring functions.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-MS-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Extensibility for New Code Smells and Refactorings} \\[2mm]
    \textbf{Objective:} Confirm that the tool’s architecture allows for the addition of new code smell detections and refactoring techniques with minimal code changes and disruption to existing functionality. \\[2mm]
    \textbf{Scope:} This test applies to the tool’s extensibility, including modularity of code structure, ease of integration for new detection methods, and support for customization. \\[2mm]
    \textbf{Methodology:} Code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a code walkthrough focusing on the modularity and structure of the code smell detection and refactoring components.
      \item Add a sample code smell detection and refactoring function to validate the ease of integration within the existing architecture.
      \item Verify that the new function integrates seamlessly without altering existing features and that it is accessible through the tool’s main interface.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Once the system is complete, the development team will perform the code walkthrough and integration. They will review and approve any structural changes required. \\[2mm]
    \textbf{Tools and Resources:} Code editor, tool’s developer documentation, sample code smell and refactoring patterns \\[2mm]
    \textbf{Acceptance Criteria:} New code smells and refactoring functions can be added within the existing modular structure, requiring minimal changes. The new function does not impact the performance or functionality of existing features.


    \item \textbf{Maintainable and Adaptable Codebase} \\[2mm]
    \textbf{Objective:} Ensure that the codebase is modular, well-documented, and maintainable, supporting future updates and adaptations for new Python versions and standards. \\[2mm]
    \textbf{Scope:} This test covers the maintainability of the codebase, including structure, documentation, and modularity of key components. \\[2mm]
    \textbf{Methodology:} Static analysis and documentation walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase to verify the modular organization and clear separation of concerns between components.
      \item Examine documentation for code clarity and completeness, especially around key functions and configuration files.
      \item Assess code comments and the quality of function/method naming conventions, ensuring readability and consistency for future maintenance.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Once the system is complete, the development team will conduct the code review, to identify areas for improvement. If necessary, they will also ensure to improve the quality of the documentation. \\[2mm]
    \textbf{Tools and Resources:} Code editor, documentation templates, code commenting standards, Python development guides \\[2mm]
    \textbf{Acceptance Criteria:} The codebase is modular and maintainable, with sufficient documentation to support future development. All major components are organized to allow for easy updates with minimal impact on existing functionality.
  
  \item \textbf{Easy rollback of updates in case of errors} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Latest version of the tool installed with the ability to apply and revert updates \\
    \textbf{Input/Condition:} User applies a simulated new update and initiates a rollback \\
    \textbf{Output/Result:} The system reverts to the previous stable state without any errors \\[2mm]
    \textbf{How test will be performed:} The tester will apply a simulated update. Following this, they will initiate the rollback function, which should restore the tool to its previous stable version. The tester will verify that all features function as expected post-rollback and document the time taken to complete the rollback process
\end{enumerate}

\newpage

\noindent
\colorrule

\subsubsection{Security}
\colorrule

\medskip

\noindent
The following subsection tests cover all Security requirements listed in the SRS \cite{SRS}. These tests seek to validate that the tool is protected against unauthorized access, data breaches, and external threats.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-SRT-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{User authentication before accessing tool features} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} System installed, user unauthenticated \\
    \textbf{Input/Condition:} User attempts to submit code or view refactoring reports \\
    \textbf{Output/Result:} Access is denied \\[2mm]
    \textbf{How test will be performed:} The tester will first attempt to submit code and access refactored reports without logging in, verifying that access is denied. The tester will then log in using valid company credentials and repeat the actions to confirm access is granted only after successful authentication.
  
  \item \textbf{Internal-Only Communication with Energy and Reinforcement Learning Tools} \\[2mm]
    \textbf{Objective:} Ensure that the refactoring tool communicates exclusively with the internal energy consumption tool and reinforcement learning model, without exposing any public API endpoints. \\[2mm]
    \textbf{Scope:} This test applies to all network and API interactions between the refactoring tool and internal services, ensuring no direct access is available to users or external applications. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a code walkthrough of the network and API components, focusing on the access control configurations for the energy consumption tool and reinforcement learning model.
      \item Inspect the code for any exposed API endpoints or network configurations that might allow external access.
      \item Attempt to access the internal tools directly from an external environment, ensuring that all external attempts are blocked.
      \item Verify that the tool’s communication is contained within internal environments and restricted to authorized system components.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review and testing, ensuring secure access protocols. \\[2mm]
    \textbf{Tools and Resources:} Access to the codebase, network configuration files, and security audit tools \\[2mm]
    \textbf{Acceptance Criteria:} No public or external API endpoints exist for the internal tools, and only the refactoring tool can access the energy consumption and reinforcement learning models.
  
    \item \textbf{Preventing Unauthorized Changes to Refactored Code and Reports} \\[2mm]
    \textbf{Objective:} Ensure the tool’s refactored code and energy reports are protected from any unauthorized external modifications, maintaining data integrity and user trust. \\[2mm]
    \textbf{Scope:} This test applies to the data security of refactored code and energy report storage layers, verifying that access is restricted to authorized users and processes only. \\[2mm]
    \textbf{Methodology:} Static analysis and code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase and database configurations to verify the implementation of access controls and data security measures.
      \item Confirm that the tool’s security settings prevent any unauthorized external modifications, maintaining data integrity across all storage layers.
      \item Document any vulnerabilities found and evaluate with the development team to ensure improvements are made where necessary.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review while the project supervisor will oversee the test results and approve any necessary security enhancements. \\[2mm]
    \textbf{Tools and Resources:} Access to security configuration files, code editor \\[2mm]
    \textbf{Acceptance Criteria:} The review attendees find no egregious faults within the system that might allow unauthorized external access or modifications to refactored code and energy report data.

  \item \textbf{Notification and consent for data handling} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} System idle \\
    \textbf{Input/Condition:} User initiates refactoring on their source code \\
    \textbf{Output/Result:} Tool displays data handling notice and requests explicit consent before data collection \\[2mm]
    \textbf{How test will be performed:} The tester will begin the refactoring process, and the tool should present a notice explaining data collection, storage, and processing practices, in compliance with PIPEDA. The user must provide explicit consent before proceeding. The tester will confirm that no data collection occurs until consent is granted.
  
  \item \textbf{Confidential Handling of User Data in Compliance with PIPEDA} \\[2mm]
    \textbf{Objective:} Ensure that all user-submitted data, energy reports, and refactored code are treated as confidential, encrypted during storage and transmission, and managed according to PIPEDA. \\[2mm]
    \textbf{Scope:} This test applies to the tool’s data handling practices, specifically the encryption protocols for transmission and storage, and data modification options for user compliance requests. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the encryption settings in the codebase to confirm that all data related to user submissions, energy reports, and refactored code is encrypted during transmission and storage.
      \item Verify that an option is available for users to request modifications to their personal data as per PIPEDA requirements.
      \item Document any gaps in data security or user request handling, and collaborate with the development team to implement improvements as needed.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review and implement any necessary improvements, with the project supervisor overseeing the compliance with PIPEDA standards. \\[2mm]
    \textbf{Tools and Resources:} Access to encryption libraries, security configuration files \\[2mm]
    \textbf{Acceptance Criteria:} All user data is encrypted during storage and transmission, and users have a reliable method for requesting data modifications as per PIPEDA specifications.

  \item \textbf{Audit Logs for User Actions} \\[2mm]
    \textbf{Objective:} Ensure the tool maintains tamper-proof logs of key user actions, including code submissions, login events, and access to refactored code and reports, to ensure accountability and traceability. \\[2mm]
    \textbf{Scope:} This test applies to the logging mechanisms for user actions, focusing on the security and tamper-proof nature of logs. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the logging mechanisms within the codebase to confirm that events such as logins, code submissions, and report accesses are properly recorded with timestamps and user identifiers.
      \item Document the integrity of the logs and any vulnerabilities found, and collaborate with the development team on any necessary improvements.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review, with oversight by the project supervisor to verify that logging mechanisms meet security requirements. \\[2mm]
    \textbf{Tools and Resources:} Access to log files, logging library documentation, security testing tools \\[2mm]
    \textbf{Acceptance Criteria:} Logs are tamper-proof, recording all critical user actions with integrity, and resistant to unauthorized modifications.

  \item \textbf{Audit Logs for Refactoring Processes} \\[2mm]
    \textbf{Objective:} Ensure that the tool maintains a secure, tamper-proof log of all refactoring processes, including pattern analysis, energy analysis, and report generation, for accountability in refactoring events. \\[2mm]
    \textbf{Scope:} This test covers the logging of refactoring events, ensuring logs are complete and tamper-proof for future auditing needs. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase to confirm that each refactoring event (e.g., pattern analysis, energy analysis, report generation) is logged with details such as timestamps and event descriptions.
      \item Document any logging gaps or security vulnerabilities, and consult with the development team to implement enhancements.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will review and test the logging mechanisms, with the project supervisor ensuring alignment with auditing requirements. \\[2mm]
    \textbf{Tools and Resources:} Access to logging components, tamper-proof logging tools \\[2mm]
    \textbf{Acceptance Criteria:} All refactoring processes are logged in a secure, tamper-proof manner, ensuring complete traceability for future audits.

  \item \textbf{Immunity Against Malware and Unauthorized Programs} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} The tool is deployed in a controlled test environment with security protocols enabled, ready to be tested against simulated malware attacks. \\
    \textbf{Input/Condition:} Simulated malware attacks are executed using Atomic Red Team by Red Canary\cite{ARTCanary}, targeting vulnerabilities such as unauthorized data access, process interference, and data tampering. \\
    \textbf{Output/Result:} The tool detects, blocks, and logs all simulated malware activities without any compromise to data integrity or tool functionality. \\[2mm]
    \textbf{How test will be performed:} The tester will deploy the tool in a secure, isolated test environment and initiate simulated malware attacks using Atomic Red Team. Each simulation will mimic various malware behaviours, including attempts to access or modify data and disrupt the refactoring process. The tester will observe and document the tool's responses to each simulated attack, verifying that it blocks unauthorized actions, maintains data integrity, and logs the events for traceability.
\end{enumerate}

\newpage

\noindent
\colorrule

\subsubsection{Cultural}
\colorrule

\medskip

\noindent
The following subsection tests cover all Cultural requirements listed in the SRS \cite{SRS}. These test are to ensure that the tool is accessible and appropriate for a global audience, avoiding any culturally sensitive or inappropriate elements. 

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-CULT-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Cultural sensitivity of icons and colours} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give an answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} Users complete tasks requiring prompts and answer the survey found in \ref{A.2} on cultural sensitivity of the interface design.

  \item \textbf{Support for metric and imperial units} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Tool ready with energy consumption metrics displayed in the default unit system \\
    \textbf{Input/Condition:} User toggles the measurement units between metric and imperial \\
    \textbf{Output/Result:} Energy consumption measurements update correctly between metric and imperial units \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the settings, locate the measurement unit toggle, and switch between metric and imperial units. After each toggle, the displayed energy consumption data should reflect the correct measurement units. The tester will validate accuracy by comparing values against known conversions to ensure the toggle functions accurately and smoothly.

  \item \textbf{Cultural sensitivity of content} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} More than 60\% of users give an answer greater than 3 the survey question targeting this test. \\[2mm]
    \textbf{How test will be performed:} Users complete tasks requiring prompts and answer the survey found in \ref{A.2} on cultural sensitivity of the content of the system.
\end{enumerate}

\newpage

\noindent
\colorrule

\subsubsection{Compliance}
\colorrule

\medskip

\noindent
The following subsection tests cover all Compliance requirements listed in the SRS \cite{SRS}. The tests focus on adherence to PIPEDA, CASL, and ISO 9001, as well as SSADM standards, ensuring the tool complies with relevant regulations and aligns with professional development practices.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-CPL-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Compliance with PIPEDA and CASL} \\[2mm]
    \textbf{Objective:} Ensure the tool’s data collection, usage, storage, and communication practices are fully compliant with the Personal Information Protection and Electronic Documents Act (PIPEDA) and Canada’s Anti-Spam Legislation (CASL), to avoid legal penalties and enhance user trust. \\[2mm]
    \textbf{Scope:} This test applies to all processes related to data handling, storage, and user communication to verify compliance with PIPEDA and CASL. \\[2mm]
    \textbf{Methodology:} Documentation walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the tool’s data handling and storage protocols to confirm compliance with PIPEDA, particularly focusing on secure storage, data usage transparency, and privacy rights.
      \item Verify the presence of a user consent mechanism that informs users of data collection and provides options for managing their data.
      \item Inspect communication practices to ensure compliance with CASL, confirming that the tool provides users with notification and opt-in options for all communications.
      \item Document any gaps in compliance and consult with the development team for required adjustments.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the compliance review and implement any necessary updates. \\[2mm]
    \textbf{Tools and Resources:} Access to documentation on PIPEDA and CASL requirements, tool’s data handling and communication protocols, test user accounts for opt-in verification \\[2mm]
    \textbf{Acceptance Criteria:} The tool complies with all PIPEDA and CASL requirements, with secure data handling, user consent options, and compliant communication practices.

\item \textbf{Compliance with ISO 9001 and SSADM Standards} \\[2mm]
    \textbf{Objective:} Ensure the tool’s quality management and software development processes align with ISO 9001 for quality management and SSADM (Structured Systems Analysis and Design Method) standards for software development, building stakeholder trust and market acceptance. \\[2mm]
    \textbf{Scope:} This test covers the tool’s adherence to ISO 9001 quality management practices and SSADM methodologies for software development processes. \\[2mm]
    \textbf{Methodology:} Documentation walkthrough and code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a review of the tool’s quality management procedures to verify alignment with ISO 9001 standards, including documentation, testing, and feedback mechanisms.
      \item Examine software development workflows to confirm adherence to SSADM standards, focusing on design, analysis, and structured development practices.
      \item Identify any deviations from ISO 9001 and SSADM requirements, document these findings, and discuss necessary adjustments with the development team.
      \item Validate improvements in quality management and software development after implementing recommendations.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the standards compliance review, and the project supervisor will oversee the review process. \\[2mm]
    \textbf{Tools and Resources:} Access to ISO 9001 and SSADM standards documentation, project quality management records, and development workflows \\[2mm]
    \textbf{Acceptance Criteria:} The tool’s quality management and software development processes fully adhere to ISO 9001 and SSADM standards, supporting a high-quality, structured approach to development.
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements} \label{trace-sys}

\begin{table}[H]
  \centering
  \caption{Functional Requirements and Corresponding Test Sections}
  \begin{tabular}{|p{0.6\textwidth}|p{0.3\textwidth}|}
    \toprule \textbf{Section} & \textbf{Functional Requirement} \\ 
    
    \midrule
    Input Acceptance Tests & FR 1 \\ \hline
    Code Smell Detection Tests & FR 2 \\ \hline
    Refactoring Suggestion Tests & FR 4 \\ \hline
    Output Validation Tests & FR 3, FR 6 \\ \hline
    Tests for Report Generation & FR 9 \\ \hline
    Documentation Availability Tests & FR 10 \\ \hline
    IDE Integration Tests & FR 11 \\
    \bottomrule
  \end{tabular}
  \label{tab:sections_requirements}
\end{table}

\label{tab:nfr-trace-reqs}
\begin{table}[H]
  \centering
  \caption{Look \& Feel Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Look and Feel
    LF-1 & LFR-AP 1 \\ 
    LF-2 & LFR-AP 2 \\ 
    LF-3 & LFR-AP 3 \\ 
    LF-4 & LFR-AP 5 \\ 
    LF-5 & LFR-AP 4, LFR-ST 1-3 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Usability \& Humanity Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Usability and Humanity
    UH-1 & UHR-PS1 1 \\ 
    UH-2 & UHR-PS1 2, MS-SP 1 \\ 
    UH-3 & UHR-LRN 2 \\ 
    UH-4 & UHR-ACS 1 \\ 
    UH-5 & UHR-ACS 2 \\ 
    UH-6 & UHR-EOU 1 \\ 
    UH-7 & UHR-EOU 2 \\ 
    UH-8 & UHR-LRN 1 \\ 
    UH-9 & UHR-UPL 1 \\ 
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Performance Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Performance
    PF-1 & PR-SL 1, PR-SL 2, PR-CR 1 \\ 
    PF-2 & PR-SCR 1 \\ 
    PF-3 & PR-PAR 1 \\ 
    PF-4 & PR-PAR 2 \\ 
    PF-5 & PR-PAR 3 \\ 
    PF-6 & PR-RFT 1 \\ 
    PF-7 & PR-RFT 2 \\ 
    PF-8 & PR-LR 1, MS-MNT 5 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Operational \& Environmental Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Operational and Environmental
    Not explicitly tested & OER-EP 1 \\
    Not explicitly tested & OER-EP 2 \\
    OPE-1 & OER-WE 1 \\
    OPE-2 & OER-IAS 1 \\
    OPE-3 & OER-IAS 2 \\
    OPE-4 & OER-IAS 3 \\
    OPE-5 & OER-PR 1 \\
    Tested by FRs & OER-RL 1 \\
    Not explicitly tested & OER-RL 2 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Maintenance \& Support Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Maintenance and Support
    MS-1 & MS-MNT 1, PR-SER 1 \\
    MS-2 & MS-MNT 2 \\
    MS-3 & MS-MNT 3 \\
    Not explicitly tested & MS-MNT 4 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Security Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Security
    SRT-1 & SR-AR 1 \\
    SRT-2 & SR-AR 2 \\
    SRT-3 & SR-IR 1 \\
    SRT-4 & SR-PR 1 \\
    SRT-5 & SR-PR 2 \\
    SRT-6 & SR-AUR 1 \\
    SRT-7 & SR-AUR 2 \\
    SRT-8 & SR-IM 1 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Cultural Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Cultural
    CULT-1 & CULT 1 \\
    CULT-2 & CULT 2 \\
    CULT-3 & CULT 3 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Compliance Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Compliance
    CPL-1 & CL-LR 1 \\
    CPL-2 & CL-SCR 1 \\ 
    \bottomrule
  \end{tabular}
\end{table}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially lay out your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

This will be done after the Detailed Design.

% \subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}


% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item \textbf{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% \textbf{Initial State:} 
					
% \textbf{Input:} 
					
% \textbf{Output:} \wss{The expected result for the given inputs}

% \textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}

% \textbf{How test will be performed:} 
					
% \item \textbf{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% \textbf{Initial State:} 
					
% \textbf{Input:} 
					
% \textbf{Output:} \wss{The expected result for the given inputs}

% \textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}

% \textbf{How test will be performed:} 

% \item \textbf{...\\}
    
% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module 1}
		
% \begin{enumerate}

% \item \textbf{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% \textbf{Initial State:} 
					
% Input/Condition: 
					
% Output/Result: 
					
% \textbf{How test will be performed:} 
					
% \item \textbf{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% \textbf{Initial State:} 
					
% \textbf{Input:} 
					
% Output: 
					
% \textbf{How test will be performed:} 

% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}


\subsubsection{Refactoring Controller}

\textbf{Goal:} These tests verify the behavior of the RefactorerController in various scenarios, ensuring that it handles refactoring correctly and interacts with external components as expected.

\begin{itemize}
\item \textbf{Verifying Successful Refactoring with a Valid Refactorer} \newline
Verifies that the RefactorerController correctly runs the refactorer when a valid refactorer is available. It checks that the logger captures the refactoring event, the refactor method is called with the expected arguments, and the modified file path is correctly generated.

\item \textbf{Handling Case When No Refactorer is Found} \newline
Ensures that if no refactorer is found for a given smell, the RefactorerController raises a NotImplementedError. It also confirms that the appropriate error message is logged.

\item \textbf{Handling Multiple Refactorer Calls for the Same Smell} \newline
Tests the behavior of the RefactorerController when the refactorer is called multiple times for the same smell. It ensures that the smell counter is updated correctly and that unique file names are generated for each call.

\item \textbf{Verifying the Behavior When Overwrite is Disabled} \newline
Verifies that when the overwrite flag is set to false, the refactor method is called with the correct argument, ensuring that files are not overwritten when this option is disabled.

\item \textbf{Handling Case Where No Files are Modified} \newline
Checks that when no files are modified by the refactorer, the RefactorerController correctly returns an empty list of modified files.
\end{itemize}


\subsubsection{Long Element Chain Detection}

\textbf{Goal:} The long element chain detection module identifies excessive dictionary access chains that exceed a predefined threshold (default: 3). The following unit tests verify the detection capabilities. \

The tests evaluate the detection of long element chains by verifying that sequences exceeding the threshold, such as deep dictionary accesses, are correctly identified. They include edge cases, such as chains just below the threshold, variations in data structures, and multiple chains in the same scope, to assess the robustness of the detection logic.

\begin{itemize}
\item \textbf{Ignores code with no chains} \newline
Ensures that code without any nested element chains does not trigger a detection.
\item \textbf{Ignores chains below the threshold} \newline
Verifies that element chains shorter than the threshold are not flagged.

\item \textbf{Detects chains exactly at threshold} \newline
Ensures that an element chain with a length equal to the threshold is flagged.

\item \textbf{Detects chains exceeding the threshold} \newline
Verifies that chains longer than the threshold are correctly detected.

\item \textbf{Detects multiple chains in the same file} \newline
Ensures that multiple long element chains appearing in different locations within the same code file are individually detected.

\item \textbf{Detects chains inside nested functions and classes} \newline
Confirms that element chains occurring within functions and class methods are correctly identified.

\item \textbf{Reports identical chains on the same line only once} \newline
Ensures that duplicate chains appearing on a single line are not reported multiple times.

\item \textbf{Handles different variable types in chains} \newline
Verifies detection of chains that involve a mix of dictionaries, lists, tuples, and nested structures.

\item \textbf{Correctly applies custom threshold values} \newline
Ensures that adjusting the threshold parameter correctly affects detection behavior.

\item \textbf{Verifies result structure and metadata} \newline
Confirms that detected chains return correctly formatted results, including message ID, type, and occurrence details.
\end{itemize}

\subsubsection{Long Element Chain Refactoring}

\textbf{Goal:} The long element chain refactoring module simplifies deeply nested dictionary accesses by flattening them into top-level keys while preserving functionality. The following unit tests verify the correctness of this transformation.

The tests assess the ability to detect and refactor long element chains by verifying correct dictionary transformations, accessing pattern updates, and handling of various data structures. Edge cases, such as shallow accesses, multiple affected files, and mixed depths of access, are included to ensure robustness.

\begin{itemize}
\item \textbf{Identifies and refactors basic nested dictionary access} \newline
Ensures that deeply nested dictionary keys are detected and refactored correctly.

\item \textbf{Refactors dictionary accesses across multiple files} \newline
Verifies that dictionary changes propagate correctly when a deeply nested dictionary is accessed in different files.

\item \textbf{Handles dictionary access via class attributes} \newline
Ensures that nested dictionary accesses within class attributes are correctly detected and refactored.

\item \textbf{Ignores shallow dictionary accesses} \newline
Verifies that dictionary accesses below the predefined threshold remain unchanged.

\item \textbf{Handles multiple long element chains in the same file} \newline
Ensures that all occurrences of excessive dictionary accesses in a file are refactored individually.

\item \textbf{Detects and refactors mixed access depths} \newline
Confirms that the module correctly differentiates and processes deep accesses while ignoring shallow ones.

\item \textbf{Validates resulting metadata and formatting} \newline
Confirms that the refactored output includes well-structured results, such as correct message IDs, types, and occurrences.
\end{itemize}


\subsection{Plugin Tests}

\subsubsection{HoverManager Tests}

\textbf{Goal:} These tests verify the behavior of the HoverManager class, ensuring that hover content is correctly generated for detected smells and that refactor commands function as expected.

\begin{itemize}
\item \textbf{Verifying Hover Content for Detected Smells} \newline
Ensures that hover content is generated for detected smells on the current line in Python files. The test verifies that the correct smells are identified and that the hover content is formatted with links to refactor the smell or all smells of the same type.

\item \textbf{Verifying Hover Content When No Smells Are Present} \newline
Tests the scenario when no smells are present on the current line in the document. In this case, it ensures that the hover content is null, meaning that no hover information is displayed.

\item \textbf{Verifying the Correctness of Refactor Command for a Selected Smell} \newline
Verifies that the refactor command for a selected smell correctly triggers the `refactorSelectedSmell` function with the appropriate context and smell data. This tests that the specific refactoring action is executed when the user selects a smell to refactor.

\item \textbf{Verifying the Correctness of Refactor Command for All Smells of a Type} \newline
Ensures that the refactor command for all smells of a specific type triggers the `refactorAllSmellsOfType` function with the correct context and messageId. This tests that the refactoring action is performed for all occurrences of the same type of smell in the document.
\end{itemize}




\bibliography{../../refs/References}

\newpage

\begin{appendices}

\section{Appendix}

\wss{This is where you can place additional information.}

\subsection{Symbolic Parameters}

Not applicable at the moment.

\subsection{Usability Survey Questions} \label{A.2}

\subsubsection*{Minimalist Design}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item The tool's interface feels uncluttered, showing only essential elements.
  \item I am able to focus on refactoring tasks without unnecessary distractions (rate 1-5).
\end{enumerate}

\subsubsection*{Professional \& Authoritative Appearance}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item The tool has a professional appearance that instills confidence in its functionality.
  \item I would feel comfortable recommending this tool to other professionals based on its visual design.
\end{enumerate}

\subsubsection*{Calm and Focused Atmosphere}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item The design of the tool creates a calm environment that supports my concentration on refactoring tasks.
  \item The colours and layout used in the tool help reduce distractions and maintain my focus.
\end{enumerate}

\subsubsection*{Modern and Visually Appealing Design}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item The tool's design is modern and aligns well with contemporary software development tools.
  \item The interface design is aesthetically pleasing and enjoyable to use.
\end{enumerate}

\subsubsection*{Intuitive UI and Ease of Use}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item The tool’s interface is intuitive and easy to navigate.
  \item I can quickly find key features and settings within the tool.
\end{enumerate}

\subsubsection*{Clear and Concise Prompts}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item The prompts and instructions in the tool are clear and easy to understand.
  \item The tool’s prompts guide me effectively through processes.
\end{enumerate}

\subsubsection*{Context-Sensitive Help}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item Help resources are easy to access and relevant to my current actions.
  \item The tool provides useful help based on the task I am performing.
\end{enumerate}

\subsubsection*{Clear and Constructive Error Messaging}
Please rate each statement on a scale of 1 to 5, where 1 = Strongly Disagree and 5 = Strongly Agree.
\begin{enumerate}
  \item The error messages in the tool are helpful and clearly explain the issue.
  \item The tool provides constructive guidance on resolving errors I encounter.
\end{enumerate}

\subsubsection*{Cultural sensitivity}
\begin{enumerate}
  \item What is your ethnicity or cultural background? (This question is optional and helps us understand how the tool is perceived across different cultures.)
  \begin{itemize}
    \item African or African diaspora (e.g., African American, Afro-Caribbean)
    \item East Asian (e.g., Chinese, Japanese, Korean)
    \item South Asian (e.g., Indian, Pakistani, Bangladeshi)
    \item Southeast Asian (e.g., Filipino, Vietnamese, Thai)
    \item Middle Eastern or North African (MENA)
    \item Hispanic or Latino/a
    \item Indigenous or Native (e.g., Native American, First Nations, Aboriginal)
    \item Pacific Islander
    \item European or White/Caucasian
    \item Mixed or Multi-ethnic
    \item Prefer not to answer
    \item Other (please specify): [Open text field] 
  \end{itemize}
  \item Did you encounter any language, imagery, or content that felt insensitive?
  \begin{itemize}
    \item No
    \item Yes (Elaborate)
  \end{itemize}
\end{enumerate}

\subsubsection*{Overall Feedback}
\begin{enumerate}
  \item Are there any design improvements you would suggest? (optional)
  \item What do you like most about the tool’s design? (optional)
\end{enumerate}

\newpage{}
\section{Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}