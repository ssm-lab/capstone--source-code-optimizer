\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{paralist}
\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=black,
  linkcolor=red,
  urlcolor=blue
}

% Test

%Includes "References" in the table of contents
\usepackage[nottoc]{tocbibind}
\usepackage[toc,page]{appendix}
\usepackage[square,numbers,compress]{natbib}
\usepackage{placeins}
\bibliographystyle{abbrvnat}

\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[letterpaper, portrait, margin=1in]{geometry}
\usepackage[dvipsnames]{xcolor}

\usepackage{float}

\input{../Comments}
\input{../Common}

\newcommand{\SRS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/SRS/SRS.pdf}{SRS}}
\newcommand{\MG}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}}
\newcommand{\MIS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}}

\newcommand{\colorrule}{\textcolor{BlueViolet}{\rule{\linewidth}{2pt}}}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{4cm}p{4cm}X}
  \toprule {\bf Date} & {\bf Name} & {\bf Notes}\\
  \midrule
  November 4th, 2024 & All & Created initial revision of VnV Plan\\
January 3rd, 2025 & Sevhena Walker & Modified template for static tests, clarified test-SRT-3\\
  March 10th, 2025 & Nivetha Kuruparan, Sevhena Walker & Revised Functional and Non-Functional Requirements\\
  April 1st, 2025 & Sevhena Walker & Modified Implementation Verification plan: refined unit testing tools.\\
  April 1st, 2025 & Sevhena Walker & Updated Automated testing and Verification Tools to include plugin tools.\\
  April 1st, 2025 & Sevhena Walker & Updated 4.2 and 4.5.2 to reflect the new tests.\\
  April 1st, 2025 & Sevhena Walker & Fixed some links. Fixed table formatting and some grammar.\\
  April 3rd, 2025 & Nivetha Kuruparan & Major Revisions for General Information Section\\
  April 3rd, 2025 & Nivetha Kuruparan & Major Revisions for Plan Section\\
  April 3rd, 2025 & Nivetha Kuruparan & Major Revisions for Test Functional Requirements Section\\
  April 3rd, 2025 & Nivetha Kuruparan & Major Revisions for Test Non-Functional Requirements Section\\
  April 3rd, 2025 & Nivetha Kuruparan & Heavily Revised VSCode Plugin Unit Tests\\
  April 3rd, 2025 & Nivetha Kuruparan & Fixed Links\\
  \bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
  However, this does not mean listing every verification and
  validation technique
  that has ever been devised.  The VnV plan should also be a \textbf{feasible}
  plan. Execution of the plan should be possible with the time and
  team available.
  If the full plan cannot be completed during the time available, it
  can either be
  modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
  the design stage.  This means that the sections related to unit testing cannot
  initially be completed.  The sections will be filled in after the design stage
  is complete.  the final version of the VnV plan should have all
  sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

% \section{Symbols, Abbreviations, and Acronyms}

% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{l l}
%   \toprule
%   \textbf{symbol} & \textbf{description}\\
%   \midrule
%   T & Test\\
%   \bottomrule
% \end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%  ~\cite{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

% \newpage

\pagenumbering{arabic}

\noindent This document outlines the process and methods to ensure that the
software meets its requirements and functions as intended. This
document provides a structured approach to evaluating the product,
incorporating both verification (to confirm that the software is
built correctly) and validation (to confirm that the correct software
has been built). By systematically identifying and mitigating
potential issues, the V\&V process aims to enhance quality, reduce
risks, and ensure compliance with both functional and non-functional
requirements.\\

\noindent The following sections will go over the approach for verification and
validation, including the team structure, verification strategies at
various stages, and tools to be employed. Furthermore, a detailed list
of system and unit tests are also included in this document.

\section{General Information}

\subsection{Summary}

The software being tested is called EcoOptimizer. EcoOptimizer is a Visual Studio Code (VS Code) extension designed to help developers identify and refactor energy-inefficient code in Python. It uses a Python package as its backend to detect code smells and estimate the carbon emissions that can be saved by applying the suggested refactorings. Users are given the option to accept or reject these refactorings directly within the editor. EcoOptimizer aims to promote energy-aware software development by making it easier to write efficient code without altering the original functionality.

\subsection{Objectives}

The primary objective of this project is to build confidence in the correctness and energy efficiency of the EcoOptimizer system, ensuring it reliably identifies and refactors energy-inefficient Python code without altering its original behaviour. Usability is a major focus—particularly within the VS Code extension—as the tool is designed to suggest improvements and keep developers informed while they write code. A core feature of EcoOptimizer is giving users the autonomy to accept or reject refactorings, empowering them to make informed decisions about the changes being applied to their code. These qualities—correctness, energy efficiency, user control, and usability—are essential to the project’s success, as they shape the overall user experience, practical adoption, and sustainable benefits of the tool.\\

\noindent Certain objectives have been intentionally excluded due to resource constraints. For instance, we will not independently verify third-party libraries or dependencies, and will assume that they are adequately tested and maintained by their original developers.

\subsection{Challenge Level and Extras}

Our project, set at a general challenge level, includes two additional focuses: the creation of a user manual and the completion of a usability report. The user manual provides clear and accessible instructions for developers, covering installation, setup, and usage of both the VS Code extension and the underlying Python package. It is designed to help users quickly integrate the tool into their development workflow. The usability report summarizes findings from a structured usability testing session, offering valuable insights into how effectively the tool meets user needs. These findings are used to refine the user interface and improve the overall user experience, ensuring the tool is both intuitive and practical for real-world use.

\subsection{Relevant Documentation}

The Verification and Validation (VnV) plan relies on three key documents to guide testing and assessment:
\begin{itemize}
  \item[] \textbf{Software Requirements Specification
    (\SRS)~\cite{SRS}:} The foundation for the VnV plan, as it
    defines the functional and non-functional requirements the
    software must meet; aligning tests with these requirements
    ensures that the software performs as expected in terms of
    correctness, performance, and usability.

  \item[] \textbf{Module Interface Specification (\MG)~\cite{MGDoc}:}
    Provides detailed information about each module's interfaces,
    which is crucial for integration testing to verify that all
    modules interact correctly within the system.

  \item[] \textbf{Module Guide (\MIS)~\cite{MISDoc}:} Outlines the
    system's architectural design and module structure, ensuring the
    design of tests that align with the intended flow and
    dependencies within the system.
\end{itemize}

\newpage\section{Plan}

The following section outlines the comprehensive Verification and
Validation (VnV) strategy, detailing the team structure, specific
plans for verifying the Software Requirements Specification (SRS),
design, implementation, and overall VnV process, as well as the
automated tools employed and the approach to software validation.

\subsection{Verification and Validation Team}

The Verification and Validation (VnV) Team for the EcoOptimizer project consists of the following members and their specific roles:

\begin{itemize}
  \item \textbf{Sevhena Walker}: Lead Tester. Oversees and
    coordinates the testing process, ensuring all feedback is applied
    and all project goals are met.
  \item \textbf{Mya Hussain}: Functional Requirements Tester. Tests
    the software to verify that it meets all specified functional requirements.
  \item \textbf{Ayushi Amin}: Integration Tester. Focuses on testing
    the connection between the various components of the Python
    package, the VSCode plugin, and the GitHub Action to ensure
    seamless integration.
  \item \textbf{Tanveer Brar}: Non-Functional Requirements Tester.
    Assesses performance/security compliance with project standards.
  \item \textbf{Nivetha Kuruparan}: Non-Functional Requirements
    Tester. Ensures that the final product meets user expectations
    regarding user experience and interface intuitiveness.
  \item \textbf{Istvan David} (Supervisor): Supervises the overall
    VnV process, providing feedback and guidance based on industry
    standards and practices.
\end{itemize}

\subsection{SRS Verification Plan}

The verification of the Software Requirements Specification will be conducted through a structured four-phase approach encompassing requirements validation, stakeholder review, user acceptance testing, and continuous verification.

\subsubsection{Requirements Validation}
The validation process will begin with comprehensive test coverage of all functional and non-functional requirements. A suite of test cases will be developed, employing automated testing frameworks such as PyTest~\cite{pytest} for functional requirements while reserving manual testing for edge cases and complex scenarios. To ensure complete traceability, we will maintain a matrix that explicitly links each requirement to its corresponding implementation, test cases, and test results.

\subsubsection{Stakeholder Review}
A formal review session will be conducted with our project supervisor, Dr. Istvan David, following a structured agenda. Prior to the meeting, we will prepare and distribute a package containing: (1) a summary of key requirements and design decisions, (2) visual aids including requirement diagrams and tables, and (3) specific questions regarding high-risk areas. During the session, we will conduct a detailed walkthrough of critical system components, particularly focusing on the energy measurement module and refactoring logic.

\subsubsection{User Acceptance Testing}
The verification process will include rigorous user acceptance testing involving 5--10 representative developers from our target user base. These tests will validate both the usability requirements (ensuring tasks can be completed within the \textbf{MAX\_TASK\_CLICKS} threshold of 4) and the effectiveness of energy metrics presentation (as specified in \textbf{FR6}). Feedback from these sessions will be systematically analyzed to identify any discrepancies between the SRS and actual user expectations.

\subsubsection{Continuous Verification}
To maintain alignment between the SRS and evolving system implementation, we will conduct biweekly review sessions. These sessions will serve to:
\begin{itemize}[nosep]
    \item Assess the impact of any requirement changes
    \item Update documentation to reflect system modifications
    \item Verify that all changes maintain consistency with the original specifications
\end{itemize}

\begin{table}[H]
\centering
\caption{SRS Verification Checklist}
\begin{tabular}{|p{0.35\textwidth}|p{0.60\textwidth}|}
\hline
\textbf{Phase} & \textbf{Verification Activities} \\ \hline
Requirements Validation & 
\begin{itemize}
    \item[$\square$] Develop comprehensive test suite
    \item[$\square$] Conduct automated and manual testing
    \item[$\square$] Maintain traceability matrix
\end{itemize} \\ \hline
Stakeholder Review & 
\begin{itemize}
    \item[$\square$] Prepare review materials
    \item[$\square$] Conduct formal walkthrough
    \item[$\square$] Address high-risk concerns
\end{itemize} \\ \hline
User Acceptance Testing & 
\begin{itemize}
    \item[$\square$] Recruit representative users
    \item[$\square$] Validate usability metrics
    \item[$\square$] Analyze feedback
\end{itemize} \\ \hline
Continuous Verification & 
\begin{itemize}
    \item[$\square$] Conduct biweekly reviews
    \item[$\square$] Maintain change documentation
\end{itemize} \\ \hline
\end{tabular}
\end{table}

\subsection{Design Verification Plan}

The design verification will focus exclusively on validating the system architecture and implementation specifications through structured peer and supervisor reviews. Classmates will conduct checklist-driven evaluations of the design document, paying particular attention to the refactoring options and the VS Code extension's interface design. Feedback will be consolidated in GitHub Issues and addressed in design refinement meetings.\\

\noindent A formal review with Dr. Istvan David will verify three critical aspects: (1) the energy measurement module's integration, (2) compliance with VS Code extension best practices, and (3) the fault-tolerance of the refactoring pipeline. We will prepare UML sequence diagrams and component specifications to facilitate this technical discussion, documenting all action items in our project board with clear resolution deadlines.

\begin{table}[H]
\centering
\caption{Design Verification Checklist}
\begin{tabular}{|p{0.25\textwidth}|p{0.7\textwidth}|}
\hline
\textbf{Focus Area} & \textbf{Verification Tasks} \\ \hline
Core Architecture & \begin{itemize}
\item[$\square$] Refactoring engine modularity confirmed
\end{itemize} \\ \hline
IDE Integration & \begin{itemize}
\item[$\square$] VS Code API usage reviewed
\item[$\square$] UI/UX patterns verified
\end{itemize} \\ \hline
Data Integrity & \begin{itemize}
\item[$\square$] Energy measurement accuracy checked
\item[$\square$] Code transformation safety ensured
\end{itemize} \\ \hline
\end{tabular}
\end{table}

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation Plan for EcoOptimizer will be verified through peer reviews and targeted testing strategies. Team members and classmates will evaluate the plan's completeness using a structured checklist, focusing specifically on coverage of Python refactoring scenarios and energy efficiency measurements. Feedback will be tracked through GitHub Issues and incorporated in weekly team meetings.\\

\noindent For test effectiveness validation, we will employ mutation testing by introducing faults in sample code containing energy-inefficient patterns. This will quantitatively verify our test cases' ability to detect anomalies. The verification process will measure three key metrics: requirement coverage percentage, test case effectiveness, and issue resolution rate.

\begin{table}[H]
\centering
\caption{V\&V Plan Verification Checklist}
\begin{tabular}{|p{0.25\textwidth}|p{0.7\textwidth}|}
\hline
\textbf{Criteria} & \textbf{Verification Tasks} \\ \hline
Coverage & \begin{itemize}
\item[$\square$] All refactoring cases included
\item[$\square$] Energy metrics validation specified
\end{itemize} \\ \hline
Methodology & \begin{itemize}
\item[$\square$] Appropriate test levels defined
\item[$\square$] Fault detection strategy in place
\end{itemize} \\ \hline
Process & \begin{itemize}
\item[$\square$] Feedback mechanism established
\item[$\square$] Tracking system implemented
\end{itemize} \\ \hline
\end{tabular}
\end{table}

\noindent An iterative refinement process will be implemented, where verification findings are documented as GitHub issues and addressed in sprint reviews. This ensures the V\&V plan remains aligned with both the technical requirements of Python code optimization and the project's timeline constraints. Progress will be measured against predefined success metrics, including test coverage percentages and mutation detection rates.


\subsection{Implementation Verification Plan}

The implementation verification will ensure EcoOptimizer's codebase strictly adheres to the SRS specifications through rigorous testing protocols. Unit testing will validate core functionality using Pytest~\cite{pytest} for Python components (refactoring engine, energy measurement) and Jest~\cite{jest} for the VS Code extension (UI integration). Test cases will specifically target energy-efficient transformations identified in \textbf{FR2-FR4}.\\

\noindent Static analysis will enforce code quality using Python linters to verify compliance with PEP 8~\cite{pep8} standards (per \textbf{CR-SCR1}) and detect security vulnerabilities. Weekly peer code reviews will examine implementation quality, focusing on:
\begin{itemize}[nosep]
\item Algorithm efficiency for energy optimization
\item Correct handling of Python version-specific features \textbf{(MS-AD3})
\item VS Code extension usability (\textbf{UHR-EOU1})\\
\end{itemize}

\noindent Performance testing will validate:
\begin{itemize}[nosep]
\item Refactoring speed against \textbf{PR-SL1} thresholds
\item Energy measurement accuracy (\textbf{FR6})
\item Large codebase handling (\textbf{PR-CR1})
\end{itemize}

\begin{table}[h]
\centering
\caption{Implementation Verification Checklist}
\begin{tabular}{|p{0.25\textwidth}|p{0.7\textwidth}|}
\hline
\textbf{Category} & \textbf{Verification Tasks} \\ \hline
Functionality & \begin{itemize}
\item[$\square$] Unit tests for all refactoring methods
\item[$\square$] Energy measurement validation
\end{itemize} \\ \hline
Quality & \begin{itemize}
\item[$\square$] Static analysis completed
\item[$\square$] Code reviews conducted
\end{itemize} \\ \hline
Performance & \begin{itemize}
\item[$\square$] Processing time benchmarks
\item[$\square$] Large codebase tests
\end{itemize} \\ \hline
\end{tabular}
\end{table}

\subsection{Automated Testing and Verification Tools}

\textbf{Unit Testing Framework:} The project uses standard testing tools for each part of the system: \texttt{Pytest}~\cite{pytest} for the Python backend and \texttt{Jest}~\cite{jest} for the TypeScript frontend. These tools were chosen because they are widely used in their respective ecosystems, well-documented, and compatible with the project's CI/CD pipeline. Together they provide test coverage for both backend and frontend components.\\

\noindent\textbf{Code Coverage Tools and Plan for Summary:} The codebase will be analyzed to determine the percentage of code executed during tests using language-specific tools:

\begin{itemize}
    \item \textbf{Python:} \texttt{pytest-cov}~\cite{pytest-cov} provides granular-level coverage including branch, line, and path analysis. Its seamless integration with \texttt{Pytest}~\cite{pytest} ensures coverage metrics are generated during normal test execution.
    
    \item \textbf{TypeScript:} \texttt{Jest}'s~\cite{jest} built-in coverage functionality will track statement, branch, and function coverage for the VS Code extension, with configuration matching the Python tool's output format.
\end{itemize}

\noindent Initially, the aim is to achieve 40\% coverage across both codebases, gradually incrementing the level over time. Weekly reports generated from both tools will be combined to track coverage trends, identify testing gaps, and set improvement goals as the project evolves. The unified coverage data will ensure consistent quality standards are maintained throughout the full stack.\\

\noindent\textbf{Linters and Formatters:} To enforce the official
Python PEP 8~\cite{pep8} style guide and maintain code quality, the team will use
\texttt{Ruff}~\cite{ruff} for Python code and
\texttt{eslint}~\cite{eslint} paired with
\texttt{Prettier}~\cite{prettier} for the TypeScript extension.\\

\noindent\textbf{Testing Strategy for the VSCode Extension:} The
TypeScript extension will be tested using \texttt{Jest}~\cite{jest}.
Automated tests will verify interactions between the extension and
the editor, reducing regressions during development.

\subsection{Software Validation Plan}

EcoOptimizer will be validated through:
\begin{itemize}
    \item \textbf{Functional Testing}: Evaluation against open-source Python projects to verify:
    \begin{itemize}
        \item Energy efficiency improvements (\textbf{FR3}, \textbf{FR6})
        \item Refactoring accuracy (\textbf{FR4})
    \end{itemize}
    
    \item \textbf{Usability Testing}: Sessions with Dr. David and Python developers assessing:
    \begin{itemize}
        \item VS Code extension workflow (\textbf{UHR-EOU1-2})
        \item Developer experience metrics
    \end{itemize}
    
    \item \textbf{Formal Review}: Rev 0 demo with Dr. David to validate:
    \begin{itemize}
        \item SRS requirement implementation
        \item System behavior against specifications
    \end{itemize}
\end{itemize}

\newpage\section{System Tests}

This section outlines the tests for verifying both functional and
nonfunctional requirements of the software, ensuring it meets user
expectations and performs reliably. This includes tests for code
quality, usability, performance, security, and traceability, covering
essential aspects of the software’s operation and compliance.

\subsection{Tests for Functional Requirements}

The subsections below outline tests corresponding to functional
requirements in the \SRS~\cite{SRS}. Each test is associated with a
unique functional area, helping to confirm that the tool meets the
specified requirements. Each functional area has its own subsection for clarity.

\noindent
\colorrule

\subsubsection{Code Input Acceptance Tests}
\colorrule

\medskip

\noindent
This section covers the tests for ensuring the system correctly
accepts Python source code files, detects errors in invalid files,
and provides suitable feedback (\textbf{FR1}).

\begin{enumerate}[label={\bf
    \textcolor{Maroon}{test-FR-IA-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Valid Python File Acceptance} \\[2mm]
    \textbf{Control:} Manual \\ 
    \textbf{Initial State:} Tool is idle within the VS Code workspace. \\ 
    \textbf{Input:} A valid Python file (filename.py) containing syntactically correct code. \\ 
    \textbf{Output:} The system accepts the file without errors and displays any detected code smells if present. \\[2mm]
    \textbf{Test Case Derivation:} Confirming that the system
    correctly processes valid Python files in the supported environment, as specified in \textbf{FR1}. \\[2mm]
    \textbf{How test will be performed:} Open a syntactically valid `.py` file in VS Code with the extension enabled. Verify that the tool processes the file and optionally displays code smells if any are present.

  \item \textbf{Feedback for Python File with Bad Syntax} \\[2mm]
    \textbf{Control:} Manual \\ 
    \textbf{Initial State:} Tool is idle within the VS Code workspace. \\ 
    \textbf{Input:} A `.py` file (badSyntax.py) containing deliberate syntax errors that prevent parsing. \\ 
    \textbf{Output:} The system detects the issue, halts further analysis, and displays an appropriate error message within the editor. \\[2mm]
    \textbf{Test Case Derivation:} Ensures graceful handling of invalid Python syntax and appropriate user feedback, satisfying \textbf{FR1}. \\[2mm]
    \textbf{How test will be performed:} Load a Python file with syntax errors in VS Code, then observe whether the extension flags the syntax issue and stops further processing.

  \item \textbf{Feedback for Non-Python File}\\[2mm]
    \textbf{Control:} Manual \\ 
    \textbf{Initial State:} Tool is idle within the VS Code workspace. \\ 
    \textbf{Input:} A non-Python file (e.g., `notes.txt` or `script.js`). \\ 
    \textbf{Output:} The system ignores the file or displays a message indicating that the file type is unsupported. \\[2mm]
    \textbf{Test Case Derivation:} Validates that the system filters non-Python files, consistent with the requirement that it must exclusively process `.py` files (\textbf{FR1}). \\[2mm]
    \textbf{How test will be performed:} Attempt to open a non-Python file in VS Code and check that the extension does not attempt analysis or refactoring and provides clear messaging if applicable.

\end{enumerate}

\noindent\colorrule


\subsubsection{Code Smell Detection Tests and Refactoring
Suggestion (RS) Tests} \label{4.1.2}
\colorrule

\medskip

\noindent
This area includes tests to verify the detection and refactoring
of specified code smells that impact energy efficiency. These tests will be
done through unit testing.

\begin{enumerate}[label={\bf
\textcolor{Maroon}{test-FR-IA-\arabic*}}, wide=0pt, font=\itshape]
\item \textbf{Successful Refactoring Execution} \\[2mm]
\textbf{Control:} Automated \\
\textbf{Initial State:} Tool is idle in the VS Code environment. \\
\textbf{Input:} A valid Python file with a detectable code smell. \\
\textbf{Output:} The system applies the appropriate refactoring and updates the code view. \\[2mm]
\textbf{Test Case Derivation:} Ensures the tool correctly identifies a smell (e.g., LEC001), chooses an applicable refactoring, and applies it successfully, per \textbf{FR2} and \textbf{FR3}. \\[2mm]
\textbf{How test will be performed:} Provide a valid Python file containing a known smell, trigger refactoring via the VS Code interface, and confirm the output includes refactored code as expected.

\item \textbf{No Available Refactorer Handling} \\[2mm]
\textbf{Control:} Automated \\
\textbf{Initial State:} Tool is idle. \\
\textbf{Input:} A valid Python file containing a code smell that does not yet have a supported refactorer. \\
\textbf{Output:} The system does not apply changes and logs or displays an informative message. \\[2mm]
\textbf{Test Case Derivation:} Verifies that unsupported code smells are gracefully handled without errors, per \textbf{FR2}. \\[2mm]
\textbf{How test will be performed:} Provide a valid Python file with an unsupported smell and observe that the system notifies the user without attempting modification.

\item \textbf{Multiple Refactoring Calls on Same File} \\[2mm]
\textbf{Control:} Automated \\
\textbf{Initial State:} Tool is idle. \\
\textbf{Input:} A valid Python file with a detectable code smell, refactored more than once. \\
\textbf{Output:} The tool processes the file repeatedly and applies changes incrementally. \\[2mm]
\textbf{Test Case Derivation:} Confirms the system can handle repeated invocations and re-apply applicable refactorings, per \textbf{FR3}. \\[2mm]
\textbf{How test will be performed:} Refactor a file containing a supported smell multiple times and verify that each run performs valid operations and results in updated outputs.

\item \textbf{Handling Empty Modified Files List} \\[2mm]
\textbf{Control:} Automated \\
\textbf{Initial State:} Tool is idle. \\
\textbf{Input:} A valid Python file where the code smell is detected, but the refactorer makes no modifications. \\
\textbf{Output:} The system does not generate output files and notifies the user appropriately. \\[2mm]
\textbf{Test Case Derivation:} Confirms the tool handles no-op refactorers correctly, per \textbf{FR4}. \\[2mm]
\textbf{How test will be performed:} Supply a file where the refactorer returns an unchanged version of the code and verify that no new files are created and that appropriate feedback is displayed or logged.

\end{enumerate}

\noindent\colorrule

\subsubsection{Tests for Reporting Functionality}
\colorrule

\medskip

\noindent
The reporting functionality of the tool is crucial for providing
users with meaningful insights into the energy impact of refactorings
and the smells being addressed. This section outlines tests that
ensure the energy metrics and refactoring summaries are accurately
presented, as required by \textbf{FR6} and \textbf{FR15}.

\begin{enumerate}[label={\bf
    \textcolor{Maroon}{test-FR-RP-\arabic*}}, wide=0pt, font=\itshape]

  \item \textbf{Energy Consumption Metrics Displayed Post-Refactoring} \\[2mm]
    \textbf{Control:} Manual \\
    \textbf{Initial State:} The tool has measured energy usage before and after refactoring. \\
    \textbf{Input:} Energy data collected for the original and refactored code. \\
    \textbf{Output:} A clear comparison of energy consumption is displayed in the UI. \\[2mm]
    \textbf{Test Case Derivation:} Verifies that energy metrics are properly calculated and presented to users, as per \textbf{FR6}. \\[2mm]
    \textbf{How test will be performed:} Refactor a file and review the visual or textual display of energy usage before and after, ensuring the values match backend logs.

  \item \textbf{Detected Code Smells and Refactorings Reflected in UI} \\[2mm]
    \textbf{Control:} Manual \\
    \textbf{Initial State:} The tool has completed code analysis and refactoring. \\
    \textbf{Input:} Output of the detection and refactoring modules. \\
    \textbf{Output:} The user interface displays the detected code smells and associated refactorings clearly. \\[2mm]
    \textbf{Test Case Derivation:} Ensures transparency of changes and supports informed decision-making by the user, in line with \textbf{FR15}. \\[2mm]
    \textbf{How test will be performed:} Open a code file with detectable smells, trigger a refactor, and inspect the view displaying the summary of changes and available actions.

\end{enumerate}

\noindent\colorrule

\subsubsection{Visual Studio Code Interactions}
\colorrule

\medskip

\noindent
This section corresponds to features related to the user’s interaction with the Visual Studio Code extension interface, including previewing and toggling smells, customizing the UI, and reviewing code comparisons. These tests verify that the extension enables users to interact with refactorings in an intuitive and informative manner, as outlined in \textbf{FR8}, \textbf{FR9}, \textbf{FR10}, \textbf{FR11}, \textbf{FR12}, \textbf{FR13}, \textbf{FR14}, \textbf{FR15}, \textbf{FR16}, and \textbf{FR17}.\\

\noindent These features are primarily tested through automated unit tests integrated in the extension codebase. For implementation and test details, please refer to the unit testing suite.

\noindent\colorrule

\subsubsection{Documentation Availability Tests}
\colorrule

\medskip

\noindent
The following test is designed to ensure the availability of
documentation as per \textbf{FR 7} and \textbf{FR 5}.

\begin{enumerate}[label={\bf
    \textcolor{Maroon}{test-FR-DA-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Test for Documentation Availability}\\[2mm]
    \textbf{Control:} Manual\\
    \textbf{Initial State:} The system may or may not be installed.\\
    \textbf{Input:} User attempts to access the documentation.\\
    \textbf{Output:} The documentation is available and covers
    installation, usage (\textbf{FR 5}), and troubleshooting.\\[2mm]
    \textbf{Test Case Derivation:} Validates that the documentation
    meets user needs (\textbf{FR 7}).\\[2mm]
    \textbf{How test will be performed:} Review the documentation for
    completeness and clarity.
\end{enumerate}

\newpage\subsection{Tests for Nonfunctional Requirements}

The section will cover system tests for the non-functional
requirements (NFR) listed in the \SRS \hspace{1pt}
document\cite{SRS}. The goal for these tests is to address the fit
criteria for the requirements. Each test will be linked back to a
specific NFR that can be observed in section \ref{trace-sys}.\\

\noindent For non-functional requirements that are not linked to a test in the below sections, has either been covered in the functional requirements test plan or unit tests plan. Please see traceability matrix for more details.

\noindent\colorrule

\subsubsection{Look and Feel}
\colorrule

\medskip

\noindent
The following subsection tests cover all Look and Feel non-functional
requirements listed in the SRS~\cite{SRS}. They aim to validate that
the system provides a modern, visually appealing, and supportive
developer experience. These tests ensure that the tool facilitates
refactoring decisions through clear interfaces and satisfies design
expectations based on user perception.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-LF-\arabic*}}, wide=0pt, font=\itshape]
  
  \item \textbf{Side-by-side Code Comparison in IDE Plugin} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} LFR-AP1 \\ 
    \textbf{Initial State:} IDE plugin open in VS Code with a sample
    code file loaded \\ 
    \textbf{Input/Condition:} The user initiates a refactoring operation \\ 
    \textbf{Output/Result:} The plugin displays the original and
    refactored code side by side \\[2mm]
    \textbf{How test will be performed:} The tester will open a
    sample file and apply a refactoring. They will verify that the
    original and refactored code are shown side by side with
    functional accept/reject buttons. This confirms users can make
    informed refactoring decisions in a visually supportive layout.

  \item \textbf{Design Acceptance Survey} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} LFR-ST1, \textit{implicitly covers} LFR-AP2 \\ 
    \textbf{Initial State:} IDE plugin open \\ 
    \textbf{Input/Condition:} Developer interacts with the plugin \\ 
    \textbf{Output/Result:} A survey response capturing the user’s
    perception of the design \\[2mm]
    \textbf{How test will be performed:} After using the plugin,
    test participants will complete the design satisfaction survey
    described in \ref{A.2}. The results will be reviewed to assess
    whether the plugin meets the project's aesthetic and usability
    expectations.\\

    \textit{Note:} LFR-AP2 is not tested directly, as it describes a
    design philosophy rather than a testable behavior. User perception of
    visual simplicity and minimalism is instead captured through feedback
    in the usability survey.

\end{enumerate}

\newpage
\noindent\colorrule

\subsubsection{Usability \& Humanity}
\colorrule

\medskip

\noindent
The following tests cover all Usability \& Humanity requirements listed in the SRS~\cite{SRS}. These tests aim to validate that the system is accessible, user-centred, intuitive, and easy to navigate. Data is collected via user surveys or static analysis, and evaluated against thresholds (e.g., 80–90\% agreement). Where applicable, tests are traceable to corresponding SRS requirements.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-UH-\arabic*}}, wide=0pt, font=\itshape]

  \item \textbf{Customizable Settings for Refactoring Preferences} \\[2mm]
    \textbf{Type:} Manual, Dynamic \\
    \textbf{Covers:} \textbf{UHR-PSI 1} \& \textbf{UHR-PSI 2} \\ 
    \textbf{Initial State:} Plugin open with settings panel accessible \\
    \textbf{Input/Condition:} User customizes enabled smells and highlight colors \\
    \textbf{Output/Result:} Preferences persist and affect plugin behavior \\[2mm]
    \textbf{How test will be performed:} Tester toggles smell types and changes highlight colours. They reload the plugin and verify that settings are retained and correctly reflected in UI and detection.

  \item \textbf{High-Contrast Theme Accessibility Check} \\[2mm]
    \textbf{Type:} Static Analysis \\
    \textbf{Covers:} \textbf{UHR-ACS 1} \\ 
    \textbf{Initial State:} Contrast-based theme styles configured \\
    \textbf{Input/Condition:} Contrast analyzer is run on UI color tokens \\
    \textbf{Output/Result:} All items meet 4.5:1 or 3:1 WCAG thresholds \\[2mm]
    \textbf{How test will be performed:} Use automated tools (e.g., WebAIM) to verify foreground/background ratios for code highlights, sidebars, and messages.

  \item \textbf{Intuitive User Interface for Core Functionality} \\[2mm]
    \textbf{Type:} User Testing, Survey-Based \\
    \textbf{Covers:} \textbf{UHR-EOU 1} \\ 
    \textbf{Initial State:} Plugin open with test tasks prepared \\
    \textbf{Input/Condition:} Users complete core tasks (detect, refactor, configure) \\
    \textbf{Output/Result:} 90\% of users complete each task in $\leq$ 3 clicks and rate the interaction as intuitive \\[2mm]
    \textbf{How test will be performed:} Clicks per task are recorded. After each task, users answer the question “Was this process intuitive?” on a 5-point Likert scale. The question is listed in Appendix~\ref{A.2}. \\
    \textbf{Quantifiable Metric:} At least 85\% of responses must score 4 or 5 on the intuitiveness scale. \\
    \textbf{Use of Results:} Responses scoring below threshold will trigger UX redesign of the corresponding interaction. Open feedback (if given) will be coded thematically to identify patterns in confusion or friction points.

  \item \textbf{Clear and Concise User Prompts} \\[2mm]
    \textbf{Type:} Survey-Based \\
    \textbf{Covers:} \textbf{UHR-EOU 2} \\ 
    \textbf{Initial State:} User encounters plugin prompts (e.g., file missing, confirm refactor) \\
    \textbf{Input/Condition:} Users follow prompts and evaluate clarity \\
    \textbf{Output/Result:} 90\% of users agree prompts are helpful and unambiguous \\[2mm]
    \textbf{How test will be performed:} After interacting with all major system prompts, users complete a survey (Appendix~\ref{A.2}) where they rate the clarity of each message on a 5-point scale and provide optional comments. \\
    \textbf{Quantifiable Metric:} 90\% of users must rate each prompt $\geq$ 4 for clarity and helpfulness. \\
    \textbf{Use of Results:} Any prompt scoring below target will be reviewed and rewritten. Free-text feedback will be grouped by theme to identify language, formatting, or placement issues.

  \item \textbf{Context-Sensitive Help Based on User Actions} \\[2mm]
    \textbf{Type:} Manual \\
    \textbf{Covers:} \textbf{UHR-LRN 1} \\ 
    \textbf{Initial State:} Help system available via command/hover \\
    \textbf{Input/Condition:} User performs smell-related actions, requests help \\
    \textbf{Output/Result:} Help shown is contextually relevant, appears within \textless= 3 clicks \\[2mm]
    \textbf{How test will be performed:} Tester performs tasks like “Apply refactor” or “Toggle smell” and verifies help popups match the current feature.

    \item \textbf{Clear and Constructive Error Messaging} \\[2mm]
    \textbf{Type:} Survey-Based \\
    \textbf{Covers:} \textbf{UHR-UPL 1} \\ 
    \textbf{Initial State:} Plugin triggers common errors (e.g., no workspace configured, backend offline) \\
    \textbf{Input/Condition:} User encounters error messages during usage and evaluates tone and clarity \\
    \textbf{Output/Result:} 80\% of users agree the message is polite, understandable, and helpful \\[2mm]
    \textbf{How test will be performed:} After encountering various plugin error scenarios, users complete a survey located in Appendix~\ref{A.2}. They rate each error message on tone, clarity, and helpfulness using a 5-point Likert scale, and may provide free-text feedback. \\
    \textbf{Quantifiable Metric:} Each error message must receive an average rating of $\geq$ 4 in tone, clarity, and helpfulness from at least 80\% of participants. \\
    \textbf{Use of Results:} Messages scoring below threshold will be flagged for revision. Qualitative responses will be categorized to identify recurring issues such as overly technical language, lack of actionable steps, or negative tone.

\end{enumerate}


\newpage
  \noindent
  \textcolor{Blue}{\colorrule}

\subsubsection{Performance}
\colorrule

\medskip

\noindent
The following subsection tests cover the Performance requirements listed in the SRS~\cite{SRS}. The goal is to validate that the tool can process Python files of varying sizes within acceptable performance thresholds. These tests confirm responsiveness under real-world usage, and guide profiling and optimization work for scalability.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-PF-\arabic*}}, wide=0pt, font=\itshape]
    
  \item \textbf{Performance and Capacity Validation for Analysis and Refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Covers:} PR-SL~1, PR-SL~2, PR-CR~1 \\
    \textbf{Initial State:} IDE open with multiple Python files of varying sizes prepared (small: 250 LOC, medium: 1000 LOC, large: 3000 LOC) \\
    \textbf{Input/Condition:} Initiate detection and refactoring for each file sequentially \\
    \textbf{Output/Result:} Tool completes within: 
    \begin{itemize}
      \item 20 seconds for small files ($\leq$ 250 lines)
      \item 50 seconds for medium files ($\leq$ 1000 lines)
      \item 2 minutes for large files ($\leq$ 3000 lines)
    \end{itemize}
    \textbf{How test will be performed:} Detection and refactoring will be run on each file. Timings will be recorded from start to finish. If thresholds are exceeded, logs and profiling output will be used to identify and prioritize optimization targets.

\end{enumerate}

\medskip

\noindent \textbf{Untested Requirements and Justification:}

\begin{itemize}
  \item \textbf{PR-SCR 1 (No runtime errors in refactored code)}: Verified by functional unit tests and integration tests that execute refactored code and ensure correctness and compilability.
  
  \item \textbf{PR-PAR 1 (Smell detection accuracy)}: Already covered in functional tests, which compare the detected smells against a ground truth dataset and calculate precision/recall.

  \item \textbf{PR-PAR 2 (Output validity)}: Confirmed by functional tests that ensure the generated refactored code is syntactically valid and matches Python standards.

  \item \textbf{PR-RFT 1 (Robustness to invalid input)}: Addressed through functional tests which simulate corrupt files or invalid syntax and assert the system recovers gracefully.

  \item \textbf{PR-SER 1 (Extensibility)}: Verified through manual inspection and code review of plugin architecture and smell registration pipeline; extensibility is not runtime-measurable and therefore not performance tested.

  \item \textbf{PR-LR 1 (Longevity)}: Also verified through code quality reviews. Maintainability is supported by documentation and modularity but not measurable via direct tests in this release cycle.
\end{itemize}


  \noindent
  \colorrule

\subsubsection{Operational \& Environmental}
\colorrule

\medskip

\noindent
The following subsection tests cover all Operational and Environmental requirements listed in the SRS~\cite{SRS}. This includes confirming system compatibility, installation capability, and basic usability across operational contexts. Physical environment requirements are not tested, as explained below.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-OPE-\arabic*}}, wide=0pt, font=\itshape]

  \item \textbf{VS Code Compatibility for Refactoring Library Extension} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} \textbf{OER-IAS 1} \\ 
    \textbf{Initial State:} VS Code IDE open and library not installed \\
    \textbf{Input/Condition:} User installs and opens the refactoring library extension in VS Code \\
    \textbf{Output/Result:} The extension installs successfully and runs inside VS Code \\[2mm]
    \textbf{How test will be performed:} The tester will search for the extension on the VS Code marketplace, install it, and verify that it appears in the IDE and can execute basic functionality such as detecting or refactoring a code file.

  \item \textbf{Import and Export Capabilities for Codebases and Metrics} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} \textbf{OER-IAS 2} \\
    \textbf{Initial State:} IDE plugin open with option to import/export \\
    \textbf{Input/Condition:} User imports a sample project and exports results \\
    \textbf{Output/Result:} Plugin correctly imports project and generates JSON/XML reports for refactored code and metrics \\[2mm]
    \textbf{How test will be performed:} Tester loads a sample codebase, initiates a refactor, and uses the export function. Output files are verified for valid structure and meaningful content.

  \item \textbf{PIP Package Installation Availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Covers:} \textbf{OER-PR 1} \\
    \textbf{Initial State:} Fresh Python environment without the package \\
    \textbf{Input/Condition:} User runs \texttt{pip install ecooptimizer} \\
    \textbf{Output/Result:} The package installs successfully without error \\[2mm]
    \textbf{How test will be performed:} Tester uses a clean Python virtual environment to install the library. A short script using a sample function will verify it works as expected after installation.

\end{enumerate}

\subsubsection*{Unnecessary Tests and Justifications}

\begin{itemize}
  \item \textbf{OER-EP 1 \& OER-EP 2 (Temperature \& Power Requirements)}: \\
  These describe expected hardware operating conditions. If the computer cannot operate due to temperature or power issues, the software cannot run. These are not properties of the software and are thus \textit{not tested} in this V\&V plan.

  \item \textbf{OER-RL 1 (All core functionality implemented and tested)}: \\
  This requirement is satisfied by the completion of the full functional and non-functional test suite. No specific test case is needed beyond traceability to those tests.

  \item \textbf{OER-RL 2 (Release by March 17, 2025)}: \\
  This is a delivery milestone tracked through project management, not through testing. Therefore, it is \textit{not verified dynamically} through test cases.
\end{itemize}

  \noindent
  \colorrule

\subsubsection{Maintenance and Support}
\colorrule

\medskip

\noindent
The following subsection tests cover the most critical Maintenance and Support requirements listed in the SRS~\cite{SRS}. These tests emphasize extensibility, maintainability, and recovery from faulty updates. Some lower-priority requirements are acknowledged but excluded from testing due to scope or redundancy with existing CI/CD practices.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-MS-\arabic*}}, wide=0pt, font=\itshape]

  \item \textbf{Extensibility for New Code Smells and Refactorings} \\[2mm]
    \textbf{Covers:} \textbf{MS-MNT 1} \\ 
    \textbf{Type:} Code Walkthrough and Manual Dynamic \\
    \textbf{Initial State:} Developer environment set up with project codebase \\
    \textbf{Input/Condition:} Developer adds a sample code smell and refactoring method \\
    \textbf{Output/Result:} The new smell/refactoring integrates with minimal disruption \\[2mm]
    \textbf{How test will be performed:} Developers will follow documentation to add a new smell and refactoring function. They will verify modularity and confirm that existing functionality is unaffected. Success is defined by clean integration and interface visibility without needing major code changes.

  \item \textbf{Maintainable and Adaptable Codebase} \\[2mm]
    \textbf{Covers:} \textbf{MS-MNT 2} \\ 
    \textbf{Type:} Static Analysis and Documentation Review \\
    \textbf{Initial State:} Final implementation and documentation available \\
    \textbf{Input/Condition:} Reviewers evaluate code organization and documentation \\
    \textbf{Output/Result:} Codebase is modular, readable, and sufficiently documented \\[2mm]
    \textbf{How test will be performed:} Reviewers examine file structure, naming conventions, and presence of comments. They will evaluate whether a new developer could easily navigate and modify the code. Documentation completeness will be scored on a checklist.

  \item \textbf{Rollback Support for Faulty Updates} \\[2mm]
    \textbf{Covers:} \textbf{MS-MNT 3} \\ 
    \textbf{Type:} Manual Dynamic \\
    \textbf{Initial State:} Installed version of the library in use \\
    \textbf{Input/Condition:} Faulty update is simulated; rollback is triggered \\
    \textbf{Output/Result:} System returns to previous stable state successfully \\[2mm]
    \textbf{How test will be performed:} A new version with an intentional fault is deployed. The user will invoke rollback steps (e.g., Git revert or VS Code extension downgrade). The tool should resume normal operation with no feature regressions.
\end{enumerate}

\vspace{2mm}
\noindent\textbf{Unnecessary Tests and Justifications}
\begin{itemize}
  \item \textbf{MS-MNT 4 (Automated Testing for Refactorings):} Verified through existing unit tests and CI pipeline. Manual test duplication is unnecessary.
  \item \textbf{MS-MNT 5 (Library Compatibility with Dependencies):} Assumed validated during integration testing and package dependency resolution via PIP.
\end{itemize}

\noindent  \colorrule


\subsubsection{Security}
\colorrule

\medskip

\noindent
The following subsection tests cover the primary Security requirement listed in the SRS~\cite{SRS}. These tests ensure that refactoring operations are traceable, logged securely, and protected from unauthorized tampering. Due to the tool's local-only design, network-level security and external access controls are out of scope.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-SRT-\arabic*}}, wide=0pt, font=\itshape]

  \item \textbf{Audit Logs for Refactoring Processes} \\[2mm]
    \textbf{Covers:} \textbf{SRT-AUD 1} \\ 
    \textbf{Type:} Static Analysis and Code Review \\
    \textbf{Initial State:} Fully implemented refactoring pipeline with logging enabled \\
    \textbf{Input/Condition:} Refactoring actions performed on one or more files \\
    \textbf{Output/Result:} Secure and complete logs are generated for each action \\[2mm]
    \textbf{How test will be performed:} The development team will review logging logic in the code to ensure it covers all major events: pattern detection, energy analysis, and refactor generation. Each entry must include a timestamp, file reference, and action type. Reviewers will confirm that logs are immutable and not user-editable, using tools like append-only file systems or cryptographic checksums if applicable.

\end{enumerate}

\newpage

  \noindent
  \colorrule

  \subsubsection{Cultural}
  \colorrule

  \medskip

  \noindent
  Cultural requirements are not applicable to this project since we
  are using VS Code settings and a plugin-based approach. These
  aspects do not involve cultural considerations, making such
  requirements unnecessary.

  \noindent
  \colorrule

\subsubsection{Compliance}
\colorrule

\medskip

\noindent
The following subsection tests cover all Compliance requirements listed in the SRS~\cite{SRS}. These tests ensure that the system does not collect personal user data and that the codebase adheres to established Python development standards.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-CPL-\arabic*}}, wide=0pt, font=\itshape]

  \item \textbf{User Privacy and Local Execution Compliance} \\[2mm]
    \textbf{Covers:} \textbf{CR-LR 1} \\
    \textbf{Type:} Static Review and Code Audit \\
    \textbf{Initial State:} Tool installed and ready for inspection \\
    \textbf{Input/Condition:} Reviewer inspects runtime behavior and data access patterns \\
    \textbf{Output/Result:} No personal or user-specific data is collected or transmitted \\[2mm]
    \textbf{How test will be performed:} Review the codebase to confirm that:
    \begin{itemize}
      \item The tool does not collect or store personal information.
      \item No external API requests are made that could leak user data.
      \item All operations occur locally, including refactoring, logging, and energy analysis.
    \end{itemize}
    The reviewer will also confirm the absence of telemetry or usage tracking modules. \\[2mm]
    \textbf{Acceptance Criteria:} The tool operates entirely locally, without collecting or transmitting any personal data. Privacy is preserved by design.

  \item \textbf{PEP 8 Standards Compliance} \\[2mm]
    \textbf{Covers:} \textbf{CR-SCR 1} \\
    \textbf{Type:} Static Analysis \\
    \textbf{Initial State:} Codebase fully implemented \\
    \textbf{Input/Condition:} Code is scanned using a PEP 8 linter (e.g., \texttt{flake8}, \texttt{pylint}) \\
    \textbf{Output/Result:} All code conforms to Python PEP 8 coding standards \\[2mm]
    \textbf{How test will be performed:} Run a static code analysis tool across the full Python codebase. Evaluate:
    \begin{itemize}
      \item Adherence to line length, indentation, and naming conventions.
      \item Documentation comments and docstring usage.
      \item Overall maintainability and readability.
    \end{itemize}
    Any issues will be fixed and re-evaluated. \\[2mm]
    \textbf{Acceptance Criteria:} The code passes static analysis with no critical PEP 8 violations. Style warnings are minimized, and documentation is consistent.

\end{enumerate}

\newpage\subsection{Traceability Between Test Cases and Requirements}
\label{trace-sys}

\begin{table}[H]
  \centering
  \caption{Functional Requirements and Corresponding Test Sections}
  \begin{tabular}{p{0.42\textwidth}p{0.42\textwidth}}
    \toprule \textbf{Test Section} & \textbf{Functional Requirement(s)} \\
    \midrule
    Code Input Acceptance Tests & FR1 \\
    Code Smell Detection and Refactoring Suggestion Tests & FR2, FR3, FR4 \\
    Tests for Reporting Functionality & FR6, FR15 \\
    Visual Studio Code Interactions & FR8, FR9, FR10, FR11, FR12, FR13, FR14, FR15, FR16, FR17 \\
    Documentation Availability Tests & FR7, FR5 \\
    Installation and Onboarding Tests & FR7 \\
    \bottomrule
  \end{tabular}
  \label{tab:sections_requirements}
\end{table}


  \label{tab:nfr-trace-reqs}
  \begin{table}[H]
  \centering
  \caption{Look \& Feel Tests and Corresponding Requirements}
  \label{tab:nfr-trace-lf}
  \begin{tabular}{cc}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    LF-1 & LFR-AP 1 \\
    LF-2 & LFR-ST 1, LFR-AP 2 \\
    % Note: LFR-AP 2 is tested indirectly in LF-2
    \bottomrule
  \end{tabular}
\end{table}


  \begin{table}[H]
  \centering
  \caption{Usability \& Humanity Tests and Corresponding Requirements}
  \label{tab:nfr-trace-uh}
  \begin{tabular}{cc}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    UH-1 & UHR-PSI 1, UHR-PSI 2 \\
    UH-2 & UHR-ACS 1 \\
    UH-3 & UHR-EOU 1 \\
    UH-4 & UHR-EOU 2 \\
    UH-5 & UHR-LRN 1 \\
    UH-6 & UHR-UPL 1 \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[H]
  \centering
  \caption{Performance Tests and Corresponding Requirements}
  \begin{tabular}{cc}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Performance
    PF-1 & PR-SL 1, PR-SL 2, PR-CR 1 \\
    \bottomrule
  \end{tabular}
\end{table}


  \begin{table}[H]
    \centering
    \caption{Operational \& Environmental Tests and Corresponding Requirements}
    \begin{tabular}{cc}
      \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
      \midrule
      % Operational and Environmental
      Not explicitly tested & OER-EP 1 \\
      Not explicitly tested & OER-EP 2 \\
      OPE-1 & OER-WE 1 \\
      OPE-2 & OER-IAS 1 \\
      OPE-3 & OER-IAS 2 \\
      OPE-4 & OER-IAS 3 \\
      OPE-5 & OER-PR 1 \\
      Tested by FRs & OER-RL 1 \\
      Not explicitly tested & OER-RL 2 \\
      \bottomrule
    \end{tabular}
  \end{table}

  \begin{table}[H]
    \centering
    \caption{Maintenance \& Support Tests and Corresponding Requirements}
    \begin{tabular}{cc}
      \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
      \midrule
      % Maintenance and Support
      MS-1 & MS-MNT 1, PR-SER 1 \\
      MS-2 & MS-MNT 2 \\
      MS-3 & MS-MNT 3 \\
      Not explicitly tested & MS-MNT 4 \\
      \bottomrule
    \end{tabular}
  \end{table}

\begin{table}[H]
  \centering
  \caption{Security Tests and Corresponding Requirements}
  \begin{tabular}{cc}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    SRT-1 & SR-IM 1 \\
    \bottomrule
  \end{tabular}
\end{table}


  \begin{table}[H]
    \centering
    \caption{Compliance Tests and Corresponding Requirements}
    \begin{tabular}{cc}
      \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
      \midrule
      % Compliance
      CPL-1 & CL-LR 1 \\
      CPL-2 & CL-SCR 1 \\
      \bottomrule
    \end{tabular}
  \end{table}

  \newpage\section{Unit Test Description}

  \wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

  \wss{Reference your MIS (detailed design document) and explain your overall
  philosophy for test case selection.}

  \wss{To save space and time, it may be an option to provide less
    detail in this section.
    For the unit tests you can potentially lay out your testing
    strategy here.  That is, you
    can explain how tests will be selected for each module.  For
    instance, your test building
    approach could be test cases for each access program, including
    one test for normal behaviour
    and as many tests as needed for edge cases.  Rather than create
    the details of the input
    and output here, you could point to the unit testing code.  For
    this to work, you code
  needs to be well-documented, with meaningful names for all of the tests.}

  \subsection{Detection Module}

  \subsubsection{Analyzer Controller}

  \textbf{Goal:} The analyzer controller serves as the central
  coordination unit for detecting code smells using various analysis
  methods. It interfaces with multiple analyzers, processes detection
  results, and ensures accurate filtering and customization of
  analysis options. The following unit tests verify the correctness
  of this functionality.\\

  \noindent The tests validate the proper execution of smell
  detection, correct filtering of smells based on the chosen analysis
  method, appropriate handling of missing or disabled smells, and the
  generation of custom analysis options. Edge cases such as empty
  files, incorrect configurations, and mismatched smell detection
  methods are also considered.\\

  \noindent\textbf{Target requirement(s):} FR2, FR5~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Running Smell Detection Analysis}
      \begin{itemize}
        \item Ensures the analyzer correctly detects CRC smells in a
          given Python file.
        \item Validates that detected smells include the correct
          attributes, such as message ID, file path, and confidence level.
        \item Confirms that logs capture detected smells for debugging.
      \end{itemize}

    \item \textbf{Handling Cases with No Detected Smells}
      \begin{itemize}
        \item Ensures that when no smells are detected, the
          controller logs an appropriate success message.
        \item Verifies that the returned list of smells is empty.
      \end{itemize}

    \item \textbf{Filtering Smells by Analysis Method}
      \begin{itemize}
        \item Ensures that the controller correctly filters smells
          based on the specified analysis method (e.g., AST, Astroid, Pylint).
        \item Verifies that only smells matching the given method are returned.
      \end{itemize}

    \item \textbf{Generating Custom Analysis Options}
      \begin{itemize}
        \item Ensures that the controller correctly generates custom
          options for AST and Astroid analysis.
        \item Validates that generated options include callable
          detection functions for applicable smells.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/controllers/test_analyzer_controller.py}{here}.

  \subsubsection{String Concatenation in a Loop}

  \textbf{Goal:} The string concatenation in a loop detection module
  identifies inefficient string operations occurring inside loops,
  which can significantly impact performance. The following unit
  tests verify the detection capabilities.\\

  \noindent The tests evaluate different types of string
  concatenation scenarios within loops, ensuring that various cases
  are detected correctly. These include basic concatenation, nested
  loops, conditional concatenation, type inference, and different
  string interpolation methods. The robustness of the detection logic
  is assessed through multiple test cases covering common patterns.\\

  \noindent\textbf{Target requirement(s)}: FR2~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Basic Concatenation in a Loop}
      \begin{itemize}
        \item Simple assignment string concatenation of the form
          \texttt{var = var + \( \ldots \)} inside \texttt{for} and
          \texttt{while} loops.
        \item Augmented assignment string concatenation using
          \texttt{+=} inside \texttt{for} and \texttt{while} loops.
        \item Concatenation involving object attributes (e.g.,
          \texttt{var.attr += \( \ldots \)}).
        \item Concatenation modifying values inside complex objects
          (i.e., dictionaries, iterables).
      \end{itemize}

    \item \textbf{Nested Loop Concatenation}
      \begin{itemize}
        \item String concatenation inside nested loops where the
          outer loop contributes to the concatenation.
        \item Resetting the concatenation variable inside the outer
          loop but continuing inside the inner loop.
      \end{itemize}

    \item \textbf{Conditional Concatenation}
      \begin{itemize}
        \item String concatenation inside loops with
          \texttt{if}-\texttt{else} conditions modifying the
          concatenation variable.
        \item Different branches of a condition appending different
          string literals.
      \end{itemize}

    \item \textbf{Type Inference}
      \begin{itemize}
        \item Proper detection of string types through initial
          variable assignment.
        \item Proper detection of string types through assignment type hints.
        \item Proper detection of string types through function
          definition type hints.
        \item Proper detection of string types initialized as class variables.
      \end{itemize}

    \item \textbf{String Interpolation Methods}
      \begin{itemize}
        \item Concatenation using \texttt{\%} formatting.
        \item Concatenation using \texttt{str.format()}.
        \item Concatenation using f-strings inside loops.
      \end{itemize}

    \item \textbf{Concatenation with Repeated Reassignment}
      \begin{itemize}
        \item A variable being reassigned multiple times in the same
          loop iteration before proceeding to the next iteration.
      \end{itemize}

    \item \textbf{Concatenation with Variable Access Inside the Loop}
      \begin{itemize}
        \item Cases where the concatenation variable is accessed
          inside the loop, making refactoring ineffective since the
          joined result would be required at every iteration.
      \end{itemize}

    \item \textbf{Concatenation Order Sensitivity}
      \begin{itemize}
        \item Concatenation where new values are inserted at the
          beginning of the string instead of the end.
        \item Concatenation that involves both prefix and suffix
          additions within the same loop.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/analyzers/test_str_concat_analyzer.py}{here}.

  \subsubsection{Long Element Chain}

  \textbf{Goal:} The long element chain detection module identifies
  excessive dictionary access chains that exceed a predefined
  threshold (default: 3). The following unit tests verify the
  detection capabilities.\\

  \noindent The tests evaluate the detection of long element chains
  by verifying that sequences exceeding the threshold, such as deep
  dictionary accesses, are correctly identified. They include edge
  cases, such as chains just below the threshold, variations in data
  structures, and multiple chains in the same scope, to assess the
  robustness of the detection logic.\\

  \noindent\textbf{Target requirement(s):} FR2~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Ignores code with no chains}
      \begin{itemize}
        \item Ensures that code without any nested element chains
          does not trigger a detection.
      \end{itemize}

    \item \textbf{Ignores chains below the threshold}
      \begin{itemize}
        \item Verifies that element chains shorter than the threshold
          are not flagged.
      \end{itemize}

    \item \textbf{Detects chains exactly at threshold}
      \begin{itemize}
        \item Ensures that an element chain with a length equal to
          the threshold is flagged.
      \end{itemize}

    \item \textbf{Detects chains exceeding the threshold}
      \begin{itemize}
        \item Verifies that chains longer than the threshold are
          correctly detected.
      \end{itemize}

    \item \textbf{Detects multiple chains in the same file}
      \begin{itemize}
        \item Ensures that multiple long element chains appearing in
          different locations within the same code file are
          individually detected.
      \end{itemize}

    \item \textbf{Detects chains inside nested functions and classes}
      \begin{itemize}
        \item Confirms that element chains occurring within functions
          and class methods are correctly identified.
      \end{itemize}

    \item \textbf{Reports identical chains on the same line only once}
      \begin{itemize}
        \item Ensures that duplicate chains appearing on a single
          line are not reported multiple times.
      \end{itemize}

    \item \textbf{Handles different variable types in chains}
      \begin{itemize}
        \item Verifies detection of chains that involve a mix of
          dictionaries, lists, tuples, and nested structures.
      \end{itemize}

    \item \textbf{Correctly applies custom threshold values}
      \begin{itemize}
        \item Ensures that adjusting the threshold parameter
          correctly affects detection behavior.
      \end{itemize}

    \item \textbf{Verifies result structure and metadata}
      \begin{itemize}
        \item Confirms that detected chains return correctly
          formatted results, including message ID, type, and occurrence details.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/analyzers/test_long_element_chain_analyzer.py}{here}.

  \subsubsection{Repeated Calls}

  \textbf{Goal:} The repeated calls detection module identifies
  instances where function or method calls are redundantly executed
  within the same scope, leading to unnecessary performance overhead.
  It ensures that developers receive precise recommendations for
  caching results when applicable. The following unit tests validate
  the accuracy of this functionality.\\

  \noindent The tests assess the module’s ability to detect redundant
  function calls, ignore functionally distinct calls, handle object
  state changes, and recognize cases where caching would not be
  beneficial. Edge cases, such as external function calls, built-in
  function invocations, and method calls on different objects, are
  also considered. \\

  \noindent \textbf{Target requirement(s):} FR2, FR3, PR-PAR2,
  PR-PAR3~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Detecting Repeated Function Calls}
      \begin{itemize}
        \item Ensures the detection of repeated function calls with
          identical arguments within the same scope.
        \item Validates that function calls on different arguments
          are not incorrectly flagged.
      \end{itemize}

    \item \textbf{Detecting Repeated Method Calls}
      \begin{itemize}
        \item Ensures that method calls on the same object instance
          are detected when repeated within a function.
        \item Verifies that method calls on different object
          instances are not falsely flagged.
      \end{itemize}

    \item \textbf{Handling Object State Modifications}
      \begin{itemize}
        \item Ensures that function calls on objects with modified
          attributes between calls are not flagged.
        \item Verifies that method calls are only flagged when they
          occur without intervening state changes.
      \end{itemize}

    \item \textbf{Detecting External Function Calls}
      \begin{itemize}
        \item Detects redundant external function calls such as
          \texttt{len(data.get("key"))}.
        \item Ensures that only calls with matching parameters and
          return values are considered redundant.
      \end{itemize}

    \item \textbf{Handling Built-in Functions}
      \begin{itemize}
        \item Ensures that expensive built-in function calls (e.g.,
          \texttt{max(data)}) are detected when redundant.
        \item Verifies that lightweight built-in functions (e.g.,
          \texttt{abs(-5)}) are ignored.
      \end{itemize}

    \item \textbf{Ensuring Accuracy and Performance}
      \begin{itemize}
        \item Ensures that detection does not introduce false
          positives by incorrectly flagging calls that differ in
          meaningful ways.
        \item Verifies that detection performance scales efficiently
          with large codebases.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/analyzers/test_repeated_calls_analyzer.py}{here}.

  \subsubsection{Long Message Chain}

  \textbf{Goal:} The long message chain detection module identifies
  method call chains that exceed a predefined threshold (default: 5
  calls). The following unit tests verify the detection capabilities. \\

  \noindent The tests evaluate the detection of long message chains
  by verifying that sequences exceeding the threshold, such as five
  consecutive messages within a short time, are correctly identified.
  They include edge cases, like chains just below the threshold or
  varying in length, to assess the robustness of the detection logic\\

  \noindent\textbf{Target requirement(s):} FR2 ~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Detects exact five calls chain}
        \begin{itemize}
        \item Ensures that a method chain with exactly five calls is flagged
        \end{itemize}

    \item \textbf{Detects six calls chain}
        \begin{itemize}
        \item Verifies that a chain with six method calls is detected as a smell
        \end{itemize}

    \item \textbf{Ignores chain of four calls}
        \begin{itemize}
        \item Ensures that a chain with only four calls (below threshold) is not flagged
        \end{itemize}

    \item \textbf{Detects chain with attributes and calls}
        \begin{itemize}
        \item Tests detection of a chain that involves both attribute access and method calls
        \end{itemize}

    \item \textbf{Detects chain inside a loop}
        \begin{itemize}
        \item Ensures detection of a chain meeting the threshold when inside a loop
        \end{itemize}

    \item \textbf{Detects multiple chains on one line}
        \begin{itemize}
        \item Verifies that only the first long chain on a single line is reported
        \end{itemize}

    \item \textbf{Ignores separate statements}
        \begin{itemize}
        \item Ensures that separate method calls across multiple statements are not mistakenly combined
        \end{itemize}

    \item \textbf{Ignores short chain comprehension}
        \begin{itemize}
        \item Ensures that a short chain within a list comprehension is not flagged
        \end{itemize}

    \item \textbf{Detects long chain comprehension}
        \begin{itemize}
        \item Verifies that a list comprehension with a long method chain is detected
        \end{itemize}

    \item \textbf{Detects five separate long chains}
        \begin{itemize}
        \item Ensures that multiple long chains on separate lines within the same function are individually detected
        \end{itemize}

    \item \textbf{Ignores element access chains}
        \begin{itemize}
        \item Confirms that attribute and index lookups without method calls are not flagged
        \end{itemize}
\end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/analyzers/test_long_message_chain_analyzer.py}{here}

  \subsubsection{Long Lambda Element}

  \textbf{Goal:} The following unit tests verify the correct
  detection of long lambda functions based on expression count and
  character length thresholds.\\

  \noindent The goal of this test suite is to ensure the detection
  system accurately identifies long lambda
  functions based on predefined complexity thresholds, such as
  expression count and character
  length. It verifies that the detection logic is precise, avoiding
  false positives for trivial
  or short lambdas while correctly flagging complex or lengthy ones.
  The tests cover edge cases,
  including nested lambdas and inline lambdas passed as arguments to
  functions like map or
  filter. Additionally, the suite ensures degenerate cases, such as
  empty or extremely short
  lambdas, are not mistakenly flagged. By validating these scenarios,
  the detection system
  maintains robustness and reliability in identifying problematic
  lambda expressions.\\

  \noindent\textbf{Target requirement(s):} FR2 ~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{No lambdas present}
        \begin{itemize}
        \item Ensures that when no lambda functions exist in the code, no smells are detected
        \end{itemize}

    \item \textbf{Short single lambda}
        \begin{itemize}
        \item Confirms that a single short lambda (well under the length threshold) with only one expression is not flagged
        \end{itemize}

    \item \textbf{Lambda exceeding expression count}
        \begin{itemize}
        \item Detects a lambda function that contains multiple expressions, exceeding the threshold for complexity
        \end{itemize}

    \item \textbf{Lambda exceeding character length}
        \begin{itemize}
        \item Identifies a lambda function that surpasses the maximum allowed character length, making it difficult to read
        \end{itemize}

    \item \textbf{Lambda exceeding both thresholds}
        \begin{itemize}
        \item Flags a lambda function that is both too long in character length and contains too many expressions
        \end{itemize}

    \item \textbf{Nested lambda functions}
        \begin{itemize}
        \item Ensures that both outer and inner nested lambdas are properly detected as long expressions
        \end{itemize}

    \item \textbf{Inline lambda passed to function}
        \begin{itemize}
        \item Detects lambda functions that are passed inline to functions like \texttt{map} and \texttt{filter} when they exceed the complexity thresholds
        \end{itemize}

    \item \textbf{Trivially short lambda function}
        \begin{itemize}
        \item Verifies that degenerate cases, such as a lambda with no real body or trivial operations, are not mistakenly flagged
        \end{itemize}
\end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/analyzers/test_long_lambda_element_analyzer.py}{here}

  \subsection{CodeCarbon Measurement Module}
  \textbf{Goal:} The CodeCarbon Measurement module is designed to
  measure the carbon emissions associated with running a specific
  piece of code. It integrates with the CodeCarbon library, which
  calculates the energy consumption based on the execution of a
  Python script.\\ \\
  \noindent The tests validate the functionality of the system in
  measuring and handling energy consumption data for code execution.
  They ensure that the system correctly tracks carbon emissions using
  the CodeCarbon library, handles both successful and failed
  subprocess executions, processes emissions data from CSV files, and
  appropriately logs all relevant events.\\

  \noindent \textbf{Target requirement(s):} FR5, FR6,PR-RFT1,
  PR-SCR1~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Successful Energy Measurement}
    \begin{itemize}
      \item Verifies correct emissions tracking when CodeCarbon returns valid float values
      \item Confirms proper subprocess execution and tracker API calls
      \item Ensures emissions value is stored in the meter instance
    \end{itemize}

  \item \textbf{Null Emissions Handling}
    \begin{itemize}
      \item Validates system behavior when CodeCarbon returns None
      \item Confirms meter stores None without errors
    \end{itemize}

  \item \textbf{Unexpected Emissions Type Handling}
    \begin{itemize}
      \item Tests proper logging and null conversion for non-float/non-None returns
      \item Specifically verifies handling of string and NaN values
    \end{itemize}

  \item \textbf{Subprocess Failure Resilience}
    \begin{itemize}
      \item Confirms system logs errors but preserves emissions data when subprocess fails
      \item Verifies tracker still stops properly after subprocess exceptions
    \end{itemize}

  \item \textbf{CSV Data Extraction}
    \begin{itemize}
      \item Validates correct parsing of multi-row emissions CSV files
      \item Ensures last row is properly returned as current emissions
    \end{itemize}

  \item \textbf{Missing Emissions File Handling}
    \begin{itemize}
      \item Tests proper error logging when emissions file is missing
      \item Verifies None is returned and stored in emissions\_data
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/src/ecooptimizer/measurements/codecarbon_energy_meter.py}{here}.

  \subsection{Refactoring Module}

  \subsubsection{Refactorer Controller}

  \textbf{Goal:} These tests verify the behavior of the
  RefactorerController in various scenarios, ensuring that it handles
  refactoring correctly and interacts with external components as expected.\\

  \noindent \textbf{Target requirement(s):} FR5, UHR-UPL1~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Verifying Successful Refactoring with a Valid Refactorer}
      \begin{itemize}
        \item Ensures that the RefactorerController correctly runs
          the refactorer when a valid refactorer is available.
        \item Checks that the logger captures the refactoring event.
        \item Ensures the refactor method is called with the expected arguments.
        \item Verifies that the modified file path is correctly generated.
      \end{itemize}

    \item \textbf{Handling Case When No Refactorer is Found}
      \begin{itemize}
        \item Ensures that if no refactorer is found for a given
          smell, the RefactorerController raises a \texttt{NotImplementedError}.
        \item Confirms that the appropriate error message is logged.
      \end{itemize}

    \item \textbf{Handling Multiple Refactorer Calls for the Same Smell}
      \begin{itemize}
        \item Tests the behavior when the refactorer is called
          multiple times for the same smell.
        \item Ensures that the smell counter is updated correctly.
        \item Verifies that unique file names are generated for each call.
      \end{itemize}

    \item \textbf{Verifying the Behavior When Overwrite is Disabled}
      \begin{itemize}
        \item Ensures that when the overwrite flag is set to false,
          the refactor method is called with the correct argument.
        \item Verifies that files are not overwritten when this
          option is disabled.
      \end{itemize}

    \item \textbf{Handling Case Where No Files are Modified}
      \begin{itemize}
        \item Checks that when no files are modified by the
          refactorer, the RefactorerController correctly returns an
          empty list of modified files.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/controllers/test_refactorer_controller.py}{here}.

  \subsubsection{String Concatenation in a Loop}

  \textbf{Goal:} The refactoring module transforms inefficient string
  concatenation into a more performant approach using list
  accumulation and \texttt{''.join()}. Unit tests for this module ensure that:\\

  \noindent\textbf{Target requirement(s):} FR3, FR6~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Proper Initialization of the List}
      \begin{itemize}
        \item The string variable being concatenated is replaced with
          a list at the start of the loop.
        \item The list is initialized to an empty list if the initial
          string is obviously empty.
        \item Otherwise, the list is initialized with the initial
          string as the first item.
      \end{itemize}

    \item \textbf{Correct Usage of List Methods in the Loop}
      \begin{itemize}
        \item The \texttt{append()} method is used instead of
          assignments inside the loop.
        \item The \texttt{insert()} method is used to insert values
          at the beginning of the string.
        \item If the string is re-initialized inside the loop, the
          \texttt{clear()} method is used to empty the list before
          re-initializing it.
        \item If multiple concatenations happen in the same loop,
          each is accumulated in the correct list.
      \end{itemize}

    \item \textbf{Correct String Joining After the Loop}
      \begin{itemize}
        \item The accumulated list is joined using
          \texttt{''.join(list)} after the loop.
        \item The final assignment replaces the original
          concatenation variable with the joined string.
      \end{itemize}

    \item \textbf{Handling of Edge Cases from Detection}
      \begin{itemize}
        \item If the concatenation variable is accessed inside the
          loop, no refactoring is applied.
        \item Conditional concatenations result in conditional list appends.
        \item Order-sensitive concatenations (e.g., prefix
          insertions) preserve the original behavior.
      \end{itemize}

    \item \textbf{Preserving Readability and Maintainability}
      \begin{itemize}
        \item The refactored code maintains readability and does not
          introduce unnecessary complexity.
        \item Nested loop modifications preserve the correct scoping
          of the refactored lists.
        \item The refactored code maintains proper formatting and indentation.
        \item Unnecessary modifications to unrelated code are avoided.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_str_concat_in_loop_refactor.py}{here}.

  \subsubsection{Long Element Chain}

  \textbf{Goal:} The long element chain refactoring module simplifies
  deeply nested dictionary accesses by flattening them into top-level
  keys while preserving functionality. The following unit tests
  verify the correctness of this transformation.\\

  \noindent The tests assess the ability to detect and refactor long
  element chains by verifying correct dictionary transformations,
  accessing pattern updates, and handling of various data structures.
  Edge cases, such as shallow accesses, multiple affected files, and
  mixed depths of access, are included to ensure robustness. \\

  \noindent\textbf{Target requirement(s):} FR3, FR6, PR-PAR3~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Identifies and Refactors Basic Nested Dictionary Access}
      \begin{itemize}
        \item Ensures that deeply nested dictionary keys are detected
          and refactored correctly.
      \end{itemize}

    \item \textbf{Refactors Dictionary Accesses Across Multiple Files}
      \begin{itemize}
        \item Verifies that dictionary changes propagate correctly
          when a deeply nested dictionary is accessed in different files.
      \end{itemize}

    \item \textbf{Handles Dictionary Access via Class Attributes}
      \begin{itemize}
        \item Ensures that nested dictionary accesses within class
          attributes are correctly detected and refactored.
      \end{itemize}

    \item \textbf{Ignores Shallow Dictionary Accesses}
      \begin{itemize}
        \item Verifies that dictionary accesses below the predefined
          threshold remain unchanged.
      \end{itemize}

    \item \textbf{Handles Multiple Long Element Chains in the Same File}
      \begin{itemize}
        \item Ensures that all occurrences of excessive dictionary
          accesses in a file are refactored individually.
      \end{itemize}

    \item \textbf{Detects and Refactors Mixed Access Depths}
      \begin{itemize}
        \item Confirms that the module correctly differentiates and
          processes deep accesses while ignoring shallow ones.
      \end{itemize}

    \item \textbf{Validates Resulting Metadata and Formatting}
      \begin{itemize}
        \item Confirms that the refactored output includes
          well-structured results, such as correct message IDs,
          types, and occurrences.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_long_element_chain_refactor.py}{here}.

  \subsubsection{Member Ignoring Method}

  \textbf{Goal:} The member ignoring method refactoring module
  ensures that methods that do not reference instance attributes or
  methods are converted into static methods. This transformation
  improves code clarity, enforces proper design patterns, and
  eliminates unnecessary instance bindings. The following unit tests
  validate the correctness of this functionality.\\

  \noindent The tests assess the correct detection of methods that
  can be converted to static methods, proper removal of \texttt{self}
  as a parameter, correct updating of method calls, and handling of
  inheritance. Edge cases such as instance-dependent methods,
  overridden methods, and preserving existing decorators are also considered.\\

  \noindent \textbf{Target requirement(s):} FR3, FR6~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Correct Addition of the \texttt{@staticmethod} Decorator}
      \begin{itemize}
        \item The method receives the \texttt{@staticmethod}
          decorator when it does not use \texttt{self} or any
          instance attributes.
        \item The decorator is added directly above the method
          definition, preserving any existing decorators.
      \end{itemize}

    \item \textbf{Removal of the \texttt{self} Parameter}
      \begin{itemize}
        \item The first parameter of the method is removed if it is
          named \texttt{self}.
        \item Other parameters remain unchanged.
        \item The method signature is correctly adjusted to reflect the removal.
      \end{itemize}

    \item \textbf{Modify Instance Calls to the Method}
      \begin{itemize}
        \item Instance objects of the class calling the method will
          be modified to use a static call directly from the class itself.
        \item Calls in a different file from the one where the smell
          was detected will also have the appropriate calls modified.
      \end{itemize}

    \item \textbf{Handling of Methods in Classes with Inheritance}
      \begin{itemize}
        \item If a subclass instance calls the method, the call is
          also modified.
        \item If a method is overridden in a subclass, refactoring is
          \textbf{not} applied.
      \end{itemize}

    \item \textbf{Ensuring No Modification to Instance-Dependent Methods}
      \begin{itemize}
        \item Methods that reference \texttt{self} directly (e.g.,
          \texttt{self.attr}, \texttt{self.method()}) are not modified.
        \item Methods that indirectly access instance attributes
          (e.g., through another method call) are left unchanged.
      \end{itemize}

    \item \textbf{Preserving Readability and Maintainability}
      \begin{itemize}
        \item The refactored code maintains proper formatting and indentation.
        \item Unnecessary modifications to unrelated code are avoided.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_member_ignoring_method.py}{here}.

  \subsubsection{Use a Generator}

  \textbf{Goal:} The use-a-generator refactoring module optimizes
  list comprehensions used within functions like \texttt{all()} and
  \texttt{any()} by transforming them into generator expressions.
  This refactoring improves memory efficiency while maintaining code
  correctness. The following unit tests validate the accuracy of this
  functionality.\\

  \noindent The tests ensure that list comprehensions inside
  \texttt{all()} and \texttt{any()} calls are correctly converted to
  generator expressions while preserving original behavior.
  Additional test cases verify proper handling of multiline
  comprehensions, nested conditions, and various iterable types. Edge
  cases, such as improperly formatted comprehensions or
  comprehensions spanning multiple lines, are also considered.\\

  \noindent \textbf{Target requirement(s):} FR3, FR5, FR6, PR-PAR3~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Refactoring List Comprehensions Inside \texttt{all()} Calls}
      \begin{itemize}
        \item Ensures that list comprehensions within \texttt{all()}
          are transformed into generator expressions.
        \item Validates that the transformation preserves original
          functionality.
      \end{itemize}

    \item \textbf{Refactoring List Comprehensions Inside \texttt{any()} Calls}
      \begin{itemize}
        \item Ensures that list comprehensions within \texttt{any()}
          are transformed into generator expressions.
        \item Verifies that the modified code remains functionally
          identical to the original.
      \end{itemize}

    \item \textbf{Handling Multi-line List Comprehensions}
      \begin{itemize}
        \item Ensures that multi-line list comprehensions are
          refactored correctly.
        \item Verifies that proper indentation and formatting are
          preserved after transformation.
      \end{itemize}

    \item \textbf{Handling Edge Cases}
      \begin{itemize}
        \item Ensures that improperly formatted comprehensions do not
          result in errors.
        \item Confirms that refactoring does not introduce
          unnecessary modifications or change unrelated code.
      \end{itemize}

    \item \textbf{Ensuring Correct Formatting and Readability}
      \begin{itemize}
        \item Validates that refactored code adheres to Python’s
          style guidelines.
        \item Ensures that readability and maintainability are
          preserved after refactoring.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_list_comp_any_all_refactor.py}{here}.

  \subsubsection{Cache Repeated Calls}

  \textbf{Goal:} The cache repeated calls refactoring module
  optimizes redundant function and method calls by storing results in
  local variables, reducing unnecessary recomputation. This
  refactoring enhances performance by eliminating duplicate
  executions of expensive operations. The following unit tests
  validate the accuracy of this functionality.\\

  \noindent The tests ensure that function and method calls that
  produce identical results are cached and replaced with stored
  values where applicable. Additional test cases verify proper
  handling of method calls on objects, preserving object state, and
  integrating with function arguments. Edge cases, such as function
  calls with varying inputs, method calls on different instances, and
  presence of docstrings, are also considered.\\

  \noindent \textbf{Target requirement(s):} FR3, FR5, FR6, PR-PAR3,
  PR-PAR2~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Refactoring Repeated Function Calls}
      \begin{itemize}
        \item Ensures that repeated function calls within a scope are
          replaced with cached results.
        \item Validates that caching is only applied when function
          arguments remain unchanged.
      \end{itemize}

    \item \textbf{Refactoring Repeated Method Calls}
      \begin{itemize}
        \item Ensures that method calls on the same object instance
          are cached and replaced with stored values.
        \item Verifies that method calls on different object
          instances are not incorrectly refactored.
      \end{itemize}

    \item \textbf{Handling Object State Modifications}
      \begin{itemize}
        \item Ensures that method calls on objects whose attributes
          change between calls are not cached.
        \item Verifies that method calls are only cached when no
          state changes occur.
      \end{itemize}

    \item \textbf{Handling Edge Cases}
      \begin{itemize}
        \item Ensures that function calls with varying arguments are
          not incorrectly cached.
        \item Confirms that caching does not interfere with function
          scope, closures, or nested function calls.
      \end{itemize}

    \item \textbf{Refactoring in the Presence of Docstrings}
      \begin{itemize}
        \item Ensures that refactoring does not alter function
          behavior when docstrings are present.
        \item Verifies that caching maintains readability and proper formatting.
      \end{itemize}

    \item \textbf{Ensuring Correct Formatting and Readability}
      \begin{itemize}
        \item Validates that refactored code adheres to Python’s
          style guidelines.
        \item Ensures that readability and maintainability are
          preserved after refactoring.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_repeated_calls.py}{here}.

  \subsubsection{Long Parameter List}

  \textbf{Goal:} The Long Parameter List refactoring module replaces
  function definitions and calls involving a large number of
  parameters with grouped parameter instances. This enhances the
  efficiency of the code as related parameters are grouped together
  into instantiated parameters. The validity of the functionality can
  be ensured through unit tests.\\

  \noindent The tests ensure that all cases for function
  declarations, including constructors, instance methods, static
  methods and standalone functions are updated with the group
  parameter instances. Similarly, corresponding calls to these
  functions as well as references to original parameters are
  preserved. The refactored result also preserves use of default
  values in function signature and positional arguments in function calls.\\

  \noindent \textbf{Target requirement(s):} FR3, FR6~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Handle Constructor with More Parameters than
      Configured Limit, All Used}
      \begin{itemize}
        \item The refactor correctly handles constructors with 8
          parameters, including a mix of positional and keyword arguments.
        \item Function signature and instantiation are updated to
          include all 8 parameters.
      \end{itemize}

    \item \textbf{Handle Constructor More Parameters than Configured
      Limit, some of which are Unused}
      \begin{itemize}
        \item Constructor correctly handles unused parameters by
          excluding them from the updated signature and instantiation.
        \item The refactor updates the constructor and function body
          to properly reflect the used parameters.
      \end{itemize}

    \item \textbf{Handle Instance Method More Parameters than
      Configured Limit (2 Defaults)}
      \begin{itemize}
        \item The refactor correctly handles instance methods with 8
          parameters, including default values for some.
        \item The method signature and instantiation are updated to
          preserve the default values while reflecting the used parameters.
      \end{itemize}

    \item \textbf{Handle Instance Method Refactor with More
      Parameters than Configured Limit, some of which are Unused}
      \begin{itemize}
        \item Unused parameters in the instance method are correctly
          excluded and grouped into new data and config parameter classes.
        \item Function maintains the correct structure and that the
          method now accepts the new class objects instead of
          individual parameters.
      \end{itemize}

    \item \textbf{Handle Static Method Refactor with More Parameters
        than Configured Limit, some of which are Unused and some have
      Default Values}
      \begin{itemize}
        \item Unused parameters are correctly excluded from the
          static method signature and instantiation.
        \item Parameters with default values are properly maintained
          in the updated method signature and body.
      \end{itemize}

    \item \textbf{Handle Standalone Function Refactor with More
      Parameters than Configured Limit, some of which are Unused}
      \begin{itemize}
        \item Unused parameter is excluded from the function
          signature after the refactor.
        \item Refactored function uses grouped parameter classes to
          handle the remaining parameters efficiently.
      \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_long_parameter_list_refactor.py}{here}.

  \subsubsection{Long Message Chain}

  \textbf{Goal:} The following unit tests verify the correctness of
  refactoring long element chains into more readable and efficient
  code while maintaining valid Python syntax.\\

  \noindent This test suite focuses on validating the refactoring of
  long method chains into
  intermediate variables, improving code readability while preserving
  functionality.
  It ensures that simple method chains are split correctly and that
  special cases,
  such as f-strings or chains with arguments, are handled
  appropriately. The tests
  also verify that refactoring maintains proper indentation,
  especially within nested
  blocks like if statements or loops. Additionally, the suite confirms that
  refactoring does not alter the behavior of the original code, even in contexts
  like print statements. By testing both long and short chains, the
  suite ensures
  consistency and correctness across various scenarios.\\

  \noindent \textbf{Target requirement(s):} FR5, FR6, FR3 ~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Basic method chain refactoring}
        \begin{itemize}
            \item Ensures that a simple method chain is refactored correctly into separate intermediate variables.
        \end{itemize}

    \item \textbf{F-string chain refactoring}
        \begin{itemize}
            \item Verifies that method chains applied to f-strings are properly broken down while preserving correctness.
        \end{itemize}
      
    \item \textbf{Modifications even if the chain is not long}
        \begin{itemize}
            \item Ensures that method chains are refactored consistently, even if they do not exceed the length threshold.
        \end{itemize}
      
    \item \textbf{Proper indentation preserved}
        \begin{itemize}
            \item Confirms that the refactored code maintains the correct indentation when inside a block statement such as an \texttt{if} condition.
        \end{itemize}

    \item \textbf{Method chain with arguments}
        \begin{itemize}
            \item Tests that method chains containing arguments (e.g., \texttt{replace("H", "J")}) are correctly refactored.
        \end{itemize}

    \item \textbf{Print statement preservation}
        \begin{itemize}
            \item Ensures that method chains within a \texttt{print} statement are refactored without altering their functionality.
        \end{itemize}
      
    \item \textbf{Nested method chains}
        \begin{itemize}
            \item Verifies that nested method chains (e.g., method calls on method results) are properly refactored into intermediate variables.
        \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_long_message_chain_refactoring.py}{here}

  \subsubsection{Long Lambda Element}

  \textbf{Goal:} The following unit tests verify the correctness of
  refactoring long lambda expressions into named functions while
  maintaining valid Python syntax and preserving code functionality.\\

  \noindent The goal of this test suite is to ensure long lambda
  expressions are refactored into named
  functions while maintaining code functionality, readability, and
  proper syntax. It verifies
  that simple single-line lambdas are converted correctly and that
  more complex cases, such as
  multi-line lambdas or those with multiple parameters, are handled
  appropriately. The tests
  ensure that refactoring preserves the original behavior of the
  code, even for lambdas used
  as keyword arguments or passed to functions like map or reduce.
  Additionally, the suite
  confirms that no unnecessary changes, such as added print
  statements, are introduced during
  refactoring. By covering a wide range of cases, the suite ensures
  the refactoring process
  is both reliable and effective.\\

  \noindent \textbf{Target requirement(s):} FR5, FR6, FR3 ~\cite{SRS} \\

  \begin{itemize}
    \item \textbf{Basic lambda conversion}
        \begin{itemize}
            \item Verifies that a simple single-line lambda expression is correctly converted into a named function with proper indentation and structure.
        \end{itemize}
    
    \item \textbf{No extra print statements}
        \begin{itemize}
            \item Ensures that the refactoring process does not introduce unnecessary print statements when converting lambda expressions.
        \end{itemize}
      
    \item \textbf{Lambda in function argument}
        \begin{itemize}
            \item Tests that lambda expressions used as arguments to other functions (e.g., in \texttt{map()} calls) are properly refactored while maintaining the original function call structure.
        \end{itemize}
      
    \item \textbf{Multi-argument lambda}
        \begin{itemize}
            \item Verifies that lambda expressions with multiple parameters are correctly converted into named functions with the appropriate parameter list.
        \end{itemize}
      
    \item \textbf{Lambda with keyword arguments}
        \begin{itemize}
            \item Ensures that lambda expressions used as keyword arguments in function calls are properly refactored while preserving the original keyword argument syntax and indentation.
        \end{itemize}
      
    \item \textbf{Very long lambda function}
        \begin{itemize}
            \item Tests the refactoring of complex, multi-line lambda expressions with extensive mathematical operations, verifying that the converted function maintains the original logic and structure.
        \end{itemize}
  \end{itemize}

  \noindent The test cases for this module can be found
  \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/tests/refactorers/test_long_lambda_element_refactoring.py}{here}

\subsection{VsCode Plugin}

\subsubsection{Configure Workspace Command}

\textbf{Goal:} The configure workspace command identifies valid workspace folders containing Python files and prompts the user to select one. Upon selection, the workspace is marked as configured and saved to persistent state for future operations.

\medskip

\noindent The tests validate that valid workspace folders are detected, Python files are identified correctly, user selections are respected, and appropriate updates are made to both VS Code context and extension state.

\medskip

\noindent\textbf{Target requirement(s):} FR1, FR2~\cite{SRS}

\begin{itemize}
  \item \textbf{Folder Scanning}
    \begin{itemize}
      \item Detects top-level and nested directories containing \texttt{.py} files.
      \item Correctly identifies directories with Python entry points (e.g., \texttt{main.py} or \texttt{\_\_init\_\_.py}).
    \end{itemize}

  \item \textbf{Quick Pick Interaction}
    \begin{itemize}
      \item Displays a list of valid Python folders.
      \item Accepts user selection and confirms configuration.
    \end{itemize}

  \item \textbf{Workspace State Update}
    \begin{itemize}
      \item Stores selected folder path under the configured workspace key.
      \item Sets VS Code context key \texttt{workspaceState.workspaceConfigured} to true.
    \end{itemize}

  \item \textbf{Feedback to User}
    \begin{itemize}
      \item Shows information message indicating selected folder.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/commands/configureWorkspace.test.ts}{here}.

\subsubsection{Reset Configuration Command}

\textbf{Goal:} The reset configuration command prompts the user for confirmation and, if accepted, clears the stored workspace path and resets the internal plugin context to an unconfigured state.

\medskip

\noindent The tests verify that the workspace state is properly reset only when the user confirms the action. The system must also update the relevant VS Code context key to reflect the unconfigured state.

\medskip

\noindent\textbf{Target requirement(s):} FR3~\cite{SRS}

\begin{itemize}
  \item \textbf{User Confirmation}
    \begin{itemize}
      \item Prompts user with a warning message before clearing configuration.
    \end{itemize}

  \item \textbf{Workspace State Reset}
    \begin{itemize}
      \item Clears the stored workspace path from extension state.
    \end{itemize}

  \item \textbf{Context Reset}
    \begin{itemize}
      \item Updates VS Code context key \texttt{workspaceState.workspaceConfigured} to false.
    \end{itemize}

  \item \textbf{Cancel Handling}
    \begin{itemize}
      \item Skips reset process if the user cancels the confirmation dialog.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/commands/resetConfiguration.test.ts}{here}.

\subsubsection{File and Folder Smell Detection Commands}

\textbf{Goal:} The \texttt{detectSmellsFile} and \texttt{detectSmellsFolder} commands are responsible for initiating the detection of code smells in individual files and entire directories, respectively. These commands coordinate the interaction between the cache system, backend smell detection API, and the UI rendering logic within the SmellsViewProvider.

\medskip

\noindent These unit tests ensure robust handling of various scenarios, including invalid input files, empty folders, disabled smell settings, cached data reuse, server downtime, backend errors, and recursive directory scans. They also validate that user-facing messages, caching, and smell highlighting are executed correctly based on system state.

\medskip

\noindent\textbf{Target requirement(s):} FR2, FR3, FR15, OER-IAS2~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Skipping Non-Python and Untitled Files}
    \begin{itemize}
      \item The file detection command ignores non-Python files and unsaved/untitled documents.
    \end{itemize}

  \item \textbf{Using Cached Smells}
    \begin{itemize}
      \item Cached smells are reused when the file hash and settings match.
      \item Cached smells are immediately rendered in the Smells View.
    \end{itemize}

  \item \textbf{Fetching Smells from Backend}
    \begin{itemize}
      \item When no cache is available, the backend is called to fetch smells.
      \item Retrieved smells are stored in the cache and displayed in the UI.
    \end{itemize}

  \item \textbf{Handling No Enabled Smells}
    \begin{itemize}
      \item A warning message is shown when no smells are enabled in the configuration.
    \end{itemize}

  \item \textbf{Handling No Smells Found}
    \begin{itemize}
      \item The tool displays a message indicating that the file has no detectable smells.
      \item The cache is updated to reflect the empty result.
    \end{itemize}

  \item \textbf{Handling API and Server Errors}
    \begin{itemize}
      \item Server-down conditions show appropriate warning messages.
      \item API response failures result in error messages and a "failed" status in the UI.
      \item Unexpected thrown exceptions are caught and logged to the output channel.
    \end{itemize}

  \item \textbf{Folder Analysis and Recursion}
    \begin{itemize}
      \item The folder command scans recursively to identify `.py` files and shows progress.
      \item A warning is shown if no Python files are found in the directory.
      \item The total number of analyzed files is reported to the user.
    \end{itemize}

  \item \textbf{Handling Folder Access Errors}
    \begin{itemize}
      \item Directory scan failures (e.g., permission errors) are caught and logged cleanly without crashing the extension.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/commands/detectSmells.test.ts}{here}.

\subsubsection{Export Metrics Command}

\textbf{Goal:} The export metrics command saves workspace-level energy metrics to a local JSON file. This test suite ensures it handles missing data, invalid paths, file vs. directory distinction, and file system errors gracefully.

\medskip

\noindent The tests validate correct behavior across different workspace path types, proper access to extension state, and informative error messaging for common failure scenarios.

\medskip

\noindent\textbf{Target requirement(s):} FR6, FR15, OER-IAS3~\cite{SRS} \\

\begin{itemize}
  \item \textbf{No Metrics Data}
    \begin{itemize}
      \item The command shows an information message if there is no metrics data available to export.
    \end{itemize}

  \item \textbf{No Workspace Path Configured}
    \begin{itemize}
      \item The command shows an error message if the workspace path is missing or unset.
    \end{itemize}

  \item \textbf{Export to Workspace Directory}
    \begin{itemize}
      \item If the workspace path is a directory, the metrics JSON is saved directly inside it.
      \item The user is notified of the output file location.
    \end{itemize}

  \item \textbf{Export to Parent of File Path}
    \begin{itemize}
      \item If the workspace path is a file, the parent directory is used for export.
    \end{itemize}

  \item \textbf{Invalid Path Type Handling}
    \begin{itemize}
      \item The command shows an error message if the path type is unknown or unsupported.
    \end{itemize}

  \item \textbf{Filesystem Access Errors}
    \begin{itemize}
      \item The command handles and reports errors when accessing the file system (e.g., stat failures).
    \end{itemize}

  \item \textbf{File Write Failures}
    \begin{itemize}
      \item If the write operation fails, the user is shown an error message indicating export failure.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/commands/exportMetricsData.test.ts}{here}.

\subsubsection{Filter Smell Command Registration}

\textbf{Goal:} This command module registers all filter-related UI commands used to configure active smells and their parameters in the EcoOptimizer sidebar. The tests validate command registration, user interaction (e.g., input box), and the proper delegation to `FilterViewProvider`.

\medskip

\noindent The tests ensure that smell toggling, option editing, mass selection, and default resets all call the correct provider methods and handle edge cases such as missing inputs or invalid user values.

\medskip

\noindent\textbf{Target requirement(s):} FR13, FR14, OER-IAS1~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Command Registration}
    \begin{itemize}
      \item Each command is correctly registered with VS Code using the expected command IDs.
      \item All registered disposables are added to the extension context’s subscriptions.
    \end{itemize}

  \item \textbf{Toggling Individual Smells}
    \begin{itemize}
      \item The command toggles a specific smell using \texttt{toggleSmell}.
    \end{itemize}

  \item \textbf{Editing Smell Options – Valid Input}
    \begin{itemize}
      \item The user is prompted for a new value via an input box.
      \item The new numeric value is passed to \texttt{updateOption} and the filter view is refreshed.
    \end{itemize}

  \item \textbf{Editing Smell Options – Invalid or Missing Input}
    \begin{itemize}
      \item If the user enters a non-numeric value, the option is not updated.
      \item If the smell or option key is missing, an error message is shown.
    \end{itemize}

  \item \textbf{Select/Deselect All Smells}
    \begin{itemize}
      \item The \texttt{selectAllFilterSmells} and \texttt{deselectAllFilterSmells} commands call \texttt{setAllSmellsEnabled(true/false)} respectively.
    \end{itemize}

  \item \textbf{Reset to Filter Defaults}
    \begin{itemize}
      \item The \texttt{setFilterDefaults} command calls \texttt{resetToDefaults} on the filter provider.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/commands/filterSmells.test.ts}{here}.

\subsubsection{Refactor Workflow Commands}

\textbf{Goal:} The refactoring workflow includes the core commands responsible for initiating, applying, and discarding refactorings. These tests verify that backend communication, session setup, user feedback, and file handling work correctly across both individual and batch refactor scenarios.

\medskip

\noindent This suite ensures refactor commands respect workspace configuration, gracefully handle backend issues, display energy savings, manage diff editors, and correctly update workspace state and metrics. Error handling for edge cases like file system failures and missing data is also validated.

\medskip

\noindent\textbf{Target requirement(s):} FR3, FR4, FR15, OER-IAS4~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Workspace and Server Preconditions}
    \begin{itemize}
      \item Shows an error if the workspace is not configured.
      \item Shows a warning if the backend server is unavailable.
    \end{itemize}

  \item \textbf{Refactoring Execution – Single Smell}
    \begin{itemize}
      \item Initiates refactoring for one smell using the backend.
      \item Updates internal state and view providers.
      \item Displays progress and success messages.
    \end{itemize}

  \item \textbf{Refactoring Execution – All Smells of Same Type}
    \begin{itemize}
      \item Calls bulk API endpoint for all smells of the given type.
      \item Displays type-based success messages.
    \end{itemize}

  \item \textbf{Refactoring Failure Handling}
    \begin{itemize}
      \item Shows an error if the backend fails or throws.
      \item Resets UI state and hides any open diff editors.
    \end{itemize}

  \item \textbf{Session Initialization via \texttt{startRefactorSession}}
    \begin{itemize}
      \item Opens a VS Code diff editor with original and refactored files.
      \item Displays energy savings in an information message.
      \item Handles missing energy savings values gracefully (e.g., N/A).
    \end{itemize}

  \item \textbf{Accepting Refactorings}
    \begin{itemize}
      \item Copies refactored files into the workspace.
      \item Updates metrics data and clears smell cache for affected files.
      \item Sets file status to "outdated" in the UI.
      \item Handles missing session data or filesystem errors.
    \end{itemize}

  \item \textbf{Rejecting Refactorings}
    \begin{itemize}
      \item Cleans up session state and restores file status.
      \item Handles and logs cleanup failures if they occur.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/commands/refactorSmell.test.ts}{here}.

\subsubsection{Wipe Workspace Cache Command}

\textbf{Goal:} This command allows users to reset all cached smell data and file statuses for the current workspace. The tests ensure correct user confirmation, safe cache clearing, and accurate feedback messaging across all user interaction scenarios.

\medskip

\noindent This module’s functionality is essential for maintaining control over stale or outdated results, especially in long development sessions. The tests verify proper UI messaging, cancellation handling, and full cache cleanup when confirmed.

\medskip

\noindent\textbf{Target requirement(s):} FR10, OER-IAS5~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Confirmation Prompt}
    \begin{itemize}
      \item A warning dialog is shown before clearing the workspace cache.
      \item The dialog must include a modal and an explicit "Confirm" action.
    \end{itemize}

  \item \textbf{Cache Clearing and UI Refresh}
    \begin{itemize}
      \item If the user confirms, all cached smells are deleted.
      \item The SmellsView is refreshed and statuses are reset.
      \item A success message is shown after completion.
    \end{itemize}

  \item \textbf{Cancellation and Dismissal Handling}
    \begin{itemize}
      \item If the user cancels or dismisses the dialog, no cache operations are performed.
      \item A message confirms that the operation was cancelled.
    \end{itemize}

  \item \textbf{Non-Confirm Input Handling}
    \begin{itemize}
      \item If the user clicks any button other than "Confirm", the operation is treated as cancelled.
    \end{itemize}

  \item \textbf{Message Validity}
    \begin{itemize}
      \item Success messages are shown only after the cache is successfully cleared.
      \item Cancel messages are mutually exclusive with success messages.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/commands/wipeWorkCache.test.ts}{here}.

\subsubsection{Workspace Modified Listener}

\textbf{Goal:} This listener monitors Python file changes and deletions within the configured workspace. Its responsibilities include invalidating outdated cache entries, updating UI statuses, and auto-triggering smell detection if "smell linting" is enabled. It also ensures resource cleanup upon disposal.

\medskip

\noindent The following tests verify correct event handling on file saves, edits, and deletions. They also validate configuration checks, error handling, and the listener's lifecycle management.

\medskip

\noindent\textbf{Target requirement(s):} FR10, FR16, OER-IAS5~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Initialization Behavior}
    \begin{itemize}
      \item Skips setup if no workspace path is configured.
      \item Initializes file watcher for `.py` files when path is configured.
    \end{itemize}

  \item \textbf{Handling File Changes}
    \begin{itemize}
      \item Clears smell cache and marks file as outdated if it exists in the cache.
      \item Skips files that are not in the cache.
      \item Logs error messages for cache invalidation failures.
    \end{itemize}

  \item \textbf{Handling File Deletions}
    \begin{itemize}
      \item Clears cache and removes file entry from view if file had cached smells.
      \item Skips deletion logic if file was not cached.
      \item Logs error if cache clearing fails.
    \end{itemize}

  \item \textbf{Smell Detection on Save}
    \begin{itemize}
      \item Triggers smell detection when a Python file is saved and smell linting is enabled.
      \item Ignores non-Python files.
    \end{itemize}

  \item \textbf{Disposal}
    \begin{itemize}
      \item Disposes of both the file watcher and save listener properly.
      \item Logs disposal confirmation.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/listeners/workspaceModifiedListener.test.ts}{here}.

\subsubsection{Backend Service Communication}

\textbf{Goal:} These backend API functions interact with the server for health checks, logging setup, smell detection, and smell refactoring. They must handle network issues gracefully, validate required inputs, and ensure proper feedback is logged in the output console.

\medskip

\noindent The following tests validate success and failure scenarios for each backend endpoint, including malformed requests, network errors, and missing workspace paths. They also verify correct request payloads and logging output.

\medskip

\noindent\textbf{Target requirement(s):} FR2, FR3, FR6, FR10, OER-IAS2~\cite{SRS}

\begin{itemize}
  \item \textbf{Server Health Check}
    \begin{itemize}
      \item Sets status to \texttt{UP} if `/health` responds successfully.
      \item Sets status to \texttt{DOWN} on error or network failure, and logs the issue.
    \end{itemize}

  \item \textbf{Logging Initialization}
    \begin{itemize}
      \item Sends proper payload to `/logs/init`.
      \item Returns success on valid response.
      \item Handles server or network errors with log messages and fallback.
    \end{itemize}

  \item \textbf{Smell Detection}
    \begin{itemize}
      \item Sends file path and smell configuration to `/smells`.
      \item Parses and returns detected smells on success.
      \item Throws clear errors and logs backend-provided details on failure.
    \end{itemize}

  \item \textbf{Refactor Single Smell}
    \begin{itemize}
      \item Sends smell and workspace path to `/refactor`.
      \item Throws error when workspace path is missing.
      \item Handles and logs both successful and failed backend responses.
    \end{itemize}

  \item \textbf{Refactor by Smell Type}
    \begin{itemize}
      \item Sends smell type and workspace path to `/refactor-by-type`.
      \item Validates required fields and logs backend failures.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/api/backend.test.ts}{here}.

\subsubsection{File Highlighter}

\textbf{Goal:} The file highlighter decorates code editor lines based on detected smells using user-defined styling preferences (underline, border, flashlight, etc.). This module ensures that the VS Code UI reflects current cache data and user configuration, applying or clearing decorations dynamically.

\medskip

\noindent The tests verify correct singleton instantiation, dynamic decoration updates, conditional smell filtering, and visual style application across multiple configurations and editor scenarios.

\medskip

\noindent\textbf{Target requirement(s):} FR8, FR13, OER-IAS1~\cite{SRS}

\begin{itemize}
  \item \textbf{Singleton Instantiation}
    \begin{itemize}
      \item Ensures only one instance of the highlighter is created.
    \end{itemize}

  \item \textbf{Highlight Update Triggers}
    \begin{itemize}
      \item Calls \texttt{highlightSmells} only for visible Python editors.
      \item Filters files correctly in \texttt{updateHighlightsForFile}.
    \end{itemize}

  \item \textbf{Highlight Rendering}
    \begin{itemize}
      \item Applies decorations for all enabled and valid smells.
      \item Skips rendering if no data is cached or smell is disabled.
      \item Ignores invalid line numbers.
    \end{itemize}

  \item \textbf{Custom Decoration Styles}
    \begin{itemize}
      \item Supports underline, flashlight, and border-arrow styles.
      \item Defaults to underline for unknown style keys.
    \end{itemize}

  \item \textbf{Decoration Disposal}
    \begin{itemize}
      \item \texttt{resetHighlights()} clears all decorations and disposes them properly.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/ui/fileHighlighter.test.ts}{here}.

\subsubsection{Hover Manager}

\textbf{Goal:} The hover manager displays contextual smell information when the user hovers over affected lines in Python files. It enriches the user experience by providing inline guidance and actionable links to trigger refactorings.

\medskip

\noindent The tests verify correct registration of the hover provider, graceful degradation when conditions are unmet, and correct Markdown formatting and behavior for smells occurring on the hovered line.

\medskip

\noindent\textbf{Target requirement(s):} FR13, FR16, OER-IAS1~\cite{SRS}

\begin{itemize}
  \item \textbf{Provider Registration}
    \begin{itemize}
      \item Registers hover provider for \texttt{python} language files.
      \item Adds registration to extension subscriptions.
    \end{itemize}

  \item \textbf{Hover Preconditions}
    \begin{itemize}
      \item Returns nothing for non-Python files.
      \item Returns nothing when no smells are cached.
      \item Returns nothing if no smells occur on the hovered line.
    \end{itemize}

  \item \textbf{Hover Display Logic}
    \begin{itemize}
      \item Creates and returns a \texttt{Hover} object with formatted message.
      \item Displays smell description and a clickable refactor command.
      \item Escapes special characters in Markdown to prevent formatting issues.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/ui/hoverManager.test.ts}{here}.

\subsubsection{Line Selection Manager}

\textbf{Goal:} The line selection manager adds inline decorations that summarize code smells present on the user's current line selection. It enhances feedback clarity while ensuring only one comment is shown at a time.

\medskip

\noindent The tests ensure that decoration is only applied to valid selections with associated smells, that it updates correctly when switching lines or clearing cache, and that decoration content reflects smell counts accurately.

\medskip

\noindent\textbf{Target requirement(s):} FR13, FR16, OER-IAS1~\cite{SRS}

\begin{itemize}
  \item \textbf{Initialization}
    \begin{itemize}
      \item Registers a callback on smell updates.
      \item Initializes internal state to null decoration and null last line.
    \end{itemize}

  \item \textbf{Comment Rendering}
    \begin{itemize}
      \item Skips rendering when no editor is active or selection is multiline.
      \item Skips when no smells are cached or no smells match the selected line.
      \item Skips if user selects the same line twice.
      \item Adds a comment with the smell type if one smell is present.
      \item Adds a comment with count when multiple smells exist.
      \item Decorations appear as inline text after the selected line.
    \end{itemize}

  \item \textbf{Comment Removal}
    \begin{itemize}
      \item Removes previous decorations before applying a new one.
      \item Removes comment when cache is cleared for the current file or for all files.
      \item Skips removal if unrelated file’s cache is cleared.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/ui/lineSelection.test.ts}{here}.

\subsubsection{Cache Initialization}

\textbf{Goal:} This module restores file statuses from previously cached analysis results when the extension is activated. It removes entries for deleted or out-of-scope files and updates the UI accordingly.

\medskip

\noindent The tests verify that cache entries outside the workspace or pointing to non-existent files are removed, and that remaining entries are properly restored into the UI. It also validates summary log reporting and gracefully handles missing workspace configuration.

\medskip

\noindent\textbf{Target requirement(s):} FR10, OER-CACHE1~\cite{SRS}

\begin{itemize}
  \item \textbf{No Workspace Configured}
    \begin{itemize}
      \item Skips initialization and logs a warning if no workspace path is set.
    \end{itemize}

  \item \textbf{Removing Invalid Cache Entries}
    \begin{itemize}
      \item Removes entries for files outside the configured workspace.
      \item Removes entries for files that are no longer present on disk.
    \end{itemize}

  \item \textbf{Restoring Valid Cache}
    \begin{itemize}
      \item Files with smells get status \texttt{passed} and corresponding smells are injected.
      \item Clean files are marked with status \texttt{no\_issues}.
    \end{itemize}

  \item \textbf{Summary Logging}
    \begin{itemize}
      \item Logs how many files were restored, how many had smells, and how many were removed from the cache.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/utils/initializeStatusesFromCache.test.ts}{here}.

\subsubsection{Smells Data Management}

\textbf{Goal:} This module provides utilities for reading, parsing, and retrieving code smell configurations. It handles loading/saving JSON configurations, filtering enabled smells, and resolving smell metadata for hover/tooltips.

\medskip

\noindent The tests validate the integrity and correctness of the config loading and writing process, proper filtering of enabled smells, and lookup logic for acronyms, names, and descriptions based on message IDs.

\medskip

\noindent\textbf{Target requirement(s):} FR4, FR5, OER-CONFIG1~\cite{SRS}

\begin{itemize}
  \item \textbf{loadSmells}
    \begin{itemize}
      \item Successfully loads a smells configuration from disk.
      \item Displays an error message if the file is missing or malformed.
    \end{itemize}

  \item \textbf{saveSmells}
    \begin{itemize}
      \item Writes updated configuration to disk.
      \item Displays an error message if the save operation fails.
    \end{itemize}

  \item \textbf{getFilterSmells}
    \begin{itemize}
      \item Returns the full dictionary of loaded smells.
    \end{itemize}

  \item \textbf{getEnabledSmells}
    \begin{itemize}
      \item Returns only smells marked as enabled.
      \item Includes parsed analyzer options with correct types.
    \end{itemize}

  \item \textbf{Metadata Resolvers}
    \begin{itemize}
      \item \texttt{getAcronymByMessageId} resolves acronyms.
      \item \texttt{getNameByMessageId} resolves full names.
      \item \texttt{getDescriptionByMessageId} resolves descriptions.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/utils/smellsData.test.ts}{here}.

\subsubsection{Tracked Diff Editors}

\textbf{Goal:} This module maintains a registry of active diff editors created during refactoring sessions. It provides utilities to register, query, and programmatically close diff tabs within the VS Code environment.

\medskip

\noindent The tests validate the correct registration and tracking of diff editors, accurate identification of tracked editors via URI comparison, and complete cleanup of tabs and memory state after closure operations.

\medskip

\noindent\textbf{Target requirement(s):} FR12, OER-IAS2~\cite{SRS}

\begin{itemize}
  \item \textbf{registerDiffEditor}
    \begin{itemize}
      \item Adds a URI pair to the tracked diff editors set.
      \item Supports multiple pairs without interference.
    \end{itemize}

  \item \textbf{isTrackedDiffEditor}
    \begin{itemize}
      \item Returns true only for previously registered URI pairs.
      \item Fails gracefully for unregistered or mismatched pairs.
      \item Is case-sensitive when comparing URI strings.
    \end{itemize}

  \item \textbf{closeAllTrackedDiffEditors}
    \begin{itemize}
      \item Closes all diff tabs currently open in the workspace if they match tracked URIs.
      \item Skips irrelevant tabs or malformed inputs.
      \item Clears the internal tracked set after execution.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/utils/trackedDiffEditors.test.ts}{here}.

\subsubsection{Refactor Action Buttons}

\textbf{Goal:} These UI buttons provide users with the ability to accept or reject a proposed refactoring. They must be initialized correctly, displayed when a refactoring session is active, and hidden when the session ends. The buttons are tied to a VS Code context key used to control view behavior.

\medskip

\noindent The tests verify that the buttons are properly initialized and registered with the extension context, appear or disappear based on user interaction, and correctly set or reset the \texttt{refactoringInProgress} context variable.

\medskip

\noindent\textbf{Target requirement(s):} FR12~\cite{SRS}

\begin{itemize}
  \item \textbf{Button Visibility}
    \begin{itemize}
      \item The accept and reject buttons are shown when a refactoring session begins.
      \item The buttons are hidden when the session ends or is cancelled.
    \end{itemize}

  \item \textbf{Context Key Updates}
    \begin{itemize}
      \item \texttt{refactoringInProgress} is set to \texttt{true} when buttons are shown.
      \item \texttt{refactoringInProgress} is set to \texttt{false} when buttons are hidden.
    \end{itemize}

  \item \textbf{Subscription Registration}
    \begin{itemize}
      \item Buttons are registered to the extension context upon initialization.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found
\href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/main/test/utils/refactorActionButtons.test.ts}{here}.

  % \subsection{Unit Testing Scope}

  \wss{What modules are outside of the scope.  If there are modules that are
    developed by someone else, then you would say here if you aren't planning on
    verifying them.  There may also be modules that are part of your
    software, but
    have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

  % \subsection{Tests for Functional Requirements}

  % \wss{Most of the verification will be through automated unit testing.  If
  %   appropriate specific modules can be verified by a non-testing based
  %   technique.  That can also be documented in this section.}

  % \subsubsection{Module 1}

  % \wss{Include a blurb here to explain why the subsections below
  % cover the module.
  %   References to the MIS would be good.  You will want tests from a black box
  %   perspective and from a white box perspective.  Explain to the
  % reader how the
  %   tests were selected.}

  % \begin{enumerate}

  % \item \textbf{test-id1\\}

  % Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  %   be automatic}

  % \textbf{Initial State:}

  % \textbf{Input:}

  % \textbf{Output:} \wss{The expected result for the given inputs}

  % \textbf{Test Case Derivation:} \wss{Justify the expected value
  % given in the Output field}

  % \textbf{How test will be performed:}

  % \item \textbf{test-id2\\}

  % Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  %   be automatic}

  % \textbf{Initial State:}

  % \textbf{Input:}

  % \textbf{Output:} \wss{The expected result for the given inputs}

  % \textbf{Test Case Derivation:} \wss{Justify the expected value
  % given in the Output field}

  % \textbf{How test will be performed:}

  % \item \textbf{...\\}

  % \end{enumerate}

  % \subsubsection{Module 2}

  % ...

  % \subsection{Tests for Nonfunctional Requirements}

  % \wss{If there is a module that needs to be independently assessed for
  %   performance, those test cases can go here.  In some projects, planning for
  %   nonfunctional tests of units will not be that relevant.}

  % \wss{These tests may involve collecting performance data from previously
  %   mentioned functional tests.}

  % \subsubsection{Module 1}

  % \begin{enumerate}

  % \item \textbf{test-id1\\}

  % Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  %   be automatic}

  % \textbf{Initial State:}

  % Input/Condition:

  % Output/Result:

  % \textbf{How test will be performed:}

  % \item \textbf{test-id2\\}

  % Type: Functional, Dynamic, Manual, Static etc.

  % \textbf{Initial State:}

  % \textbf{Input:}

  % Output:

  % \textbf{How test will be performed:}

  % \end{enumerate}

  % \subsubsection{Module 2}

  % ...

  % \subsection{Traceability Between Test Cases and Modules}

  % \wss{Provide evidence that all of the modules have been considered.}

  \bibliography{../../refs/References}

  \newpage

  \begin{appendices}

    \section{Appendix}

    \wss{This is where you can place additional information.}

    \subsection{Symbolic Parameters}

    Not applicable at the moment.

    \subsection{Usability Survey Questions} \label{A.2}

    See the surveys folder under \href{https://github.com/ssm-lab/capstone--source-code-optimizer/tree/main/docs/Extras/UsabilityTesting/surveys}{\texttt{docs/Extras/UsabilityTesting}}.

    \newpage{}
    \section{Reflection}

    \wss{This section is not required for CAS 741}

    The information in this section will be used to evaluate the team
    members on the
    graduate attribute of Lifelong Learning.

    \input{../Reflection.tex}

    \begin{enumerate}
      \item What went well while writing this deliverable?
      \item What pain points did you experience during this deliverable, and how
        did you resolve them?
      \item What knowledge and skills will the team collectively need
        to acquire to
        successfully complete the verification and validation of your project?
        Examples of possible knowledge and skills include dynamic
        testing knowledge,
        static testing knowledge, specific tool usage, Valgrind etc.
        You should look to
        identify at least one item for each team member.
      \item For each of the knowledge areas and skills identified in
        the previous
        question, what are at least two approaches to acquiring the knowledge or
        mastering the skill?  Of the identified approaches, which will each team
        member pursue, and why did they make this choice?
    \end{enumerate}

    \subsubsection*{Mya Hussain}
    \begin{itemize}
      \item \textit{What went well while writing this deliverable?} \\

        Writing functional tests for the capstone project went surprisingly
        smoothly. I found that having a clear understanding of the project's
        requirements and functionalities made it easier to structure my tests
        logically. The existing documentation provided a solid foundation,
        allowing me to focus on creating relevant scenarios without needing
        extensive revisions.Overall, I felt a sense of accomplishment
        as I was able to
        write robust tests that will contribute to the project's
        success.

      \item \textit{What pain points did you experience during this
        deliverable, and how did you resolve them?}\\

        One challenge I faced was ensuring that each test was precise and
        effectively communicated its purpose. At times, I found myself
        overthinking the wording or structure, which slowed me down. To
        tackle this, I started breaking down each test into simple components,
        focusing on the core functionality rather than getting lost in the
        details. I also struggled with organizing the tests logically to
        create a seamless flow in the documentation. I resolved this by
        grouping tests thematically, which made it easier to follow. Despite
        the frustrations, I learned to embrace the process and appreciate the
        importance of thorough documentation in building a robust project.
        Balancing this deliverable and the POC was also challenging as there
        wasnt much turnaroud time between the two and we hadn't coded anything
        previously.

    \end{itemize}

    \subsubsection*{Sevhena Walker}
    \begin{itemize}
      \item \textit{What went well while writing this deliverable?} \\

        I was responsible for writing system tests for the projects
        non-functional requirements and I found the process to be
        very useful for gaining a deep understanding of all the
        qualities a system should have. When you write a requirement,
        obviously, there is some thought put into it, but actually
        writing out the test really sheds light on all the facets
        that go into that requirement. I feel like I have even more
        to contribute to my team after this deliverable.

      \item \textit{What pain points did you experience during this
        deliverable, and how did you resolve them?}\\

        Writing out all those tests was extremely long, and I found
        myself re-writing tests more than once while pondering on the
        best way to test the requirements. Some tests needed to be
        combined due to a near identical testing process and some
        needed more depth. I also sometimes struggled with
        determining if some testing could even be feasibly done with
        our team's resources. To resolve this I held a discussion or
        2 with my team so that we could have more brains working on
        the matter and to ensure that I wasn't making some important
        decisions unilaterally.
    \end{itemize}

    \subsubsection*{Nivetha Kuruparan}
    \begin{itemize}
      \item \textit{What went well while writing this deliverable?} \\

        Working on the Verification and Validation (VnV) plan for the
        Source Code Optimizer project was a pretty smooth experience
        overall. I really enjoyed defining the roles in section 3.1,
        which helped clarify what everyone was responsible for. This
        not only made the team feel more involved but also kept us on
        track with our testing strategies.

        Diving into the Design Verification Plan in section 3.3 was
        another highlight for me. It helped me get a better grasp of
        the requirements from the SRS. I felt more confident knowing
        we had a solid verification approach that covered all the
        bases, including functional and non-functional requirements.
        The discussions about incorporating static verification
        techniques and the importance of regular peer reviews were
        eye-opening and really enhanced our strategy for maintaining
        code quality.

      \item \textit{What pain points did you experience during this
        deliverable, and how did you resolve them?}\\

        I struggled a bit with figuring out how to effectively
        integrate feedback mechanisms into our VnV plan. It was tough
        to think through how to keep the feedback loop going
        throughout development. I tackled this by setting up a clear
        process for documenting feedback during our code reviews and
        testing phases, which I included in section 3.4. This not
        only improved our documentation but also helped us stay
        committed to continuously improving as we moved forward.

    \end{itemize}

    \subsubsection*{Ayushi Amin}
    \begin{itemize}
      \item \textit{What went well while writing this deliverable?} \\

        Writing this deliverable was a really crucial part of the process. It
        helped me see the bigger picture of how we’re going to ensure everything
        in the SRS gets tested properly. What went well was the
        clarity that came
        from laying out the plan step by step. Even though we haven’t
        put it into
        action yet, just knowing we have a solid structure in place
        gives me confidence.

        Another highlight was sharing our completed sections with Dr. Istvan. It
        was great to get his feedback and know that he appreciated the level of
        detail we included. Having that validation made me feel like
        we’re on the
        right track. It also reminded me how important it is to be thorough from
        the start, so we’re not scrambling later when we’re deep into testing.
        Having all the requiremnts and test cases mapped out helps me
        stress less
        as I know have an idea of what the proejct will look like and have these
        documents top guide the process in case we get stuck or
        forget something.

      \item \textit{What pain points did you experience during this
        deliverable, and how did you resolve them?}\\

        One of the challenges was trying to anticipate potential gaps or issues
        in our testing process while still being in the planning phase. Thinking
        through how to cover both functional and non-functional requirements in
        the SRS in a comprehensive yet practical way was tricky. We
        resolved this
        by deciding to create a traceability matrix, which will help
        us ensure that
        every requirement is accounted for once we move into the
        testing phase. Even
        though the matrix isn’t done yet, just planning to use it gives a sense
        of structure.

        Another tough spot was figuring out how to handle usability
        and performance
        testing in a way that doesn’t feel overly theoretical. Since
        we’re not at the
        implementation stage, it’s hard to gauge what users will
        really need. To work
        through this, I focused on drawing from what we know about
        our end-users and
        aligning our plan with the goals outlined in the SRS. Keeping
        that user-centered
        perspective helped ground the plan, making it feel more
        actionable even at this
        early stage.
    \end{itemize}

    \subsubsection*{Tanveer Brar}
    \begin{itemize}
      \item \textit{What went well while writing this deliverable?} \\

        Clearly pointing out the tools to use for various aspects of
        Automated Validation and Testing(such as unit test framework,
        linter) has created a well-defined plan for this
        verification. Now the project has a structured approach to
        validation. Knowing the tools before implementation will
        allow both code quality enforcement and the gathering of
        coverage metrics. For the Software Validation Plan, external
        data source(open source Python code bases for testing) has
        added confidence that the validation approach would align
        closely with real world scenarios.

      \item \textit{What pain points did you experience during this
        deliverable, and how did you resolve them?}\\

        One of the challenges was ensuring compatibility between
        different tools for automated testing and validation plan.
        For example, code coverage tool needs to be supported by the
        unit testing framework. To resolve this, I conducted research
        on all validation tools, to choose the ones that fit into the
        project's needs while being compatible with each other.

    \end{itemize}

    \subsubsection*{Group Reflection}
    \begin{itemize}
      \item \textit{What knowledge and skills will the team
          collectively need to acquire to
          successfully complete the verification and validation of your project?
          Examples of possible knowledge and skills include dynamic
          testing knowledge,
          static testing knowledge, specific tool usage, Valgrind
          etc.  You should look to
        identify at least one item for each team member.\\}

        Sevhena will need to deepen her understanding of test
        coordination and project
        tracking using GitHub Issues. She’ll focus on creating
        detailed issue templates
        for various testing stages, managing the workflow through
        Kanban boards, and using
        labels and milestones effectively to track progress.
        Additionally, mastering test
        case documentation and ensuring efficient communication
        through GitHub’s discussion
        and comment features will be critical.

        Mya will enhance her skills in functional testing by learning
        to write comprehensive
        test cases directly linked to GitHub Issues. She will
        leverage GitHub Actions to
        automate repetitive functional tests and integrate them into
        the development workflow.
        Familiarity with continuous integration pipelines and how
        they relate to functional
        testing will help her verify that all functional requirements
        are met consistently.

        Ayushi will focus on integration testing by ensuring that the
        Python package, VSCode
        plugin, and GitHub Action work together seamlessly. She’ll
        develop expertise in using
        PyJoules to assess energy efficiency during integration tests
        and learn to create
        automated workflows via GitHub Actions. Ensuring smooth
        integration of PyTorch models
        and maintaining consistent coding standards with Pylint will
        be essential. She’ll
        also manage dependencies and coordinate with the team using
        GitHub’s multi-repository
        capabilities.

        Tanveer will deepen her knowledge of performance testing
        using PyJoules to monitor
        and optimize energy consumption. She will also need to
        develop skills in security
        testing, ensuring that the Python code adheres to best
        security practices, possibly
        integrating tools like Bandit along with Pylint for static
        code analysis. Setting
        up and maintaining performance benchmarks using GitHub Issues
        will ensure transparency
        and continuous improvement.

        Nivetha will enhance her skills in usability and user
        experience testing, particularly
        in evaluating the intuitiveness of the VSCode plugin
        interface. She will focus on
        collecting and analyzing user feedback, linking it to GitHub
        Issues to drive interface
        improvements. Documenting user experience testing and
        ensuring that the product’s UI
        meets user expectations will be a significant part of her
        role. Using Pylint to maintain
        consistent code quality in user-facing components will also
        be essential.

        Istvan will provide oversight by monitoring the team’s
        progress, using GitHub Insights
        to ensure that testing processes meet industry standards. He
        will guide the team in
        integrating PyJoules, Pylint, and PyTorch effectively into
        the V\&V workflow, offering
        feedback and ensuring alignment with project goals.

        All group members will have to learn how to use pytests to
        perform test cases in this
        entire project.

      \item \textit{For each of the knowledge areas and skills
          identified in the previous
          question, what are at least two approaches to acquiring the
          knowledge or
          mastering the skill?  Of the identified approaches, which
          will each team
        member pursue, and why did they make this choice?\\}


        \textbf{Sevhena Walker (Lead Tester)}
        \begin{itemize}
          \item \textbf{Knowledge Areas:} Test coordination,
            PyJoules, GitHub Actions, Pylint.
          \item \textbf{Approaches:}
            \begin{itemize}
              \item Online Courses and Tutorials: Enroll in courses
                focused on test automation, PyJoules, and GitHub Actions.
              \item Hands-on Practice: Apply knowledge directly by
                setting up test cases and automation workflows in the project.
            \end{itemize}
          \item \textbf{Preferred Approach:} Hands-on Practice
          \item \textbf{Reason:} This approach allows her to see
            immediate results and iterate quickly, building
            confidence in her coordination and automation skills.
        \end{itemize}

        \textbf{Mya Hussain (Functional Requirements Tester)}
        \begin{itemize}
          \item \textbf{Knowledge Areas:} PyTorch, functional
            testing, GitHub Actions, Pylint.
          \item \textbf{Approaches:}
            \begin{itemize}
              \item Technical Documentation and Community Forums:
                Study PyTorch documentation and participate in forums
                like Stack Overflow.
              \item Mentorship and Collaboration: Pair with
                experienced team members or mentors to get guidance
                and feedback on functional testing practices.
            \end{itemize}
          \item \textbf{Preferred Approach:} Technical Documentation
            and Community Forums
          \item \textbf{Reason:} It allows her to explore topics
            deeply and find solutions to specific issues, promoting
            self-sufficiency.
        \end{itemize}

        \textbf{Ayushi Amin (Integration Tester)}
        \begin{itemize}
          \item \textbf{Knowledge Areas:} PyJoules, integration
            testing, PyTorch, GitHub Actions.
          \item \textbf{Approaches:}
            \begin{itemize}
              \item Workshops and Webinars: Attend live or recorded
                sessions focused on energy-efficient software
                development and integration testing techniques.
              \item Project-Based Learning: Directly work on
                integrating components and iteratively improving
                based on project needs.
            \end{itemize}
          \item \textbf{Preferred Approach:} Project-Based Learning
          \item \textbf{Reason:} It aligns with her role's focus on
            real-world integration, providing relevant experience and
            immediate feedback.
        \end{itemize}

        \textbf{Tanveer Brar (Non-Functional Requirements Tester -
        Performance/Security)}
        \begin{itemize}
          \item \textbf{Knowledge Areas:} Performance testing with
            PyJoules, security testing, Pylint.
          \item \textbf{Approaches:}
            \begin{itemize}
              \item Specialized Training Programs: Join programs or
                bootcamps that focus on performance and security testing.
              \item Peer Learning: Collaborate with team members and
                participate in knowledge-sharing sessions.
            \end{itemize}
          \item \textbf{Preferred Approach:} Peer Learning
          \item \textbf{Reason:} It promotes team synergy and allows
            him to gain practical insights from those working on similar tasks.
        \end{itemize}

        \textbf{Nivetha Kuruparan (Non-Functional Requirements Tester
        - Usability/UI)}
        \begin{itemize}
          \item \textbf{Knowledge Areas:} Usability testing, user
            experience, GitHub Issues, Pylint.
          \item \textbf{Approaches:}
            \begin{itemize}
              \item User Feedback Analysis: Conduct regular user
                testing sessions and analyze feedback.
              \item Online UX/UI Design Courses: Enroll in courses
                that focus on usability principles and user experience design.
            \end{itemize}
          \item \textbf{Preferred Approach:} User Feedback Analysis
          \item \textbf{Reason:} This approach provides real-world
            insights into how the product is perceived and used,
            making adjustments more relevant.
        \end{itemize}

        \textbf{Istvan David (Supervisor)}
        \begin{itemize}
          \item \textbf{Knowledge Areas:} Supervising V\&V processes,
            providing feedback, ensuring industry standards.
          \item \textbf{Approaches:}
            \begin{itemize}
              \item Industry Conferences and Seminars: Attend events
                focused on software verification and validation trends.
              \item Continuous Professional Development: Engage in
                regular self-study and professional development activities.
            \end{itemize}
          \item \textbf{Preferred Approach:} Continuous Professional Development
          \item \textbf{Reason:} This method allows for a consistent
            update of skills and knowledge aligned with evolving
            industry standards.
        \end{itemize}

    \end{itemize}

  \end{appendices}

  \end{document}
