\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{paralist}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}

% Test

%Includes "References" in the table of contents
\usepackage[nottoc]{tocbibind}
\usepackage[toc,page]{appendix}
\usepackage[square,numbers,compress]{natbib}
\usepackage{placeins}
\bibliographystyle{abbrvnat}

\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[letterpaper, portrait, margin=1in]{geometry}
\usepackage[dvipsnames]{xcolor}

\usepackage{float}

\input{../Comments}
\input{../Common}

\newcommand{\SRS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/SRS/SRS.pdf}{SRS}}
\newcommand{\MG}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG}}
\newcommand{\MIS}{\href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}}

\newcommand{\colorrule}{\textcolor{BlueViolet}{\rule{\linewidth}{2pt}}}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{4cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
November 4th, 2024 & 0.0 & Created initial revision of VnV Plan\\
March 10th, 2025 & 0.1 & Revised Functional and Non-Functional Requirements\\

\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

% \listoffigures
% \wss{Remove this section if it isn't needed}

\newpage

% \section{Symbols, Abbreviations, and Acronyms}

% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{l l} 
%   \toprule		
%   \textbf{symbol} & \textbf{description}\\
%   \midrule 
%   T & Test\\
%   \bottomrule
% \end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%  ~\cite{SRS} tables, if appropriate}

% \wss{Remove this section if it isn't needed}

% \newpage

\pagenumbering{arabic}

This document outlines the process and methods to ensure that the software meets its requirements and functions as intended. This document provides a structured approach to evaluating the product, incorporating both verification (to confirm that the software is built correctly) and validation (to confirm that the correct software has been built). By systematically identifying and mitigating potential issues, the V\&V process aims to enhance quality, reduce risks, and ensure compliance with both functional and non-functional requirements.\\

The following sections will go over the approach for verification and validation, including the team structure, verification strategies at various stages and tools to be employed. Furthermore, a detailed list of system and unit tests are also included in this document.

\section{General Information}

\subsection{Summary}

The software being tested is called EcoOptimizer. EcoOptimizer is a python refactoring library that focuses on optimizing code in a way that reduces its energy consumption. The system will be capable to analyze python code in order to spot inefficiencies (code smells) within, measuring the energy efficiency of the inputted code and, of course, apply appropriate refactorings that preserve the initial function of the source code. \\

Furthermore, peripheral tools such as a Visual Studio Code (VS Code) extension and GitHub Action are also to be tested. The extension will integrate the library with Visual Studio Code for a more efficient development process and the GitHub Action will allow a proper integration of the library into continuous integration (CI) workflows.

\subsection{Objectives}

The primary objective of this project is to build confidence in the \textbf{correctness} and \textbf{energy efficiency} of the refactoring library, ensuring that it performs as expected in improving code efficiency while maintaining functionality. Usability is also emphasized, particularly in the user interfaces provided through the \textbf{VS Code extension} and \textbf{GitHub Action} integrations, as ease of use is critical for adoption by software developers. These qualities—correctness, energy efficiency, and usability—are central to the project’s success, as they directly impact user experience, performance, and the sustainable benefits of the tool.\\

Certain objectives are intentionally left out-of-scope due to resource constraints. We will not independently verify external libraries or dependencies; instead, we assume they have been validated by their respective development teams. 

\subsection{Challenge Level and Extras}

Our project, set at a \textbf{general} challenge level, includes two additional focuses: \textbf{user documentation} and \textbf{usability testing}. The user documentation aims to provide clear, accessible guidance for developers, making it easy to understand the tool’s setup, functionality, and integration into existing workflows. Usability testing will ensure that the tool is intuitive and meets user needs effectively, offering insights to refine the user interface and optimize interactions with its features.

\subsection{Relevant Documentation}

The Verification and Validation (VnV) plan relies on three key documents to guide testing and assessment: 
\begin{itemize}
  \item[] \textbf{Software Requirements Specification (\SRS)~\cite{SRS}:} The foundation for the VnV plan, as it defines the functional and non-functional requirements the software must meet; aligning tests with these requirements ensures that the software performs as expected in terms of correctness, performance, and usability.
  
  \item[] \textbf{Module Interface Specification (\MG)~\cite{MGDoc}:} Provides detailed information about each module's interfaces, which is crucial for integration testing to verify that all modules interact correctly within the system.
  
  \item[] \textbf{Module Guide (\MIS)~\cite{MISDoc}:} Outlines the system's architectural design and module structure, ensuring the design of tests that align with the intended flow and dependencies within the system.
\end{itemize}

\section{Plan}

The following section outlines the comprehensive Verification and Validation (VnV) strategy, detailing the team structure, specific plans for verifying the Software Requirements Specification (SRS), design, implementation, and overall VnV process, as well as the automated tools employed and the approach to software validation.

\subsection{Verification and Validation Team}

The Verification and Validation (VnV) Team for the Source Code Optimizer project consists of the following members and their specific roles:

\begin{itemize}
    \item \textbf{Sevhena Walker}: Lead Tester. Oversees and coordinates the testing process, ensuring all feedback is applied and all project goals are met.
    \item \textbf{Mya Hussain}: Functional Requirements Tester. Tests the software to verify that it meets all specified functional requirements.
    \item \textbf{Ayushi Amin}: Integration Tester. Focuses on testing the connection between the various components of the Python package, the VSCode plugin, and the GitHub Action to ensure seamless integration.
    \item \textbf{Tanveer Brar}: Non-Functional Requirements Tester. Assesses performance/security compliance with project standards.
    \item \textbf{Nivetha Kuruparan}: Non-Functional Requirements Tester. Ensures that the final product meets user expectations regarding user experience and interface intuitiveness.
    \item \textbf{Istvan David} (supervisor): Supervises the overall VnV process, providing feedback and guidance based on industry standards and practices.
\end{itemize}

\subsection{SRS Verification Plan}

\textbf{Function \& Non-Functional Requirements:}
\begin{itemize}
    \item A comprehensive test suite that covers all requirements specified in the SRS will be created.
    \item Each requirement will be mapped to specific test cases to ensure maximum coverage.
    \item Automated and manual testing will be conducted to verify that the implemented system meets each functional requirement.
    \item Usability testing with representative users will be carried out to validate user experience requirements and other non-functional requirements.
    \item Performance tests will be conducted to verify that the system meets specified performance requirements.
\end{itemize}

\textbf{Traceability Matrix:}
\begin{itemize}
    \item We will create a requirements traceability matrix that links each SRS requirement to its corresponding implementation, test cases, and test results.
    \item This matrix will help identify any requirements that may have been overlooked during development.
\end{itemize}

\textbf{Supervisor Review:}
\begin{itemize}
    \item After the implementation of the system, we will conduct a formal review session with key stakeholders such as our project supervisor, Dr. Istvan David.
    \item The stakeholders will be asked to verify that each requirement in the SRS is mapped out to specific expectations of the project. 
    \item Prior to meeting, we will provide a summary of key requirements and design decisions and prepare a list specific questions or areas where we seek guidance.
    \item During the meeting, we will present an overview of the SRS using tables and other visual aids. We will conduct a walk through of critical section. Finally, we will discuss any potential risks or challenges identified.
\end{itemize}

\textbf{User Acceptance Testing (UAT):}
\begin{itemize}
    \item We will involve potential end-users in testing the system to ensure it meets real-world usage scenarios.
    \item Feedback from UAT will be used to identify any discrepancies between the SRS and user expectations.
\end{itemize}

\textbf{Continuous Verification:}
\begin{itemize}
    \item Throughout the development process, we will regularly review and update the SRS to ensure it remains aligned with the evolving system.
    \item Any changes to requirements will be documented and their impact on the system assessed.
\end{itemize}

\textbf{\textit{\\Checklist for SRS Verification Plan}}
\begin{itemize}
    \item[$\square$] Create comprehensive test suite covering all SRS requirements
    \item[$\square$] Map each requirement to specific test cases
    \item[$\square$] Conduct automated testing for functional requirements
    \item[$\square$] Perform manual testing for functional requirements
    \item[$\square$] Carry out usability testing with representative users
    \item[$\square$] Conduct performance tests to verify system meets requirements
    \item[$\square$] Create requirements traceability matrix
    \item[$\square$] Link each SRS requirement to implementation in traceability matrix
    \item[$\square$] Link each SRS requirement to test cases in traceability matrix
    \item[$\square$] Link each SRS requirement to test results in traceability matrix
    \item[$\square$] Schedule formal review session with project supervisor
    \item[$\square$] Prepare summary of key requirements and design decisions for supervisor review
    \item[$\square$] Prepare list of specific questions for supervisor review
    \item[$\square$] Create visual aids for SRS overview presentation
    \item[$\square$] Conduct walkthrough of critical SRS sections during review
    \item[$\square$] Discuss potential risks and challenges with supervisor
    \item[$\square$] Organize User Acceptance Testing (UAT) with potential end-users
    \item[$\square$] Collect and analyze UAT feedback
    \item[$\square$] Identify discrepancies between SRS and user expectations from UAT
    \item[$\square$] Establish process for regular SRS review and updates
    \item[$\square$] Document any changes to requirements
    \item[$\square$] Assess impact of requirement changes on the system
\end{itemize}

\subsection{Design Verification Plan}

\textbf{Peer Review Plan:}
\begin{itemize}
    \item Each team member along with other classmates will thoroughly review the entire Design Document.
    \item A checklist-based approach will be used to ensure all key elements are covered.
    \item Feedback will be collected and discussed in a dedicated team meeting.
\end{itemize}

\textbf{Supervisor Review:}
\begin{itemize}
    \item A structured review meeting will be scheduled with our project supervisor, Dr. Istvan David.
    \item We will present an overview of the design using visual aids (e.g., diagrams, tables).
    \item We will conduct a walkthrough of critical sections.
    \item We will use our project's issue tracker to document and follow up on any action items or changes resulting from this review.
\end{itemize}

\begin{itemize}
  \item[$\square$] All functional requirements are mapped to specific design elements 
  \item[$\square$] Each functional requirement is fully addressed by the design 
  \item[$\square$] No functional requirements are overlooked or partially implemented 
  \item[$\square$] Performance requirements are met by the design 
  \item[$\square$] Scalability considerations are incorporated 
  \item[$\square$] Reliability and availability requirements are satisfied 
  \item[$\square$] Usability requirements are reflected in the user interface design
  \item[$\square$] High-level architecture is clearly defined 
  \item[$\square$] Architectural decisions are justified with rationale 
  \item[$\square$] Architecture aligns with project constraints and goals 
  \item[$\square$] All major components are identified and described 
  \item[$\square$] Interactions between components are clearly specified 
  \item[$\square$] Component responsibilities are well-defined 
  \item[$\square$] Appropriate data structures are chosen for each task 
  \item[$\square$] Efficient algorithms are selected for critical operations 
  \item[$\square$] Rationale for data structure and algorithm choices is provided
  \item[$\square$] UI design is consistent with usability requirements 
  \item[$\square$] User flow is logical and efficient 
  \item[$\square$] Accessibility considerations are incorporated 
  \item[$\square$] All external interfaces are properly specified 
  \item[$\square$] Interface protocols and data formats are defined 
  \item[$\square$] Error handling for external interfaces is addressed 
  \item[$\square$] Comprehensive error handling strategy is in place
  \item[$\square$] Exception scenarios are identified and managed 
  \item[$\square$] Error messages are clear and actionable 
  \item[$\square$] Authentication and authorization mechanisms are described 
  \item[$\square$] Data encryption methods are specified where necessary 
  \item[$\square$] Security best practices are followed in the design
  \item[$\square$] Design allows for future expansion and feature additions 
  \item[$\square$] Code modularity and reusability are considered 
  \item[$\square$] Documentation standards are established for maintainability 
  \item[$\square$] Performance bottlenecks are identified and addressed 
  \item[$\square$] Resource utilization is optimized 
  \item[$\square$] Performance testing strategies are outlined 
  \item[$\square$] Design adheres to established coding standards 
  \item[$\square$] Industry best practices are followed 
  \item[$\square$] Design patterns are appropriately applied
  \item[$\square$] All major design decisions are justified 
  \item[$\square$] Trade-offs are explained with pros and cons 
  \item[$\square$] Alternative approaches considered are documented 
  \item[$\square$] Documents is clear, concise, and free of ambiguities 
  \item[$\square$] Documents follows a logical structure 
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation (V\&V) Plan for the Source Code Optimizer project serves as a critical document that requires a thorough examination to confirm its validity and effectiveness. To achieve this, the following strategies will be implemented:

\begin{enumerate}
    \item \textbf{Peer Review}: Team members and peers will conduct a detailed review of the V\&V plan. This process aims to uncover any gaps or areas that could benefit from enhancement, leveraging the collective insights of the group to strengthen the overall plan.
    
    \item \textbf{Fault Injection Testing}: We will utilize mutation testing to assess the capability of our test cases to identify intentionally introduced faults. By generating variations of the original code, we can evaluate whether our testing strategies are robust enough to catch these discrepancies, hence enhancing the reliability of our verification process.
    
    \item \textbf{Feedback Loop Integration}: Continuous feedback from review sessions and testing activities will be systematically integrated to refine the V\&V plan. This ongoing process ensures the plan evolves based on insights gained from practical testing and peer input.
\end{enumerate}


\noindent To comprehensively verify the V\&V plan, we will utilize the following checklist:

\begin{itemize}
    \item[$\square$] Does the V\&V plan include all necessary aspects of software verification and validation?
    \item[$\square$] Are the roles and responsibilities clearly outlined within the V\&V framework?
    \item[$\square$] Is there a diversity of testing methodologies included (e.g., unit testing, integration testing, system testing)?
    \item[$\square$] Does the plan have a clear process for incorporating feedback and gaining continuous improvement?
    \item[$\square$] Are success criteria established for each phase of testing?
    \item[$\square$] Is mutation testing considered to evaluate the effectiveness of the test cases?
    \item[$\square$] Are mechanisms in place to monitor and address any identified issues during the V\&V process?
    \item[$\square$] Does the V\&V plan align with the project timeline, available resources, and other constraints?
\end{itemize}

\subsection{Implementation Verification Plan}

The Implementation Verification Plan for the Source Code Optimizer project aims to ensure that the software implementation adheres to the requirements and design specifications defined in the SRS. Key components of this plan include:

\begin{itemize}
    \item \textbf{Unit Testing}: A comprehensive suite of unit tests will be established to validate the functionality of individual components within the optimizer. These tests will specifically focus on the effectiveness of the code refactoring methods employed by the optimizer, utilizing \texttt{pytest}~\cite{pytest} for writing and executing these tests.
    
    \item \textbf{Static Code Analysis}: To maintain high code quality, static analysis tools will be employed. These tools will help identify potential bugs, security vulnerabilities, and adherence to coding standards in the Python codebase, ensuring that the optimizer is both efficient and secure.
    
    \item \textbf{Code Walkthroughs and Reviews}: The development team will hold regular code reviews and walkthrough sessions to collaboratively evaluate the implementation of the source code optimizer. These sessions will focus on code quality, readability, and compliance with the project’s design patterns. Additionally, the final presentation will provide an opportunity for a thorough code walkthrough, allowing peers to contribute feedback on usability and functionality.
    
    \item \textbf{Continuous Integration}: The project will implement continuous integration practices using tools like GitHub Actions. This approach will automate the build and testing processes, allowing the team to verify that each change to the optimizer codebase meets the established quality criteria and integrates smoothly with the overall system.
    
    \item \textbf{Performance Testing}: The performance of the source code optimizer will be assessed to simulate various usage scenarios. This testing will focus on evaluating how effectively the optimizer processes large codebases and applies refactorings, ensuring that the tool operates efficiently under different workloads.
\end{itemize}

\subsection{Automated Testing and Verification Tools}

\textbf{Unit Testing Framework:} Pytest is chosen as the main framework for unit testing due to its \begin{inparaenum}[(i)]
  \item scalability
  \item integration with other tools (\texttt{pytest-cov}~\cite{pytest-cov} for code coverage)
  \item extensive support for parameterized tests.
  \end{inparaenum} These features make it easy to test the codebase as it grows, adapting to changes throughout the project's development.\\
  
\noindent\textbf{Profiling Tool:} The codebase will be evaluated based on results from both time and memory profiling to optimize computational speed and resource usage. For time profiling (recording the number of function calls, time spent in each function, and its descendants), \texttt{cProfile} will be used, as it is included within Python, making it a convenient choice for profiling. For memory profiling, \texttt{memory\_profiler}~\cite{memory_profiler} will be used, as it is easy to install and includes built-in support for visual display of output.\\
  
\noindent\textbf{Static Analyzer:} The codebase will be statically analyzed using \texttt{ruff}~\cite{ruff}, as it provides fast linting, built-in rule enforcement, and integrates well with modern Python projects. \texttt{ruff}~\cite{ruff} enforces both linting and formatting rules, reducing the need for multiple tools.\\
  
\noindent\textbf{Code Coverage Tools and Plan for Summary:} The codebase will be analyzed to determine the percentage of code executed during tests. For granular-level coverage, \texttt{pytest-cov}~\cite{pytest-cov} will be used, as it supports branch, line, and path coverage. Additionally, \texttt{pytest-cov}~\cite{pytest-cov} integrates seamlessly with \texttt{pytest}~\cite{pytest}, ensuring that test coverage results are generated alongside test execution.\\

Initially, the aim is to achieve a 40\% coverage and gradually increment the level over time. Weekly reports generated from \texttt{pytest-cov}~\cite{pytest-cov} will be used to track coverage trends and set goals accordingly to address any gaps in testing in the growing codebase.\\
  
\noindent\textbf{Linters and Formatters:} To enforce the official Python PEP 8 style guide and maintain code quality, the team will use \texttt{ruff}~\cite{ruff} for Python code and \texttt{eslint}~\cite{eslint} paired with \texttt{Prettier}~\cite{prettier} for the TypeScript extension.\\
  
\noindent\textbf{Testing Strategy for the VSCode Extension:} The TypeScript extension will be tested using \texttt{jest}~\cite{jest}. Automated tests will verify interactions between the extension and the editor, reducing regressions during development.\\
  
\noindent\textbf{CI Plan:} As mentioned in the Development Plan, GitHub Actions will integrate the above tools within the CI pipeline. GitHub Actions will be configured to run unit tests written in \texttt{pytest}, perform static analysis using \texttt{ruff}~\cite{ruff}, and execute \texttt{pytest-cov}~\cite{pytest-cov} for test coverage. For the TypeScript extension, a pre-commit will run \texttt{eslint}~\cite{eslint} and \texttt{Prettier}~\cite{prettier}, and automated tests will be executed as a GitHub Action using \texttt{jest}~\cite{jest}. Through automated testing, any errors, code style violations, and regressions will be promptly identified.\\

\subsection{Software Validation Plan}

\begin{itemize}
    \item One or more open source Python code bases will be used to test the tool on. Based on its performance in functional and non-functional tests outlined in further sections of the document, the software can be validated against defined requirements.
    \item In addition to this, the team will reach out to Dr David as well as a group of volunteer Python developers to perform usability testing on the IDE plugin workflow as well as the CI/CD workflow.
    \item The team will conduct a comprehensive review of the requirements from Dr David through the Rev 0 Demo.
\end{itemize}

\section{System Tests}

This section outlines the tests for verifying both functional and nonfunctional requirements of the software, ensuring it meets user expectations and performs reliably. This includes tests for code quality, usability, performance, security, and traceability, covering essential aspects of the software’s operation and compliance.

\subsection{Tests for Functional Requirements}

The subsections below outline tests corresponding to functional 
requirements in the \SRS~\cite{SRS}. Each test is associated with a unique functional area, helping to confirm that the tool meets the specified requirements. Each functional area has its own subsection for clarity.

\noindent
\colorrule

\subsubsection{Code Input Acceptance Tests}
\colorrule

\medskip

\noindent
This section covers the tests for ensuring the system correctly accepts Python source code files, detects errors in invalid files, and provides suitable feedback (FR 1).

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-IA-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Valid Python File Acceptance} \\[2mm]
    \textbf{Control:} Manual \\    
    \textbf{Initial State:} Tool is idle.  \\
    \textbf{Input:} A valid Python file (filename.py) with valid standard syntax. \\
    \textbf{Output:} The system accepts the file without errors.\\[2mm]
    \textbf{Test Case Derivation:} Confirming that the system correctly processes a valid Python file as per FR 1.\\[2mm]
    \textbf{How test will be performed:} Feed a syntactically valid .py file to the tool and observe if it’s accepted without issues.
            
  \item \textbf{Feedback for Python File with Bad Syntax} \\[2mm]
    \textbf{Control:} Manual \\
    \textbf{Initial State:} Tool is idle. \\
    \textbf{Input:} A .py file (badSyntax.py) containing deliberate syntax errors that render the file unrunnable. \\
    \textbf{Output:} The system rejects the file and provides an error message detailing the syntax issue. \\[2mm]
    \textbf{Test Case Derivation:} Verifies the tool’s handling of syntactically invalid Python files to ensure user awareness of the syntax issue, meeting FR 1. \\[2mm]
    \textbf{How test will be performed:} Feed a .py file with syntax errors to the tool and check that the system identifies it as invalid and produces an appropriate error message.

  \item \textbf{Feedback for Non-Python File}\\[2mm]
    \textbf{Control:} Manual \\
    \textbf{Initial State:} Tool is idle.\\
    \textbf{Input:} A non-Python file (document.txt) or a file with an incorrect extension (script.js).\\
    \textbf{Output:} The system rejects the file and provides an error message indicating the invalid file format.\\[2mm]
    \textbf{Test Case Derivation:} Ensures the tool detects unsupported file types and provides feedback, satisfying FR 1.\\[2mm]
    \textbf{How test will be performed:} Attempt to load a .txt or other non-Python file, and verify that the system rejects it with a message indicating an invalid file type.

\noindent
\colorrule

\subsubsection{Code Smell Detection Tests and Refactoring Suggestion (RS) Tests} \label{4.1.2}
\colorrule

\medskip

\noindent
This area includes tests to verify the detection and refactoring of specified code 
smells that impact energy efficiency (FR 2). These tests will be done through unit testing.

\end{enumerate}

\noindent
\colorrule

\subsubsection{Output Validation Tests}
\colorrule

\medskip

\noindent
The following tests are designed to validate that the functionality of the original Python code remains intact after refactoring. Each test ensures that the refactored code passes the same test suite as the original code, confirming compliance with functional requirement FR 3.
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-OV-\arabic*}}, wide=0pt, font=\itshape]  
  \label{itm:FR-OV-1}
  \item \textbf{Verification of Valid Python Output}\\[2mm]
    \textbf{Control:} Manual \\
    \textbf{Initial State:} Tool has processed a file with detected code smells.\\
    \textbf{Input:} Output refactored Python code.\\
    \textbf{Output:} Refactored code is syntactically correct and Python-compliant.\\[2mm]
    \textbf{Test Case Derivation:} Ensures refactored code remains valid and usable, satisfying FR 6.\\[2mm]
    \textbf{How test will be performed:} Run a linter on the output code and verify it passes without syntax errors.
  
\end{enumerate}

\newpage

\noindent
\colorrule

\subsubsection{Tests for Reporting Functionality}
\colorrule

\medskip

\noindent
The reporting functionality of the tool is crucial for providing users with comprehensive insights into the refactoring process, including detected code smells, refactorings applied, energy consumption measurements, and the results of the original test suite. This section outlines tests that ensure the reporting feature operates correctly and delivers accurate, well-structured information as specified in the functional requirements (FR 9). 
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-RP-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{A Report With All Components Is Generated}\\[2mm]
    \textbf{Control:} Manual
    \textbf{Initial State:} The tool has completed refactoring a Python code file.\\
    \textbf{Input:} The refactoring results, including detected code smells, applied refactorings, and energy consumption metrics.\\
    \textbf{Output:} A well-structured report is generated, summarizing the refactoring process.\\[2mm]
    \textbf{Test Case Derivation:} This test ensures that the tool generates a comprehensive report that includes all necessary information as required by FR 9.\\[2mm]
    \textbf{How test will be performed:} After refactoring, the tool will invoke the report generation feature and a user can validate that the output meets the structure and content specifications.

  \item \textbf{Validation of Code Smell and Refactoring Data in Report}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} The tool has identified code smells and performed refactorings.\\
    \textbf{Input:} The results of the refactoring process.\\
    \textbf{Output:} The generated report accurately lists all detected code smells and the corresponding refactorings applied.\\[2mm]
    \textbf{Test Case Derivation:} This test verifies that the report includes correct and complete information about code smells and refactorings, in compliance with FR 9.\\[2mm]
    \textbf{How test will be performed:} The tool will compare the contents of the generated report against the detected code smells and refactorings to ensure accuracy.

  \item \textbf{Energy Consumption Metrics Included in Report}\\[2mm]
    \textbf{Control:} Manual
    \textbf{Initial State:} The tool has measured energy consumption before and after refactoring.\\
    \textbf{Input:} Energy consumption metrics obtained during the refactoring process.\\
    \textbf{Output:} The report presents a clear comparison of energy usage before and after the refactorings.\\[2mm]
    \textbf{Test Case Derivation:} This test confirms that the reporting feature effectively communicates energy consumption improvements, aligning with FR 9.\\[2mm]
    \textbf{How test will be performed:} A user will analyze the energy metrics in the report to ensure they accurately reflect the measurements taken during the refactoring.

  \item \textbf{Functionality Test Results Included in Report}\\[2mm]
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} The original test suite has been executed against the refactored code.\\
    \textbf{Input:} The outcomes of the test suite execution.\\
    \textbf{Output:} The report summarizes the test results, indicating which tests passed and failed.\\[2mm]
    \textbf{Test Case Derivation:} This test ensures that the reporting functionality accurately reflects the results of the test suite as specified in FR 9.\\[2mm]
    \textbf{How test will be performed:} The tool will generate the report and validate that it contains a summary of test results consistent with the actual test outcomes.
\end{enumerate}

\noindent
\colorrule

\subsubsection{Documentation Availability Tests}
\colorrule

\medskip

\noindent
The following test is designed to ensure the availability of documentation as per FR 10.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-DA-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Test for Documentation Availability}\\[2mm]
    \textbf{Control:} Manual\\
    \textbf{Initial State:} The system may or may not be installed.\\
    \textbf{Input:} User attempts to access the documentation.\\
    \textbf{Output:} The documentation is available and covers installation, usage, and troubleshooting.\\[2mm]
    \textbf{Test Case Derivation:} Validates that the documentation meets user needs (FR 10).\\[2mm]
    \textbf{How test will be performed:} Review the documentation for completeness and clarity.
\end{enumerate}

\noindent
\colorrule

\subsubsection{IDE Extension Tests}
\colorrule

\medskip

\noindent
The following tests are designed to ensure that the user can integrate the tool into VS Code IDE as specified in FR 11 and that the tool works as intended as an extension.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-FR-IE-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Installation of Extension in Visual Studio Code}\\[2mm]
    \textbf{Control:} Manual\\
    \textbf{Initial State:} The user has Visual Studio Code installed on their machine.\\
    \textbf{Input:} The user attempts to install the refactoring tool extension from the Visual Studio Code Marketplace.\\
    \textbf{Output:} The extension installs successfully, and the user is able to see it listed in the Extensions view.\\[2mm]
    \textbf{Test Case Derivation:} This test validates the installation process of the extension to ensure that users can easily add the tool to their development environment.\\[2mm]
    \textbf{How test will be performed:} 
    \begin{enumerate}[label=\arabic*.]
        \item Open Visual Studio Code.
        \item Navigate to the Extensions view (Ctrl+Shift+X).
        \item Search for the refactoring tool extension in the marketplace.
        \item Click on the "Install" button.
        \item After installation, verify that the extension appears in the installed extensions list.
        \item Confirm that the extension is enabled and ready for use by checking its functionality within the editor.
    \end{enumerate}

  \item \textbf{Running the Extension in Visual Studio Code}\\[2mm]
    \textbf{Control:} Manual\\
    \textbf{Initial State:} The user has successfully installed the refactoring tool extension in Visual Studio Code.\\
    \textbf{Input:} The user opens a Python file and activates the refactoring tool extension.\\
    \textbf{Output:} The extension runs successfully, and the user can see a list of detected code smells and suggested refactorings.\\[2mm]
    \textbf{Test Case Derivation:} This test validates that the extension can be executed within the development environment and that it correctly identifies code smells as per the functional requirements in the SRS.\\[2mm]
    \textbf{How test will be performed:}
    \begin{enumerate}[label=\arabic*.]
        \item Open Visual Studio Code.
        \item Open a valid Python file that contains known code smells.
        \item Activate the refactoring tool extension using the command palette (Ctrl+Shift+P) and selecting the extension command.
        \item Observe the output panel for the detection of code smells.
        \item Verify that the extension lists the identified code smells and provides appropriate refactoring suggestions.
        \item Confirm that the suggestions are relevant and feasible for the detected code smells.
    \end{enumerate}
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

The section will cover system tests for the non-functional requirements (NFR) listed in the \SRS \hspace{1pt} document\cite{SRS}. The goal for these tests is to address the fit criteria for the requirements. Each test will be linked back to a specific NFR that can be observed in section \ref{trace-sys}.

\noindent
\colorrule

\subsubsection{Look and Feel}

\colorrule

\medskip

\noindent
The following subsection tests cover all Look and Feel requirements listed in the SRS~\cite{SRS}. They seek to validate that the system is modern, visually appealing, and supporting of a calm and focused user experience. 
		
\begin{enumerate}[label={\bf \textcolor{Maroon}{test-LF-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Side-by-side code comparison in IDE plugin} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open in VS Code, with a sample code file loaded \\
    \textbf{Input/Condition:} The user initiates a refactoring operation \\
    \textbf{Output/Result:} The plugin displays the original and refactored code side by side\\[2mm]
    \textbf{How test will be performed:} The tester will open a sample code file within the IDE plugin and apply a refactoring operation. After refactoring, they will verify that the original code appears on one side of the interface and the refactored code on the other, with clear options to accept or reject each change. The tester will interact with the accept/reject buttons to ensure functionality and usability, confirming that users can seamlessly make refactoring decisions with both versions displayed side by side.

  \item \textbf{Theme adaptation in VS Code} \\[2mm]
    \textbf{Type:} Non-functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open in VS Code with either light or dark theme enabled \\
    \textbf{Input/Condition:} The user switches between light and dark themes in VS Code \\
    \textbf{Output/Result:} The plugin’s interface adjusts automatically to match the theme \\[2mm]
    \textbf{How test will be performed:} The tester will open the plugin in both light and dark themes within VS Code by toggling the theme settings in the IDE. They will observe the plugin interface each time the theme is switched, ensuring that the plugin automatically adjusts to match the selected theme without any manual adjustments required. 

  \item \textbf{Design Acceptance} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} A survey report \\[2mm]
    \textbf{How test will be performed:} After a testing session, developers fill out the survey found in \ref{A.2} evaluating their experience with the plugin.
\end{enumerate}

\noindent
\colorrule
    
\subsubsection{Usability \& Humanity}

\colorrule

\medskip

\noindent
The following subsection tests cover all Usability \& Humanity requirements listed in the SRS~\cite{SRS}. They seek to validate that the system is accessible, user-centred, intuitive and easy to navigate.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-UH-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Customizable settings for refactoring preferences} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with settings panel accessible \\
    \textbf{Input/Condition:} User customizes refactoring style and detection sensitivity \\
    \textbf{Output/Result:} Custom configurations save and load successfully \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the settings menu within the tool and adjust various options, including refactoring style, colour-coded indicators, and unit preferences (metric vs. imperial). After each adjustment, the tester will observe if the interface and refactoring suggestions reflect the changes made. 

  \item \textbf{Multilingual support in user guide} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} Bilingual user navigates to system documentation \\
    \textbf{Input/Condition:} User accesses guide in both English and French \\
    \textbf{Output/Result:} The guide is accessible in both languages \\[2mm]
    \textbf{How test will be performed:} The tester will set the tool’s language to French and access the user guide, reviewing each section to ensure accurate translation and readability. After verifying the French version, they will switch the language to English, confirming consistency in content, layout, and clarity between both versions.

  \item \textbf{YouTube installation tutorial availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\ 
    \textbf{Initial State:} User access documentation resources \\
    \textbf{Input/Condition:} User follows the provided link to a YouTube tutorial \\
    \textbf{Output/Result:} Installation tutorial is available and accessible on YouTube, and user successfully installs the system. \\[2mm]
    \textbf{How test will be performed:} The tester will start with the installation instructions provided in the user guide and follow the link to the YouTube installation tutorial. They will watch the video and proceed with each installation step as demonstrated. Throughout the process, the tester will note the clarity and pacing of the instructions, any gaps between the video and the actual steps, and if the video effectively guides them to a successful installation. 

  \item \textbf{High-Contrast Theme Accessibility Check} \\[2mm]
    \textbf{Objective:} Evaluate the high-contrast themes in the refactoring tool for compliance with accessibility standards to ensure usability for visually impaired users. \\
    \textbf{Scope:} Focus on UI components that utilize high-contrast themes, including text, buttons, and backgrounds. \\
    \textbf{Methodology:} Static Analysis \\
    \textbf{Process:} 
    \begin{itemize}
      \item Identify all colour codes used in the system and categorize them by their role in the UI (i.e. background, foreground text, buttons, etc.).
      \item Use tools to measure colour contrast ratios against WCAG thresholds (4.5:1 for normal text, 3:1 for large text~\cite{WCAG}.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Developers implement themes that pass the testing process. \\[2mm]
    \textbf{Tools and Resources:} WebAIM Color Contrast Checker, WCAG guidelines documentation, internal coding standards. \\[2mm]
    \textbf{Acceptance Criteria:} All UI elements must meet WCAG contrast ratios; documentation must accurately reflect theme usage.

  \item \textbf{Audio cues for important actions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with audio cues enabled \\
    \textbf{Input/Condition:} User performs actions triggering audio cues \\
    \textbf{Output/Result:} The system emits an audible attention catching sound. \\[2mm]
    \textbf{How test will be performed:} The tester will enable audio cues in the tool's settings, then perform a series of tasks, such as running code analysis, applying refactorings, and saving changes. Each action should trigger an audio cue indicating task completion or user feedback. The tester will evaluate the volume, timing, and appropriateness of each cue and document whether the cues enhance the user experience or cause any distractions. 

  \item \textbf{Intuitive user interface for core functionality} \\[2mm]
    \textbf{Type:} Non-Functional, User Testing, Dynamic \\
    \textbf{Initial State:} IDE plugin open with code loaded \\
    \textbf{Input/Condition:} User interacts with the plugin \\
    \textbf{Output/Result:} Users can access core functions within three clicks or less \\[2mm]
    \textbf{How test will be performed:} After a testing session, developers fill out the survey found in \ref{A.2} evaluating their experience with the plugin.

  \item \textbf{Clear and concise user prompts} \\[2mm]
    \textbf{Type:} Non-Functional, User Survey, Dynamic \\
    \textbf{Initial State:} IDE plugin prompts user for input \\
    \textbf{Input/Condition:} Users follow on-screen instructions \\
    \textbf{Output/Result:} 90\% of users report the prompts are straightforward and effective \\[2mm]
    \textbf{How test will be performed:} Users complete tasks requiring prompts and answer the survey found in \ref{A.2} on the clarity of guidance provided.

  \item \textbf{Context-sensitive help based on user actions} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with help function enabled \\
    \textbf{Input/Condition:} User engages in various actions, requiring guidance \\
    \textbf{Output/Result:} Help resources are accessible within 1-3 clicks \\[2mm]
    \textbf{How test will be performed:} The tester will perform a series of tasks within the tool, such as initiating a code analysis, applying a refactoring, and adjusting settings. At each step, they will access the context-sensitive help option to confirm that the information provided is relevant to the current task. The tester will evaluate the ease of accessing help, the relevance and clarity of guidance, and whether the help content effectively supports task completion.

  \item \textbf{Clear and constructive error messaging} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with possible error scenarios triggered \\
    \textbf{Input/Condition:} User encounters an error during use \\
    \textbf{Output/Result:} 80\% of users report that error messages are helpful and courteous \\[2mm]
    \textbf{How test will be performed:} After receiving error messages, users fill out the survey found in \ref{A.2} on their clarity and constructiveness.
\end{enumerate}

\noindent
\textcolor{Blue}{\colorrule}

\subsubsection{Performance}
\colorrule

\medskip

\noindent
The following subsection tests cover all Performance requirements listed in the SRS~\cite{SRS}. These tests validate the tool’s efficiency and responsiveness under varying workloads, including code analysis, refactoring, and data reporting.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-PF-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Performance and capacity validation for analysis and refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} IDE open with multiple python projects of varying sizes ready (1,000, 5,000, 10,000, 100,000 lines of code). \\
    \textbf{Input/Condition:} Initiate the refactoring process for each project sequentially \\
    \textbf{Output/Result:} Process completes within 15 seconds for projects up to 5,000 lines of code, 20 seconds for 10,000 lines of code and within 2 minutes for 100,000 lines of code. \\[2mm]
    \textbf{How test will be performed:} The tester will use four python projects of different sizes: small (1,000 lines), medium (5,000 and 10,000 lines), and large (100,000 lines). For each project, start the refactoring process while running a timer. The scope of the test ends when the system presents the user with the completed refactoring proposal. The time taken for each project is checked against the expected result.

  \item \textbf{Integrity of refactored code against runtime errors} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Refactoring tool ready, with user-provided code and test suite loaded \\
    \textbf{Input/Condition:} User initiates refactoring on the input code \\
    \textbf{Output/Result:} Refactored code passes all tests in the user-provided suite without runtime errors and adheres to Python syntax standards \\[2mm]
    \textbf{How test will be performed:} The refactoring tool will first apply the refactoring to the user-provided code. After refactoring, an automated test suite will run, confirming that all original tests pass, indicating no loss of functionality. The refactored code will then be validated by an automatic linter to ensure compliance with Python syntax standards.

  \item \textbf{Functionality preservation post-refactoring} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Python file ready for refactoring with proper configurations for the system \\
    \textbf{Input/Condition:} User initiates refactoring on the code file \\
    \textbf{Output/Result:} The refactored code should pass 100\% of user-provided tests \\[2mm]
    \textbf{How test will be performed:} see test \hyperref[itm:FR-OV-1]{test-FR-OV-1}

  \item \textbf{Accuracy of code smell detection} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} Python file containing pre-determined code smells ready for refactoring with proper configurations for the system \\
    \textbf{Input/Condition:} User initiates refactoring on the code file \\
    \textbf{Output/Result:} All code smells determined prior to the test are detected. \\[2mm]
    \textbf{How test will be performed:} see tests in the \hyperref[4.1.2]{Code Smell Detection} section.

  \item \textbf{Valid syntax and structure in refactored code} \\[2mm]
    \textbf{Type:} Non-Functional, Automated, Dynamic \\
    \textbf{Initial State:} A refactored code file is present in the user's workspace \\
    \textbf{Input/Condition:} A python linter is run on the refactored python file \\
    \textbf{Output/Result:} Refactored code meets Python syntax and structural standards \\[2mm]
    \textbf{How test will be performed:} see test \hyperref[itm:FR-OV-2]{test-FR-OV-2}

  \item \textbf{Handling unexpected inputs} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE open and ready with various non-standard and invalid input files \\
    \textbf{Input/Condition:} User attempts to refactor invalid code files and non-Python files \\
    \textbf{Output/Result:} Tool detects invalid input, displays a clear error message, and does not crash \\[2mm]
    \textbf{How test will be performed:} The tester will sequentially give any of the following invalid files as input to the system :
    \begin{itemize}
        \item Non-Python files (e.g., .txt, .java, .cpp, .js)
        \item Invalid Python files with syntax errors (e.g., unmatched brackets, improper indentation)
        \item Corrupted files that contain random symbols or partially deleted code
    \end{itemize}
    For each file type, the tester will initiate the refactoring process and observe the tool's response. The tool should detect each invalid input, display an error message describing the issue, and recover from the error without crashing. 

  \item \textbf{Fallback Options for Failed Refactoring Attempts} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} The tool is set up in an IDE with a sample code file that includes code smells \\
    \textbf{Input/Condition:} User initiates a refactoring process on the sample code file \\
    \textbf{Output/Result:} The tool logs failed refactoring attempts, provides a clear error notification, and suggests alternative refactoring options without interrupting the overall process. \\[2mm]
    \textbf{How test will be performed:} The tester will load a sample code file into the tool that contains code smells. Upon initiating the refactoring, the tester will observe the tool’s response to any failed attempts, verifying that it logs the error. The tool should then attempt alternative refactorings without restarting the process. The tester will document the clarity of the error message, the relevance of alternative suggestions, and confirm that the tool remains functional, supporting uninterrupted refactoring of other code smells.
  

    \item \textbf{Maintainability and Adaptability of the Tool} \\[2mm]
    \textbf{Objective:} Ensure that the tool’s codebase is structured to support future updates for new Python versions and evolving coding standards, minimizing the effort required for maintenance. \\[2mm]
    \textbf{Scope:} This test applies to the tool’s code structure, documentation quality, and modularity to facilitate adaptability and maintainability over time. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a code walkthrough to evaluate the modular structure of the codebase, verifying that components are organized to allow independent updates.
      \item Review code comments, documentation, and naming conventions to ensure clarity and consistency, supporting ease of understanding for future developers.
      \item Identify any dependencies on specific Python versions and assess the ease of updating these components for compatibility with newer versions.
      \item Document any gaps in modularity or documentation and consult with the development team on improvements to support maintainability.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the code review and documentation assessment, with the project supervisor overseeing and validating improvements for long-term adaptability. \\[2mm]
    \textbf{Tools and Resources:} Code editor, documentation templates, Python development guidelines, and coding standards \\[2mm]
    \textbf{Acceptance Criteria:} The codebase is modular, well-documented, and adaptable, allowing for straightforward updates with minimal impact on existing functionality.

\end{enumerate}

\noindent
\colorrule

\subsubsection{Operational \& Environmental}
\colorrule

\medskip

\noindent
The following subsection tests cover all Operational and Environmental requirements listed in the SRS~\cite{SRS}. Testing includes adherence to emissions standards, integration with environmental metrics, and adaptability to diverse operational settings.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-OPE-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{VS Code compatibility for refactoring library extension} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} VS Code IDE open and library installed\\
    \textbf{Input/Condition:} User installs and opens the refactoring library extension in VS Code \\
    \textbf{Output/Result:} The refactoring library extension installs successfully and runs within VS Code \\[2mm]
    \textbf{How test will be performed:} The tester will navigate to the VS Code marketplace, search for the refactoring library extension, and install it. Once installed, the tester will open the extension and perform a basic refactoring task to ensure the tool operates correctly within the VS Code environment and has access to the system library.

  \item \textbf{Import and export capabilities for codebases and metrics} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} IDE plugin open with the option to import/export codebases and metrics \\
    \textbf{Input/Condition:} User imports an existing codebase and exports refactored code and metrics reports \\
    \textbf{Output/Result:} The tool successfully imports codebases, refactors them, and exports both code and metrics reports \\[2mm]
    \textbf{How test will be performed:} The tester will load an existing codebase into the tool, initiate refactoring, and select the option to export the refactored code and metrics report. The export should generate files in the selected format. The tester will verify the file formats, check for correct data structure, and validate that the content accurately reflects the refactoring and metrics generated by the tool.

  \item \textbf{PIP package installation availability} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Python environment ready without the refactoring library installed \\
    \textbf{Input/Condition:} User installs the refactoring library using the command \texttt{pip install ecooptimizer} \\
    \textbf{Output/Result:} The library installs successfully without errors and is available for use in Python scripts \\[2mm]
    \textbf{How test will be performed:} The tester will open a new Python environment and enter the command to install the refactoring library via PIP. Once installed, the tester will import the library in a Python script and execute a basic function to confirm successful installation and functionality. The test verifies the library’s availability and ease of installation for end users.

\end{enumerate}

\noindent
\colorrule

\subsubsection{Maintenance and Support}
\colorrule

\medskip

\noindent
The following subsection tests cover all Maintenance and Support requirements listed in the SRS~\cite{SRS}. These tests focus on rollback capabilities, compatibility with external libraries, automated testing, and extensibility for adding new code smells and refactoring functions.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-MS-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Extensibility for New Code Smells and Refactorings} \\[2mm]
    \textbf{Objective:} Confirm that the tool’s architecture allows for the addition of new code smell detections and refactoring techniques with minimal code changes and disruption to existing functionality. \\[2mm]
    \textbf{Scope:} This test applies to the tool’s extensibility, including modularity of code structure, ease of integration for new detection methods, and support for customization. \\[2mm]
    \textbf{Methodology:} Code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a code walkthrough focusing on the modularity and structure of the code smell detection and refactoring components.
      \item Add a sample code smell detection and refactoring function to validate the ease of integration within the existing architecture.
      \item Verify that the new function integrates seamlessly without altering existing features and that it is accessible through the tool’s main interface.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Once the system is complete, the development team will perform the code walkthrough and integration. They will review and approve any structural changes required. \\[2mm]
    \textbf{Tools and Resources:} Code editor, tool’s developer documentation, sample code smell and refactoring patterns \\[2mm]
    \textbf{Acceptance Criteria:} New code smells and refactoring functions can be added within the existing modular structure, requiring minimal changes. The new function does not impact the performance or functionality of existing features.


    \item \textbf{Maintainable and Adaptable Codebase} \\[2mm]
    \textbf{Objective:} Ensure that the codebase is modular, well-documented, and maintainable, supporting future updates and adaptations for new Python versions and standards. \\[2mm]
    \textbf{Scope:} This test covers the maintainability of the codebase, including structure, documentation, and modularity of key components. \\[2mm]
    \textbf{Methodology:} Static analysis and documentation walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase to verify the modular organization and clear separation of concerns between components.
      \item Examine documentation for code clarity and completeness, especially around key functions and configuration files.
      \item Assess code comments and the quality of function/method naming conventions, ensuring readability and consistency for future maintenance.
    \end{itemize}
    \textbf{Roles and Responsibilities:} Once the system is complete, the development team will conduct the code review, to identify areas for improvement. If necessary, they will also ensure to improve the quality of the documentation. \\[2mm]
    \textbf{Tools and Resources:} Code editor, documentation templates, code commenting standards, Python development guides \\[2mm]
    \textbf{Acceptance Criteria:} The codebase is modular and maintainable, with sufficient documentation to support future development. All major components are organized to allow for easy updates with minimal impact on existing functionality.
  
  \item \textbf{Easy rollback of updates in case of errors} \\[2mm]
    \textbf{Type:} Non-Functional, Manual, Dynamic \\
    \textbf{Initial State:} Latest version of the tool installed with the ability to apply and revert updates \\
    \textbf{Input/Condition:} User applies a simulated new update and initiates a rollback \\
    \textbf{Output/Result:} The system reverts to the previous stable state without any errors \\[2mm]
    \textbf{How test will be performed:} The tester will apply a simulated update. Following this, they will initiate the rollback function, which should restore the tool to its previous stable version. The tester will verify that all features function as expected post-rollback and document the time taken to complete the rollback process
\end{enumerate}

\newpage

\noindent
\colorrule

\subsubsection{Security}
\colorrule

\medskip

\noindent
The following subsection tests cover all Security requirements listed in the SRS~\cite{SRS}. These tests seek to validate that the tool is protected against unauthorized access, data breaches, and external threats.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-SRT-\arabic*}}, wide=0pt, font=\itshape]  
  \item \textbf{Audit Logs for Refactoring Processes} \\[2mm]
    \textbf{Objective:} Ensure that the tool maintains a secure, tamper-proof log of all refactoring processes, including pattern analysis, energy analysis, and report generation, for accountability in refactoring events. \\[2mm]
    \textbf{Scope:} This test covers the logging of refactoring events, ensuring logs are complete and tamper-proof for future auditing needs. \\[2mm]
    \textbf{Methodology:} Code walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the codebase to confirm that each refactoring event (e.g., pattern analysis, energy analysis, report generation) is logged with details such as timestamps and event descriptions.
      \item Document any logging gaps or security vulnerabilities, and consult with the development team to implement enhancements.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will review and test the logging mechanisms, with the project supervisor ensuring alignment with auditing requirements. \\[2mm]
    \textbf{Tools and Resources:} Access to logging components, tamper-proof logging tools \\[2mm]
    \textbf{Acceptance Criteria:} All refactoring processes are logged in a secure, tamper-proof manner, ensuring complete traceability for future audits.

\newpage

\noindent
\colorrule

\subsubsection{Cultural}
\colorrule

\medskip

\noindent
Cultural requirements are not applicable to this project since we are using VS Code settings and a plugin-based approach. These aspects do not involve cultural considerations, making such requirements unnecessary.

\newpage

\noindent
\colorrule

\subsubsection{Compliance}
\colorrule

\medskip

\noindent
The following subsection tests cover all Compliance requirements listed in the SRS~\cite{SRS}. The tests focus on adherence to PIPEDA, CASL, and ISO 9001, as well as SSADM standards, ensuring the tool complies with relevant regulations and aligns with professional development practices.

\begin{enumerate}[label={\bf \textcolor{Maroon}{test-CPL-\arabic*}}, wide=0pt, font=\itshape]
  \item \textbf{Compliance with PIPEDA and CASL} \\[2mm]
    \textbf{Objective:} Ensure the tool’s data collection, usage, storage, and communication practices are fully compliant with the Personal Information Protection and Electronic Documents Act (PIPEDA) and Canada’s Anti-Spam Legislation (CASL), to avoid legal penalties and enhance user trust. \\[2mm]
    \textbf{Scope:} This test applies to all processes related to data handling, storage, and user communication to verify compliance with PIPEDA and CASL. \\[2mm]
    \textbf{Methodology:} Documentation walkthrough and static analysis \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Review the tool’s data handling and storage protocols to confirm compliance with PIPEDA, particularly focusing on secure storage, data usage transparency, and privacy rights.
      \item Verify the presence of a user consent mechanism that informs users of data collection and provides options for managing their data.
      \item Inspect communication practices to ensure compliance with CASL, confirming that the tool provides users with notification and opt-in options for all communications.
      \item Document any gaps in compliance and consult with the development team for required adjustments.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the compliance review and implement any necessary updates. \\[2mm]
    \textbf{Tools and Resources:} Access to documentation on PIPEDA and CASL requirements, tool’s data handling and communication protocols, test user accounts for opt-in verification \\[2mm]
    \textbf{Acceptance Criteria:} The tool complies with all PIPEDA and CASL requirements, with secure data handling, user consent options, and compliant communication practices.

\item \textbf{Compliance with ISO 9001 and SSADM Standards} \\[2mm]
    \textbf{Objective:} Ensure the tool’s quality management and software development processes align with ISO 9001 for quality management and SSADM (Structured Systems Analysis and Design Method) standards for software development, building stakeholder trust and market acceptance. \\[2mm]
    \textbf{Scope:} This test covers the tool’s adherence to ISO 9001 quality management practices and SSADM methodologies for software development processes. \\[2mm]
    \textbf{Methodology:} Documentation walkthrough and code walkthrough \\[2mm]
    \textbf{Process:}
    \begin{itemize}
      \item Conduct a review of the tool’s quality management procedures to verify alignment with ISO 9001 standards, including documentation, testing, and feedback mechanisms.
      \item Examine software development workflows to confirm adherence to SSADM standards, focusing on design, analysis, and structured development practices.
      \item Identify any deviations from ISO 9001 and SSADM requirements, document these findings, and discuss necessary adjustments with the development team.
      \item Validate improvements in quality management and software development after implementing recommendations.
    \end{itemize}
    \textbf{Roles and Responsibilities:} The development team will conduct the standards compliance review, and the project supervisor will oversee the review process. \\[2mm]
    \textbf{Tools and Resources:} Access to ISO 9001 and SSADM standards documentation, project quality management records, and development workflows \\[2mm]
    \textbf{Acceptance Criteria:} The tool’s quality management and software development processes fully adhere to ISO 9001 and SSADM standards, supporting a high-quality, structured approach to development.
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements} \label{trace-sys}

\begin{table}[H]
  \centering
  \caption{Functional Requirements and Corresponding Test Sections}
  \begin{tabular}{|p{0.6\textwidth}|p{0.3\textwidth}|}
    \toprule \textbf{Section} & \textbf{Functional Requirement} \\ 
    
    \midrule
    Input Acceptance Tests & FR 1 \\ \hline
    Code Smell Detection Tests & FR 2 \\ \hline
    Refactoring Suggestion Tests & FR 4 \\ \hline
    Output Validation Tests & FR 3, FR 6 \\ \hline
    Tests for Report Generation & FR 9 \\ \hline
    Documentation Availability Tests & FR 10 \\ \hline
    IDE Integration Tests & FR 11 \\
    \bottomrule
  \end{tabular}
  \label{tab:sections_requirements}
\end{table}

\label{tab:nfr-trace-reqs}
\begin{table}[H]
  \centering
  \caption{Look \& Feel Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Look and Feel
    LF-1 & LFR-AP 1 \\ 
    LF-2 & LFR-AP 2 \\ 
    LF-3 & LFR-AP 3 \\ 
    LF-4 & LFR-AP 5 \\ 
    LF-5 & LFR-AP 4, LFR-ST 1-3 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Usability \& Humanity Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Usability and Humanity
    UH-1 & UHR-PS1 1 \\ 
    UH-2 & UHR-PS1 2, MS-SP 1 \\ 
    UH-3 & UHR-LRN 2 \\ 
    UH-4 & UHR-ACS 1 \\ 
    UH-5 & UHR-ACS 2 \\ 
    UH-6 & UHR-EOU 1 \\ 
    UH-7 & UHR-EOU 2 \\ 
    UH-8 & UHR-LRN 1 \\ 
    UH-9 & UHR-UPL 1 \\ 
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Performance Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Performance
    PF-1 & PR-SL 1, PR-SL 2, PR-CR 1 \\ 
    PF-2 & PR-SCR 1 \\ 
    PF-3 & PR-PAR 1 \\ 
    PF-4 & PR-PAR 2 \\ 
    PF-5 & PR-PAR 3 \\ 
    PF-6 & PR-RFT 1 \\ 
    PF-7 & PR-RFT 2 \\ 
    PF-8 & PR-LR 1, MS-MNT 5 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Operational \& Environmental Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Operational and Environmental
    Not explicitly tested & OER-EP 1 \\
    Not explicitly tested & OER-EP 2 \\
    OPE-1 & OER-WE 1 \\
    OPE-2 & OER-IAS 1 \\
    OPE-3 & OER-IAS 2 \\
    OPE-4 & OER-IAS 3 \\
    OPE-5 & OER-PR 1 \\
    Tested by FRs & OER-RL 1 \\
    Not explicitly tested & OER-RL 2 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Maintenance \& Support Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Maintenance and Support
    MS-1 & MS-MNT 1, PR-SER 1 \\
    MS-2 & MS-MNT 2 \\
    MS-3 & MS-MNT 3 \\
    Not explicitly tested & MS-MNT 4 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Security Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Security
    SRT-1 & SR-AR 1 \\
    SRT-2 & SR-AR 2 \\
    SRT-3 & SR-IR 1 \\
    SRT-4 & SR-PR 1 \\
    SRT-5 & SR-PR 2 \\
    SRT-6 & SR-AUR 1 \\
    SRT-7 & SR-AUR 2 \\
    SRT-8 & SR-IM 1 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Cultural Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Cultural
    CULT-1 & CULT 1 \\
    CULT-2 & CULT 2 \\
    CULT-3 & CULT 3 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \centering
  \caption{Compliance Tests and Corresponding Requirements}
  \begin{tabular}{|c|c|}
    \toprule \textbf{Test ID (test-)} & \textbf{Non-Functional Requirement} \\
    \midrule
    % Compliance
    CPL-1 & CL-LR 1 \\
    CPL-2 & CL-SCR 1 \\ 
    \bottomrule
  \end{tabular}
\end{table}

\newpage

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially lay out your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Detection Module}

\subsubsection{Analyzer Controller}

\textbf{Goal:} The analyzer controller serves as the central coordination unit for detecting code smells using various analysis methods. It interfaces with multiple analyzers, processes detection results, and ensures accurate filtering and customization of analysis options. The following unit tests verify the correctness of this functionality.\\

\noindent The tests validate the proper execution of smell detection, correct filtering of smells based on the chosen analysis method, appropriate handling of missing or disabled smells, and the generation of custom analysis options. Edge cases such as empty files, incorrect configurations, and mismatched smell detection methods are also considered.\\

\noindent\textbf{Target requirement(s):} FR2, FR5~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Running Smell Detection Analysis}
    \begin{itemize}
        \item Ensures the analyzer correctly detects CRC smells in a given Python file.
        \item Validates that detected smells include the correct attributes, such as message ID, file path, and confidence level.
        \item Confirms that logs capture detected smells for debugging.
    \end{itemize}

    \item \textbf{Handling Cases with No Detected Smells}
    \begin{itemize}
        \item Ensures that when no smells are detected, the controller logs an appropriate success message.
        \item Verifies that the returned list of smells is empty.
    \end{itemize}

    \item \textbf{Filtering Smells by Analysis Method}
    \begin{itemize}
        \item Ensures that the controller correctly filters smells based on the specified analysis method (e.g., AST, Astroid, Pylint).
        \item Verifies that only smells matching the given method are returned.
    \end{itemize}

    \item \textbf{Generating Custom Analysis Options}
    \begin{itemize}
        \item Ensures that the controller correctly generates custom options for AST and Astroid analysis.
        \item Validates that generated options include callable detection functions for applicable smells.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/controllers/test_analyzer_controller.py}{here}.

\subsubsection{String Concatenation in a Loop}

\textbf{Goal:} The string concatenation in a loop detection module identifies inefficient string operations occurring inside loops, which can significantly impact performance. The following unit tests verify the detection capabilities.\\

\noindent The tests evaluate different types of string concatenation scenarios within loops, ensuring that various cases are detected correctly. These include basic concatenation, nested loops, conditional concatenation, type inference, and different string interpolation methods. The robustness of the detection logic is assessed through multiple test cases covering common patterns.\\

\noindent\textbf{Target requirement(s)}: FR2~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Basic Concatenation in a Loop}
  \begin{itemize}
      \item Simple assignment string concatenation of the form \texttt{var = var + \( \ldots \)} inside \texttt{for} and \texttt{while} loops.
      \item Augmented assignment string concatenation using \texttt{+=} inside \texttt{for} and \texttt{while} loops.
      \item Concatenation involving object attributes (e.g., \texttt{var.attr += \( \ldots \)}).
      \item Concatenation modifying values inside complex objects (i.e., dictionaries, iterables).
  \end{itemize}
  
  \item \textbf{Nested Loop Concatenation}
  \begin{itemize}
      \item String concatenation inside nested loops where the outer loop contributes to the concatenation.
      \item Resetting the concatenation variable inside the outer loop but continuing inside the inner loop.
  \end{itemize}

  \item \textbf{Conditional Concatenation}
  \begin{itemize}
      \item String concatenation inside loops with \texttt{if}-\texttt{else} conditions modifying the concatenation variable.
      \item Different branches of a condition appending different string literals.
  \end{itemize}

  \item \textbf{Type Inference}
  \begin{itemize}
      \item Proper detection of string types through initial variable assignment.
      \item Proper detection of string types through assignment type hints.
      \item Proper detection of string types through function definition type hints.
      \item Proper detection of string types initialized as class variables.
  \end{itemize}

  \item \textbf{String Interpolation Methods}
  \begin{itemize}
      \item Concatenation using \texttt{\%} formatting.
      \item Concatenation using \texttt{str.format()}.
      \item Concatenation using f-strings inside loops.
  \end{itemize}

  \item \textbf{Concatenation with Repeated Reassignment}
  \begin{itemize}
      \item A variable being reassigned multiple times in the same loop iteration before proceeding to the next iteration.
  \end{itemize}

  \item \textbf{Concatenation with Variable Access Inside the Loop}
  \begin{itemize}
      \item Cases where the concatenation variable is accessed inside the loop, making refactoring ineffective since the joined result would be required at every iteration.
  \end{itemize}

  \item \textbf{Concatenation Order Sensitivity}
  \begin{itemize}
      \item Concatenation where new values are inserted at the beginning of the string instead of the end.
      \item Concatenation that involves both prefix and suffix additions within the same loop.
  \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/analyzers/test_str_concat_in_loop.py}{here}.

\subsubsection{Long Element Chain}

\textbf{Goal:} The long element chain detection module identifies excessive dictionary access chains that exceed a predefined threshold (default: 3). The following unit tests verify the detection capabilities.\\

\noindent The tests evaluate the detection of long element chains by verifying that sequences exceeding the threshold, such as deep dictionary accesses, are correctly identified. They include edge cases, such as chains just below the threshold, variations in data structures, and multiple chains in the same scope, to assess the robustness of the detection logic.\\

\noindent\textbf{Target requirement(s):} FR2~\cite{SRS} \\

\begin{itemize}
  \item \textbf{Ignores code with no chains}
  \begin{itemize}
      \item Ensures that code without any nested element chains does not trigger a detection.
  \end{itemize}

  \item \textbf{Ignores chains below the threshold}
  \begin{itemize}
      \item Verifies that element chains shorter than the threshold are not flagged.
  \end{itemize}

  \item \textbf{Detects chains exactly at threshold}
  \begin{itemize}
      \item Ensures that an element chain with a length equal to the threshold is flagged.
  \end{itemize}

  \item \textbf{Detects chains exceeding the threshold}
  \begin{itemize}
      \item Verifies that chains longer than the threshold are correctly detected.
  \end{itemize}

  \item \textbf{Detects multiple chains in the same file}
  \begin{itemize}
      \item Ensures that multiple long element chains appearing in different locations within the same code file are individually detected.
  \end{itemize}

  \item \textbf{Detects chains inside nested functions and classes}
  \begin{itemize}
      \item Confirms that element chains occurring within functions and class methods are correctly identified.
  \end{itemize}

  \item \textbf{Reports identical chains on the same line only once}
  \begin{itemize}
      \item Ensures that duplicate chains appearing on a single line are not reported multiple times.
  \end{itemize}

  \item \textbf{Handles different variable types in chains}
  \begin{itemize}
      \item Verifies detection of chains that involve a mix of dictionaries, lists, tuples, and nested structures.
  \end{itemize}

  \item \textbf{Correctly applies custom threshold values}
  \begin{itemize}
      \item Ensures that adjusting the threshold parameter correctly affects detection behavior.
  \end{itemize}

  \item \textbf{Verifies result structure and metadata}
  \begin{itemize}
      \item Confirms that detected chains return correctly formatted results, including message ID, type, and occurrence details.
  \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/analyzers/test_detect_lec.py}{here}.

\subsubsection{Repeated Calls}

\textbf{Goal:} The repeated calls detection module identifies instances where function or method calls are redundantly executed within the same scope, leading to unnecessary performance overhead. It ensures that developers receive precise recommendations for caching results when applicable. The following unit tests validate the accuracy of this functionality.\\ 

\noindent The tests assess the module’s ability to detect redundant function calls, ignore functionally distinct calls, handle object state changes, and recognize cases where caching would not be beneficial. Edge cases, such as external function calls, built-in function invocations, and method calls on different objects, are also considered. \\

\noindent \textbf{Target requirement(s):} FR2, FR3, PR-PAR2, PR-PAR3~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Detecting Repeated Function Calls}
    \begin{itemize}
        \item Ensures the detection of repeated function calls with identical arguments within the same scope.
        \item Validates that function calls on different arguments are not incorrectly flagged.
    \end{itemize}

    \item \textbf{Detecting Repeated Method Calls}
    \begin{itemize}
        \item Ensures that method calls on the same object instance are detected when repeated within a function.
        \item Verifies that method calls on different object instances are not falsely flagged.
    \end{itemize}

    \item \textbf{Handling Object State Modifications}
    \begin{itemize}
        \item Ensures that function calls on objects with modified attributes between calls are not flagged.
        \item Verifies that method calls are only flagged when they occur without intervening state changes.
    \end{itemize}

    \item \textbf{Detecting External Function Calls}
    \begin{itemize}
        \item Detects redundant external function calls such as \texttt{len(data.get("key"))}.
        \item Ensures that only calls with matching parameters and return values are considered redundant.
    \end{itemize}

    \item \textbf{Handling Built-in Functions}
    \begin{itemize}
        \item Ensures that expensive built-in function calls (e.g., \texttt{max(data)}) are detected when redundant.
        \item Verifies that lightweight built-in functions (e.g., \texttt{abs(-5)}) are ignored.
    \end{itemize}

    \item \textbf{Ensuring Accuracy and Performance}
    \begin{itemize}
        \item Ensures that detection does not introduce false positives by incorrectly flagging calls that differ in meaningful ways.
        \item Verifies that detection performance scales efficiently with large codebases.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/analyzers/test_detect_repeated_calls.py}{here}.

\subsubsection{Long Message Chain}

\textbf{Goal:} The long message chain detection module identifies method call chains that exceed a predefined threshold (default: 5 calls). The following unit tests verify the detection capabilities. \\

\noindent The tests evaluate the detection of long message chains by verifying that sequences exceeding the threshold, such as five consecutive messages within a short time, are correctly identified. They include edge cases, like chains just below the threshold or varying in length, to assess the robustness of the detection logic\\

\noindent\textbf{Target requirement(s):} FR2 ~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Detects exact five calls chain} \newline
    Ensures that a method chain with exactly five calls is flagged.
    
    \item \textbf{Detects six calls chain} \newline
    Verifies that a chain with six method calls is detected as a smell.
    
    \item \textbf{Ignores chain of four calls} \newline
    Ensures that a chain with only four calls (below threshold) is not flagged.
    
    \item \textbf{Detects chain with attributes and calls} \newline
    Tests detection of a chain that involves both attribute access and method calls.
    
    \item \textbf{Detects chain inside a loop} \newline
    Ensures detection of a chain meeting the threshold when inside a loop.
    
    \item \textbf{Detects multiple chains on one line} \newline
    Verifies that only the first long chain on a single line is reported.
    
    \item \textbf{Ignores separate statements} \newline
    Ensures that separate method calls across multiple statements are not mistakenly combined into a single chain.
    
    \item \textbf{Ignores short chain comprehension} \newline
    Ensures that a short chain within a list comprehension is not flagged.
    
    \item \textbf{Detects long chain comprehension} \newline
    Verifies that a list comprehension with a long method chain is detected.
    
    \item \textbf{Detects five separate long chains} \newline
    Ensures that multiple long chains on separate lines within the same function are individually detected.
    
    \item \textbf{Ignores element access chains} \newline
    Confirms that attribute and index lookups without method calls are not flagged.
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/analyzers/test_long_message_chain.py}{here}


\subsubsection{Long Lambda Element}

\textbf{Goal:} The following unit tests verify the correct detection of long lambda functions based on expression count and character length thresholds.\\

\noindent The goal of this test suite is to ensure the detection system accurately identifies long lambda 
functions based on predefined complexity thresholds, such as expression count and character 
length. It verifies that the detection logic is precise, avoiding false positives for trivial 
or short lambdas while correctly flagging complex or lengthy ones. The tests cover edge cases, 
including nested lambdas and inline lambdas passed as arguments to functions like map or 
filter. Additionally, the suite ensures degenerate cases, such as empty or extremely short 
lambdas, are not mistakenly flagged. By validating these scenarios, the detection system 
maintains robustness and reliability in identifying problematic lambda expressions.\\

\noindent\textbf{Target requirement(s):} FR2 ~\cite{SRS} \\

\begin{itemize}
    \item \textbf{No lambdas present} \newline
    Ensures that when no lambda functions exist in the code, no smells are detected.
    
    \item \textbf{Short single lambda} \newline
    Confirms that a single short lambda (well under the length threshold) with only one expression is not flagged.

    \item \textbf{Lambda exceeding expression count} \newline
    Detects a lambda function that contains multiple expressions, exceeding the threshold for complexity.

    \item \textbf{Lambda exceeding character length} \newline
    Identifies a lambda function that surpasses the maximum allowed character length, making it difficult to read.

    \item \textbf{Lambda exceeding both thresholds} \newline
    Flags a lambda function that is both too long in character length and contains too many expressions.

    \item \textbf{Nested lambda functions} \newline
    Ensures that both outer and inner nested lambdas are properly detected as long expressions.

    \item \textbf{Inline lambda passed to function} \newline
    Detects lambda functions that are passed inline to functions like \texttt{map} and \texttt{filter} when they exceed the complexity thresholds.

    \item \textbf{Trivially short lambda function} \newline
    Verifies that degenerate cases, such as a lambda with no real body or trivial operations, are not mistakenly flagged.
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/analyzers/test_long_lambda_element.py}{here}

\subsection{CodeCarbon Measurement Module}
\textbf{Goal:} The CodeCarbon Measurement module is designed to measure the carbon emissions associated with running a specific piece of code. It integrates with the CodeCarbon library, which calculates the energy consumption based on the execution of a Python script.\\ \\
\noindent The tests validate the functionality of the system in measuring and handling energy consumption data for code execution. They ensure that the system correctly tracks carbon emissions using the CodeCarbon library, handles both successful and failed subprocess executions, processes emissions data from CSV files, and appropriately logs all relevant events.\\

\noindent \textbf{Target requirement(s):} FR5, FR6,PR-RFT1, PR-SCR1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Handle CodeCarbon Measurements with a Valid File Path}
    \begin{itemize}
        \item When a valid file path is provided, the system correctly invokes the subprocess for CodeCarbon.
        \item The start and stop API endpoints of the EmissionsTracker are called, and a success message is logged.
    \end{itemize}

    \item \textbf{Handle CodeCarbon Measurements with a Valid File Path that Causes a Subprocess Failure}
    \begin{itemize}
        \item The subprocess is invoked even if an error occurs during the file execution.
        \item The system logs an error message and the emissions data is set to None when execution fails.
    \end{itemize}

    \item \textbf{Results Produced by CodeCarbon Run at a Valid CSV File Path and Can Be Read}
    \begin{itemize}
        \item The emissions data can be read successfully from a valid CSV file produced by CodeCarbon.
        \item The function returns the last row of emissions data.
    \end{itemize}

    \item \textbf{Results Produced by CodeCarbon Run at a Valid CSV File Path but the File Cannot Be Read}
    \begin{itemize}
        \item An error message is logged when the CSV file cannot be read.
        \item The function returns \texttt{None} if reading the CSV file fails.
    \end{itemize}

    \item \textbf{Given CSV Path for Results Produced by CodeCarbon Does Not Have a File}
    \begin{itemize}
        \item When the given CSV path does not point to an existing file, an error message is logged.
        \item The function returns \texttt{None} when the file is missing.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/src/ecooptimizer/measurements/codecarbon_energy_meter.py}{here}.

\subsection{Refactoring Module}

\subsubsection{Refactorer Controller}

\textbf{Goal:} These tests verify the behavior of the RefactorerController in various scenarios, ensuring that it handles refactoring correctly and interacts with external components as expected.\\

\noindent \textbf{Target requirement(s):} FR5, UHR-UPL1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Verifying Successful Refactoring with a Valid Refactorer}
    \begin{itemize}
        \item Ensures that the RefactorerController correctly runs the refactorer when a valid refactorer is available.
        \item Checks that the logger captures the refactoring event.
        \item Ensures the refactor method is called with the expected arguments.
        \item Verifies that the modified file path is correctly generated.
    \end{itemize}

    \item \textbf{Handling Case When No Refactorer is Found}
    \begin{itemize}
        \item Ensures that if no refactorer is found for a given smell, the RefactorerController raises a \texttt{NotImplementedError}.
        \item Confirms that the appropriate error message is logged.
    \end{itemize}

    \item \textbf{Handling Multiple Refactorer Calls for the Same Smell}
    \begin{itemize}
        \item Tests the behavior when the refactorer is called multiple times for the same smell.
        \item Ensures that the smell counter is updated correctly.
        \item Verifies that unique file names are generated for each call.
    \end{itemize}

    \item \textbf{Verifying the Behavior When Overwrite is Disabled}
    \begin{itemize}
        \item Ensures that when the overwrite flag is set to false, the refactor method is called with the correct argument.
        \item Verifies that files are not overwritten when this option is disabled.
    \end{itemize}

    \item \textbf{Handling Case Where No Files are Modified}
    \begin{itemize}
        \item Checks that when no files are modified by the refactorer, the RefactorerController correctly returns an empty list of modified files.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/controllers/test_refactorer_controller.py}{here}.

\subsubsection{String Concatenation in a Loop}

\textbf{Goal:} The refactoring module transforms inefficient string concatenation into a more performant approach using list accumulation and \texttt{''.join()}. Unit tests for this module ensure that:\\

\noindent\textbf{Target requirement(s):} FR3, FR6~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Proper Initialization of the List}
    \begin{itemize}
        \item The string variable being concatenated is replaced with a list at the start of the loop.
        \item The list is initialized to an empty list if the initial string is obviously empty.
        \item Otherwise, the list is initialized with the initial string as the first item.
    \end{itemize}

    \item \textbf{Correct Usage of List Methods in the Loop}
    \begin{itemize}
        \item The \texttt{append()} method is used instead of assignments inside the loop.
        \item The \texttt{insert()} method is used to insert values at the beginning of the string.
        \item If the string is re-initialized inside the loop, the \texttt{clear()} method is used to empty the list before re-initializing it.
        \item If multiple concatenations happen in the same loop, each is accumulated in the correct list.
    \end{itemize}

    \item \textbf{Correct String Joining After the Loop}
    \begin{itemize}
        \item The accumulated list is joined using \texttt{''.join(list)} after the loop.
        \item The final assignment replaces the original concatenation variable with the joined string.
    \end{itemize}

    \item \textbf{Handling of Edge Cases from Detection}
    \begin{itemize}
        \item If the concatenation variable is accessed inside the loop, no refactoring is applied.
        \item Conditional concatenations result in conditional list appends.
        \item Order-sensitive concatenations (e.g., prefix insertions) preserve the original behavior.
    \end{itemize}

    \item \textbf{Preserving Readability and Maintainability}
    \begin{itemize}
        \item The refactored code maintains readability and does not introduce unnecessary complexity.
        \item Nested loop modifications preserve the correct scoping of the refactored lists.
        \item The refactored code maintains proper formatting and indentation.
        \item Unnecessary modifications to unrelated code are avoided.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_str_concat_in_loop_refactor.py}{here}.

\subsubsection{Long Element Chain}

\textbf{Goal:} The long element chain refactoring module simplifies deeply nested dictionary accesses by flattening them into top-level keys while preserving functionality. The following unit tests verify the correctness of this transformation.\\

\noindent The tests assess the ability to detect and refactor long element chains by verifying correct dictionary transformations, accessing pattern updates, and handling of various data structures. Edge cases, such as shallow accesses, multiple affected files, and mixed depths of access, are included to ensure robustness. \\

\noindent\textbf{Target requirement(s):} FR3, FR6, PR-PAR3~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Identifies and Refactors Basic Nested Dictionary Access}
    \begin{itemize}
        \item Ensures that deeply nested dictionary keys are detected and refactored correctly.
    \end{itemize}

    \item \textbf{Refactors Dictionary Accesses Across Multiple Files}
    \begin{itemize}
        \item Verifies that dictionary changes propagate correctly when a deeply nested dictionary is accessed in different files.
    \end{itemize}

    \item \textbf{Handles Dictionary Access via Class Attributes}
    \begin{itemize}
        \item Ensures that nested dictionary accesses within class attributes are correctly detected and refactored.
    \end{itemize}

    \item \textbf{Ignores Shallow Dictionary Accesses}
    \begin{itemize}
        \item Verifies that dictionary accesses below the predefined threshold remain unchanged.
    \end{itemize}

    \item \textbf{Handles Multiple Long Element Chains in the Same File}
    \begin{itemize}
        \item Ensures that all occurrences of excessive dictionary accesses in a file are refactored individually.
    \end{itemize}

    \item \textbf{Detects and Refactors Mixed Access Depths}
    \begin{itemize}
        \item Confirms that the module correctly differentiates and processes deep accesses while ignoring shallow ones.
    \end{itemize}

    \item \textbf{Validates Resulting Metadata and Formatting}
    \begin{itemize}
        \item Confirms that the refactored output includes well-structured results, such as correct message IDs, types, and occurrences.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_long_element_chain.py}{here}.

\subsubsection{Member Ignoring Method}

\textbf{Goal:} The member ignoring method refactoring module ensures that methods that do not reference instance attributes or methods are converted into static methods. This transformation improves code clarity, enforces proper design patterns, and eliminates unnecessary instance bindings. The following unit tests validate the correctness of this functionality.\\

\noindent The tests assess the correct detection of methods that can be converted to static methods, proper removal of \texttt{self} as a parameter, correct updating of method calls, and handling of inheritance. Edge cases such as instance-dependent methods, overridden methods, and preserving existing decorators are also considered.\\

\noindent \textbf{Target requirement(s):} FR3, FR6~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Correct Addition of the \texttt{@staticmethod} Decorator}
    \begin{itemize}
        \item The method receives the \texttt{@staticmethod} decorator when it does not use \texttt{self} or any instance attributes.
        \item The decorator is added directly above the method definition, preserving any existing decorators.
    \end{itemize}

    \item \textbf{Removal of the \texttt{self} Parameter}
    \begin{itemize}
        \item The first parameter of the method is removed if it is named \texttt{self}.
        \item Other parameters remain unchanged.
        \item The method signature is correctly adjusted to reflect the removal.
    \end{itemize}

    \item \textbf{Modify Instance Calls to the Method}
    \begin{itemize}
        \item Instance objects of the class calling the method will be modified to use a static call directly from the class itself.
        \item Calls in a different file from the one where the smell was detected will also have the appropriate calls modified.
    \end{itemize}

    \item \textbf{Handling of Methods in Classes with Inheritance}
    \begin{itemize}
        \item If a subclass instance calls the method, the call is also modified.
        \item If a method is overridden in a subclass, refactoring is \textbf{not} applied.
    \end{itemize}

    \item \textbf{Ensuring No Modification to Instance-Dependent Methods}
    \begin{itemize}
        \item Methods that reference \texttt{self} directly (e.g., \texttt{self.attr}, \texttt{self.method()}) are not modified.
        \item Methods that indirectly access instance attributes (e.g., through another method call) are left unchanged.
    \end{itemize}

    \item \textbf{Preserving Readability and Maintainability}
    \begin{itemize}
        \item The refactored code maintains proper formatting and indentation.
        \item Unnecessary modifications to unrelated code are avoided.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_member_ignoring_method.py}{here}.

\subsubsection{Use a Generator}

\textbf{Goal:} The use-a-generator refactoring module optimizes list comprehensions used within functions like \texttt{all()} and \texttt{any()} by transforming them into generator expressions. This refactoring improves memory efficiency while maintaining code correctness. The following unit tests validate the accuracy of this functionality.\\

\noindent The tests ensure that list comprehensions inside \texttt{all()} and \texttt{any()} calls are correctly converted to generator expressions while preserving original behavior. Additional test cases verify proper handling of multiline comprehensions, nested conditions, and various iterable types. Edge cases, such as improperly formatted comprehensions or comprehensions spanning multiple lines, are also considered.\\

\noindent \textbf{Target requirement(s):} FR3, FR5, FR6, PR-PAR3~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Refactoring List Comprehensions Inside \texttt{all()} Calls}
    \begin{itemize}
        \item Ensures that list comprehensions within \texttt{all()} are transformed into generator expressions.
        \item Validates that the transformation preserves original functionality.
    \end{itemize}

    \item \textbf{Refactoring List Comprehensions Inside \texttt{any()} Calls}
    \begin{itemize}
        \item Ensures that list comprehensions within \texttt{any()} are transformed into generator expressions.
        \item Verifies that the modified code remains functionally identical to the original.
    \end{itemize}

    \item \textbf{Handling Multi-line List Comprehensions}
    \begin{itemize}
        \item Ensures that multi-line list comprehensions are refactored correctly.
        \item Verifies that proper indentation and formatting are preserved after transformation.
    \end{itemize}

    \item \textbf{Handling Edge Cases}
    \begin{itemize}
        \item Ensures that improperly formatted comprehensions do not result in errors.
        \item Confirms that refactoring does not introduce unnecessary modifications or change unrelated code.
    \end{itemize}

    \item \textbf{Ensuring Correct Formatting and Readability}
    \begin{itemize}
        \item Validates that refactored code adheres to Python’s style guidelines.
        \item Ensures that readability and maintainability are preserved after refactoring.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_list_comp_any_all_refactor.py}{here}.

\subsubsection{Cache Repeated Calls}

\textbf{Goal:} The cache repeated calls refactoring module optimizes redundant function and method calls by storing results in local variables, reducing unnecessary recomputation. This refactoring enhances performance by eliminating duplicate executions of expensive operations. The following unit tests validate the accuracy of this functionality.\\

\noindent The tests ensure that function and method calls that produce identical results are cached and replaced with stored values where applicable. Additional test cases verify proper handling of method calls on objects, preserving object state, and integrating with function arguments. Edge cases, such as function calls with varying inputs, method calls on different instances, and presence of docstrings, are also considered.\\

\noindent \textbf{Target requirement(s):} FR3, FR5, FR6, PR-PAR3, PR-PAR2~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Refactoring Repeated Function Calls}
    \begin{itemize}
        \item Ensures that repeated function calls within a scope are replaced with cached results.
        \item Validates that caching is only applied when function arguments remain unchanged.
    \end{itemize}

    \item \textbf{Refactoring Repeated Method Calls}
    \begin{itemize}
        \item Ensures that method calls on the same object instance are cached and replaced with stored values.
        \item Verifies that method calls on different object instances are not incorrectly refactored.
    \end{itemize}

    \item \textbf{Handling Object State Modifications}
    \begin{itemize}
        \item Ensures that method calls on objects whose attributes change between calls are not cached.
        \item Verifies that method calls are only cached when no state changes occur.
    \end{itemize}

    \item \textbf{Handling Edge Cases}
    \begin{itemize}
        \item Ensures that function calls with varying arguments are not incorrectly cached.
        \item Confirms that caching does not interfere with function scope, closures, or nested function calls.
    \end{itemize}

    \item \textbf{Refactoring in the Presence of Docstrings}
    \begin{itemize}
        \item Ensures that refactoring does not alter function behavior when docstrings are present.
        \item Verifies that caching maintains readability and proper formatting.
    \end{itemize}

    \item \textbf{Ensuring Correct Formatting and Readability}
    \begin{itemize}
        \item Validates that refactored code adheres to Python’s style guidelines.
        \item Ensures that readability and maintainability are preserved after refactoring.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_repeated_calls.py}{here}.

\subsubsection{Long Parameter List}

\textbf{Goal:} The Long Parameter List refactoring module replaces function definitions and calls involving a large number of parameters with grouped parameter instances. This enhances the efficiency of the code as related parameters are grouped together into instantiated parameters. The validity of the functionality can be ensured through unit tests.\\

\noindent The tests ensure that all cases for function declarations, including constructors, instance methods, static methods and standalone functions are updated with the group parameter instances. Similarly, corresponding calls to these functions as well as references to original parameters are preserved. The refactored result also preserves use of default values in function signature and positional arguments in function calls.x\\

\noindent \textbf{Target requirement(s):} FR3, FR6~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Handle Constructor with More Parameters than Configured Limit, All Used}
    \begin{itemize}
        \item The refactor correctly handles constructors with 8 parameters, including a mix of positional and keyword arguments.
        \item Function signature and instantiation are updated to include all 8 parameters.
    \end{itemize}

    \item \textbf{Handle Constructor More Parameters than Configured Limit, some of which are Unused}
    \begin{itemize}
        \item Constructor correctly handles unused parameters by excluding them from the updated signature and instantiation.
        \item The refactor updates the constructor and function body to properly reflect the used parameters.
    \end{itemize}

    \item \textbf{Handle Instance Method More Parameters than Configured Limit (2 Defaults)}
    \begin{itemize}
        \item The refactor correctly handles instance methods with 8 parameters, including default values for some.
        \item The method signature and instantiation are updated to preserve the default values while reflecting the used parameters.
    \end{itemize}

    \item \textbf{Handle Instance Method Refactor with More Parameters than Configured Limit, some of which are Unused}
    \begin{itemize}
        \item Unused parameters in the instance method are correctly excluded and grouped into new data and config parameter classes.
        \item Function maintains the correct structure and that the method now accepts the new class objects instead of individual parameters.
    \end{itemize}

    \item \textbf{Handle Static Method Refactor with More Parameters than Configured Limit, some of which are Unused and some have Default Values}
    \begin{itemize}
        \item Unused parameters are correctly excluded from the static method signature and instantiation.
        \item Parameters with default values are properly maintained in the updated method signature and body.
    \end{itemize}

    \item \textbf{Handle Standalone Function Refactor with More Parameters than Configured Limit, some of which are Unused}
    \begin{itemize}
        \item Unused parameter is excluded from the function signature after the refactor.
        \item Refactored function uses grouped parameter classes to handle the remaining parameters efficiently.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_long_parameter_list_refactor.py
}{here}.

\subsubsection{Long Message Chain}

\textbf{Goal:} The following unit tests verify the correctness of refactoring long element chains into more readable and efficient code while maintaining valid Python syntax.\\

\noindent This test suite focuses on validating the refactoring of long method chains into 
intermediate variables, improving code readability while preserving functionality. 
It ensures that simple method chains are split correctly and that special cases, 
such as f-strings or chains with arguments, are handled appropriately. The tests 
also verify that refactoring maintains proper indentation, especially within nested 
blocks like if statements or loops. Additionally, the suite confirms that 
refactoring does not alter the behavior of the original code, even in contexts 
like print statements. By testing both long and short chains, the suite ensures 
consistency and correctness across various scenarios.\\

\noindent \textbf{Target requirement(s):} FR5, FR6, FR3 ~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Basic method chain refactoring} \newline
    Ensures that a simple method chain is refactored correctly into separate intermediate variables.

    \item \textbf{F-string chain refactoring} \newline
    Verifies that method chains applied to f-strings are properly broken down while preserving correctness.

    \item \textbf{Modifications even if the chain is not long} \newline
    Ensures that method chains are refactored consistently, even if they do not exceed the length threshold.

    \item \textbf{Proper indentation preserved} \newline
    Confirms that the refactored code maintains the correct indentation when inside a block statement such as an \texttt{if} condition.

    \item \textbf{Method chain with arguments} \newline
    Tests that method chains containing arguments (e.g., \texttt{replace("H", "J")}) are correctly refactored.

    \item \textbf{Print statement preservation} \newline
    Ensures that method chains within a \texttt{print} statement are refactored without altering their functionality.

    \item \textbf{Nested method chains} \newline
    Verifies that nested method chains (e.g., method calls on method results) are properly refactored into intermediate variables.
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_long_element_chain.py}{here}

\subsubsection{Long Lambda Element}

\textbf{Goal:} The following unit tests verify the correctness of refactoring long lambda expressions into named functions while maintaining valid Python syntax and preserving code functionality.\\

\noindent The goal of this test suite is to ensure long lambda expressions are refactored into named 
functions while maintaining code functionality, readability, and proper syntax. It verifies 
that simple single-line lambdas are converted correctly and that more complex cases, such as 
multi-line lambdas or those with multiple parameters, are handled appropriately. The tests 
ensure that refactoring preserves the original behavior of the code, even for lambdas used 
as keyword arguments or passed to functions like map or reduce. Additionally, the suite 
confirms that no unnecessary changes, such as added print statements, are introduced during 
refactoring. By covering a wide range of cases, the suite ensures the refactoring process
 is both reliable and effective.\\

\noindent \textbf{Target requirement(s):} FR5, FR6, FR3 ~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Basic lambda conversion} \newline
    Verifies that a simple single-line lambda expression is correctly converted into a named function with proper indentation and structure.

    \item \textbf{No extra print statements} \newline
    Ensures that the refactoring process does not introduce unnecessary print statements when converting lambda expressions.

    \item \textbf{Lambda in function argument} \newline
    Tests that lambda expressions used as arguments to other functions (e.g., in \texttt{map()} calls) are properly refactored while maintaining the original function call structure.

    \item \textbf{Multi-argument lambda} \newline
    Verifies that lambda expressions with multiple parameters are correctly converted into named functions with the appropriate parameter list.

    \item \textbf{Lambda with keyword arguments} \newline
    Ensures that lambda expressions used as keyword arguments in function calls are properly refactored while preserving the original keyword argument syntax and indentation.

    \item \textbf{Very long lambda function} \newline
    Tests the refactoring of complex, multi-line lambda expressions with extensive mathematical operations, verifying that the converted function maintains the original logic and structure.
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/refactorers/test_long_lambda_element_refactoring.py}{here}


\subsection{VsCode Plugin}

\subsubsection{Detect Smells Command}

\textbf{Goal:} The detect smells command is responsible for initiating the smell detection process, retrieving results from the backend, and ensuring proper highlighting in the VS Code editor. The command must handle various scenarios, including caching, missing editor instances, and server failures, while providing meaningful feedback to the user. The following unit tests verify its accuracy.\\

\noindent The tests assess the correct fetching and caching of smells, proper interaction with the highlighting module, handling of missing files or inactive editors, and the system's ability to recover from server downtime. Edge cases such as rapidly changing file hashes and updates to enabled smells are also considered.\\

\noindent\textbf{Target requirement(s):} FR10, OER-IAS1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Handling of Missing Active Editor}
    \begin{itemize}
        \item The command shows an error message when no active editor is found.
    \end{itemize}

    \item \textbf{Handling of Missing File Path}
    \begin{itemize}
        \item The command shows an error message when the active editor has no valid file path.
    \end{itemize}

    \item \textbf{Handling of No Enabled Smells}
    \begin{itemize}
        \item The command shows a warning message when no smells are enabled in the configuration.
    \end{itemize}

    \item \textbf{Using Cached Smells}
    \begin{itemize}
        \item The command uses cached smells when the file hash and enabled smells match the cached data.
        \item The command highlights the cached smells in the editor.
    \end{itemize}

    \item \textbf{Fetching New Smells}
    \begin{itemize}
        \item The command fetches new smells when the file hash changes or enabled smells are updated.
        \item The command updates the cache with the new smells and highlights them in the editor.
    \end{itemize}

    \item \textbf{Handling of Server Downtime}
    \begin{itemize}
        \item The command shows a warning message when the server is down and no cached smells are available.
    \end{itemize}

    \item \textbf{Highlighting Smells}
    \begin{itemize}
        \item The command highlights detected smells in the editor when smells are found.
        \item The command shows a success message with the number of highlighted smells.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/commands/detectSmells.test.ts}{here}.

\subsubsection{Refactor Smells Command}

\textbf{Goal:} The refactorSmell command is responsible for refactoring code areas identified as "smells" in a project. It works by refactoring areas in code that could benefit from refactoring (smells) that are chosen by the user. The process involves multiple steps, including saving the file, calling a backend refactoring service to refactor the identified smell, updating any relevant data, and initiating a refactor preview to the user.\\

\noindent The tests assess the correct fetching and caching of smells, proper interaction with the highlighting module, handling of missing files or inactive editors, and the system's ability to recover from server downtime. Edge cases such as rapidly changing file hashes and updates to enabled smells are also considered.\\

\noindent\textbf{Target requirement(s):} FR5, FR6, FR10, PR-RFT1, PR-RFT2 ~\cite{SRS} \\

\begin{itemize}
    \item \textbf{No Active Editor Found}
    \begin{itemize}
        \item The command correctly handles the case where there is no active editor open.
        \item An appropriate error message is shown when no editor or file path is available.
    \end{itemize}

    \item \textbf{Attempting to Refactor When No Smells Are Detected}
    \begin{itemize}
        \item The command does not proceed when no smells are detected in the file.
        \item An error message is shown indicating that no smells are detected for refactoring.
    \end{itemize}

    \item \textbf{Attempting to Refactor When Selected Line Doesn’t Match Any Smell}
    \begin{itemize}
        \item The command doesn't proceed if the selected line doesn't match any detected smell.
        \item An error message is shown to inform the user that no matching smell was found for refactoring.
    \end{itemize}

    \item \textbf{Refactoring a Smell When Found on the Selected Line}
    \begin{itemize}
        \item The command successfully saves the current file and triggers the refactoring of a detected smell.
        \item The \texttt{refactorSmell} method is called with the correct parameters, and the refactored preview is shown to the user.
    \end{itemize}

    \item \textbf{Handling API Failure During Refactoring}
    \begin{itemize}
        \item The command gracefully handles API failures during the refactoring process.
        \item An error message is displayed to the user if refactoring fails, with the appropriate details logged.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/commands/refactorSmell.test.ts}{here}.

\subsubsection{Document Hashing}

\textbf{Goal:} The document hashing module is responsible for generating and managing document hashes to track changes in files. By ensuring efficient tracking, this module allows caching mechanisms to work correctly and prevents unnecessary reprocessing. The following unit tests validate its correctness.\\

\noindent The tests verify the module’s ability to detect file modifications, handle new files, and prevent redundant updates when no changes occur. Edge cases such as rapid sequential edits, hash mismatches, and multiple concurrent document updates are also considered.\\

\noindent\textbf{Target requirement(s):} FR10, OER-IAS1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Handling of Unchanged Document Hashes}
    \begin{itemize}
        \item The module does not update the workspace storage if the document hash has not changed.
        \item The existing hash is retained for the document.
    \end{itemize}

    \item \textbf{Handling of Changed Document Hashes}
    \begin{itemize}
        \item The module updates the workspace storage when the document hash changes.
        \item The new hash is correctly calculated and stored.
    \end{itemize}

    \item \textbf{Handling of New Documents}
    \begin{itemize}
        \item The module updates the workspace storage when no hash exists for the document.
        \item A new hash is generated and stored for the document.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/utils/hashDocs.test.ts}{here}.


\subsubsection{File Highlighter}

\textbf{Goal:} The file highlighter module enhances code visibility by applying visual decorations to highlight detected code smells in the editor. It ensures that identified issues are clearly distinguishable while preserving readability. The following unit tests verify the correctness of this functionality.\\

\noindent The tests assess the correct creation of decorations, accurate application of highlighting based on detected smells, proper handling of initial and subsequent highlights, and the removal of outdated decorations. Edge cases such as overlapping decorations and incorrect style applications are also considered.\\

\noindent\textbf{Target requirement(s):} FR10, OER-IAS1, LFR-AP2~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Creation of Decorations}
    \begin{itemize}
        \item Decorations are created with the correct color and style for each type of code smell.
        \item The decoration type is properly initialized and can be applied to the editor.
    \end{itemize}

    \item \textbf{Highlighting Smells}
    \begin{itemize}
        \item Smells are highlighted in the editor based on their line occurrences.
        \item The hover content for each smell is correctly associated with the decoration.
    \end{itemize}

    \item \textbf{Handling of Initial Highlighting}
    \begin{itemize}
        \item On the first initialization, decorations are applied without resetting existing ones.
        \item The decorations are properly stored for future updates.
    \end{itemize}

    \item \textbf{Resetting Decorations}
    \begin{itemize}
        \item On subsequent calls, existing decorations are disposed of before applying new ones.
        \item The reset process ensures no overlapping or redundant decorations.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/ui/fileHighlighter.test.ts}{here}.

\subsubsection{Hover Manager}

\textbf{Goal:} The Hover Manager module manages hover functionality for Python files, providing hover content for detected code smells and allowing refactoring commands. The following unit tests verify the accuracy of this functionality.\\

\noindent The tests assess the ability of the \texttt{HoverManager} to register hover providers, handle hover content, update smells, generate refactor commands, and ensure correct hover content formatting. Edge cases, such as no smells and the correct propagation of updates to smells, are also considered.\\

\noindent \textbf{Target requirement(s):} LFR-AP2~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Registers hover provider for Python files}
    \begin{itemize}
        \item Verifies that the \texttt{HoverManager} correctly registers a hover provider for Python files.
    \end{itemize}

    \item \textbf{Subscribes hover provider correctly}
    \begin{itemize}
        \item Ensures that the hover provider is correctly subscribed to the context manager.
    \end{itemize}

    \item \textbf{Returns null for hover content if there are no smells}
    \begin{itemize}
        \item Checks that \texttt{getHoverContent} returns \texttt{null} when no smells are detected for a file.
    \end{itemize}

    \item \textbf{Updates smells when \texttt{getInstance} is called again}
    \begin{itemize}
        \item Verifies that when \texttt{getInstance} is called again with new smells, the manager updates the stored smells and returns the same instance.
    \end{itemize}

    \item \textbf{Updates smells correctly}
    \begin{itemize}
        \item Confirms that \texttt{updateSmells} correctly updates the list of smells in the manager.
    \end{itemize}

    \item \textbf{Generates valid hover content}
    \begin{itemize}
        \item Ensures that \texttt{getHoverContent} generates valid hover content with correctly formatted smell details, including refactor commands and proper structure.
    \end{itemize}

    \item \textbf{Registers refactor commands}
    \begin{itemize}
        \item Verifies that the refactor commands are properly registered for individual and all smells of a specific type.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/ui/hoverManager.test.ts}{here}.

\subsubsection{Line Selection Manager}

\textbf{Goal:} The Line Selection Manager module provides functionality for detecting and commenting on code smells based on a line selection. The following unit tests verify the correctness of this functionality.\\

\noindent The tests assess the ability of the \texttt{LineSelectionManager} to handle various scenarios such as missing editor instances, multiple smells on a line, single-line vs. multi-line selections, and correct comment generation. Edge cases, such as mismatched document hashes, non-existent smells, and the absence of selected text, are also considered.\\

\noindent \textbf{Target requirement(s):} UHR-EOU1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Removes last comment if decoration exists}
    \begin{itemize}
        \item Verifies that the last comment decoration is removed correctly if one exists, ensuring that the decoration is disposed of properly.
    \end{itemize}

    \item \textbf{Does not proceed if no editor is provided}
    \begin{itemize}
        \item Ensures that no action is taken when \texttt{commentLine} is called with \texttt{null} as the editor.
    \end{itemize}

    \item \textbf{Does not add comment if no smells detected for file}
    \begin{itemize}
        \item Checks that no comment is added when no smells are detected for the current file, confirming that no unnecessary decorations are applied.
    \end{itemize}

    \item \textbf{Does not add comment if document hash does not match}
    \begin{itemize}
        \item Verifies that no comment is added when the document hash in the workspace data does not match the hash of the document, ensuring that the editor's state remains consistent with the expected data.
    \end{itemize}

    \item \textbf{Does not add comment for multi-line selections}
    \begin{itemize}
        \item Tests that no comment is added when there is a multi-line selection, ensuring that only single-line selections are processed.
    \end{itemize}

    \item \textbf{Does not add comment when no smells exist at line}
    \begin{itemize}
        \item Ensures that no comment is added when no smells are associated with the selected line, preventing unnecessary decorations from being applied.
    \end{itemize}

    \item \textbf{Displays single smell comment without count}
    \begin{itemize}
        \item Verifies that a single smell is displayed correctly without a count, confirming that the decoration is applied with the correct content format.
    \end{itemize}

    \item \textbf{Adds a single-line comment if a smell is found}
    \begin{itemize}
        \item Confirms that a single-line comment is added correctly when a smell is found on the selected line, ensuring proper decoration application.
    \end{itemize}

    \item \textbf{Displays a combined comment if multiple smells exist}
    \begin{itemize}
        \item Verifies that a combined comment is displayed when multiple smells exist on the same line.
        \item Ensures that the decoration is created with the correct formatting and applied to the correct range.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/ui/lineSelection.test.ts}{here}.

\subsubsection{Handle Smells Settings}

\textbf{Goal:} The VS Code settings management module enables users to customize detection settings, update enabled smells, and ensure workspace consistency. This module integrates with the IDE to provide real-time updates when settings change. The following unit tests validate the correctness of this functionality.\\

\noindent The tests ensure that enabled smells are correctly retrieved from user configurations, updates to settings trigger the appropriate notifications, and changes are accurately reflected in the workspace. Additional test cases verify proper handling of missing configurations, format conversions, and cache clearing operations. Edge cases, such as unchanged settings and invalid inputs, are also considered.\\

\noindent \textbf{Target requirement(s):} FR10, UHR-PSI1, UHR-EOU2, OER-IAS1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Retrieving Enabled Smells}
    \begin{itemize}
        \item Ensures that the function retrieves all enabled smells from the user's VS Code settings.
        \item Validates that the function returns an empty object when no smells are enabled.
    \end{itemize}

    \item \textbf{Handling Updates to Smell Filters}
    \begin{itemize}
        \item Ensures that enabling a smell triggers a notification to the user.
        \item Confirms that disabling a smell results in a proper update message.
        \item Validates that when no changes occur, no unnecessary cache updates are performed.
    \end{itemize}

    \item \textbf{Clearing Cache on Settings Update}
    \begin{itemize}
        \item Ensures that enabling or disabling a smell triggers a workspace cache wipe.
        \item Confirms that cache clearing only occurs when actual changes are made.
    \end{itemize}

    \item \textbf{Formatting Smell Names}
    \begin{itemize}
        \item Ensures that smell names stored in kebab-case are correctly formatted into a readable format.
        \item Verifies that empty input results in an empty string without errors.
    \end{itemize}

    \item \textbf{Ensuring User-Friendly Notifications}
    \begin{itemize}
        \item Confirms that updates to smell settings provide clear, informative messages.
        \item Ensures that error handling follows polite and constructive messaging principles.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/utils/handleSmellSettings.test.ts}{here}.

\subsubsection{Wipe Workspace Cache}

\textbf{Goal:} The "Wipe Workspace Cache" command is responsible for clearing specific caches related to the workspace in a development environment, primarily to reset the state of stored data such as "smells" and file changes. It can be triggered for different reasons, which determine which caches are cleared and how the command behaves. It also updates file hashes for visible editors when appropriate. Upon successful execution, a corresponding success message is shown to the user. In case of an error, an error message is displayed to the user.\\

\noindent The tests ensure that appropriate caches are being cleared as and when instructed.\\

\noindent \textbf{Target requirement(s):} FR3, FR5, FR8~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Wipe Cache with No Reason Provided}
    \begin{itemize}
        \item Only the "smells" cache is cleared when no reason is provided.
        \item A success message indicating the workspace cache has been successfully wiped is displayed.
    \end{itemize}

    \item \textbf{Wipe Cache with Reason "manual"}
    \begin{itemize}
        \item Both the "smells" and "file changes" caches are cleared when the reason is "manual."
        \item A success message indicating the workspace cache was manually wiped is shown.
    \end{itemize}

    \item \textbf{Wipe Cache When No Files Are Open}
    \begin{itemize}
        \item The command correctly handles the case when there are no open files.
        \item A log message is generated indicating that no open files are available to update.
    \end{itemize}

    \item \textbf{Wipe Cache with Open Files}
    \begin{itemize}
        \item When there are open files, a message indicating the number of visible files is logged.
        \item The hashes for each open file are updated as expected.
    \end{itemize}

    \item \textbf{Wipe Cache with Reason "settings"}
    \begin{itemize}
        \item Only the "smells" cache is cleared when the reason is "settings."
        \item A success message is shown indicating the cache was wiped due to changes in smell detection settings.
    \end{itemize}

    \item \textbf{Wipe Cache When an Error Occurs}
    \begin{itemize}
        \item An error message is logged when an error occurs during the cache wipe process.
        \item a user-facing error message is displayed, indicating the failure to wipe the workspace cache.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/commands/wipeWorkCache.test.ts}{here}.

\subsubsection{Backend}

\textbf{Goal:} The backend handles interactions with the backend server for tasks such as checking server status, initializing logs, fetching detected smells, and refactoring those smells in the code.\\

\noindentThe tests ensure that the system correctly interacts with the backend to check the server's status, initialize logging, fetch code smells, and perform refactoring tasks. These tests confirm that the server status is accurately updated based on successful or failed responses, that log initialization behaves as expected under different conditions, and that the system correctly handles and processes detected smells for refactoring.\\


\noindent \textbf{Target requirement(s):} FR6, PR-SCR1, PR-RFT1, PR-RFT2~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Handle Server Status Check with Successful Response}
    \begin{itemize}
        \item When the server responds successfully, the status is set to \texttt{ServerStatusType.UP}.
        \item The correct update of server status to indicate that the server is operational.
    \end{itemize}

    \item \textbf{Handle Server Status Check with Error or Failure}
    \begin{itemize}
        \item When the server responds with an error or fails to respond, the status is set to \texttt{ServerStatusType.DOWN}.
        \item Correctly handles a failed server response by ensuring the status reflects the server's downtime.
    \end{itemize}

    \item \textbf{Handle Initiation of Logs with Valid Directory Path (Success)}
    \begin{itemize}
        \item When a valid directory path is provided and the backend responds successfully, the function returns \texttt{true}, indicating successful log initialization.
        \item System can successfully initialize logs with the backend and sync them.
    \end{itemize}

    \item \textbf{Handle Initiation of Logs with Valid Directory Path (Failure)}
    \begin{itemize}
        \item When the backend fails to initialize logs, the function returns false.
        \item Properly handles to provide feedback if the log initialization process fails.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--sco-vs-code-plugin/blob/plugin-multi-file/test/api/backend.test.ts}{here}.

\subsection{API Routes}

\subsubsection{Smell Detection Endpoint}

\textbf{Goal:} The smell detection endpoint provides an API for retrieving detected code smells from the backend. It ensures efficient communication with the smell detection module while handling errors gracefully. The following unit tests verify the accuracy of this functionality.\\

\noindent The tests assess the correctness of the endpoint’s response structure, error handling for missing files, validation of request data, and handling of internal exceptions. Edge cases, such as malformed requests and empty responses, are also considered.\\

\noindent\textbf{Target requirement(s):} FR10, OER-IAS1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Successful Detection of Smells}
    \begin{itemize}
        \item The API endpoint returns a successful response when the file exists and smells are detected.
        \item The response contains the correct number of detected smells and adheres to the expected data structure.
    \end{itemize}

    \item \textbf{Handling of File Not Found Errors}
    \begin{itemize}
        \item The API endpoint returns an appropriate error response when the specified file does not exist.
        \item The error message clearly indicates that the file was not found.
    \end{itemize}

    \item \textbf{Handling of Internal Server Errors}
    \begin{itemize}
        \item The API endpoint returns an error response when an unexpected exception occurs during smell detection.
        \item The error message indicates an internal server error.
    \end{itemize}

    \item \textbf{Validation of Input Data}
    \begin{itemize}
        \item The API validates the presence and correctness of required fields in the request.
        \item The API rejects invalid input with appropriate error messages.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/api/test_detect_route.py}{here}.

\subsubsection{Refactoring Endpoint}

\textbf{Goal:} The refactoring endpoint provides an API for refactoring detected code smells by leveraging the backend refactoring module. It ensures that refactored code is returned efficiently while verifying energy savings. The following unit tests validate the accuracy of this functionality.\\

\noindent The tests assess the correctness of refactored output, proper retrieval of energy measurements, error handling for missing source directories, and graceful failures when unexpected conditions arise. Edge cases such as unsuccessful refactorings and unchanged energy consumption are also considered.\\

\noindent\textbf{Target requirement(s):} FR10, OER-IAS1~\cite{SRS} \\

\begin{itemize}
    \item \textbf{Successful Refactoring Process}
    \begin{itemize}
        \item The API endpoint returns a successful response when the refactoring process completes without errors.
        \item The response includes the refactored data and updated smells.
        \item Energy measurements are correctly retrieved and compared to ensure energy savings.
    \end{itemize}

    \item \textbf{Handling of Source Directory Not Found}
    \begin{itemize}
        \item The API endpoint returns an appropriate error response when the specified source directory does not exist.
        \item The error message clearly indicates that the directory was not found.
    \end{itemize}

    \item \textbf{Handling of Energy Measurement Failures}
    \begin{itemize}
        \item The API endpoint returns an error response when initial or final energy measurements cannot be retrieved.
        \item The API endpoint returns an error response when no energy savings are detected after refactoring.
    \end{itemize}

    \item \textbf{Handling of Unexpected Errors}
    \begin{itemize}
        \item The API endpoint returns an error response when an unexpected exception occurs during the refactoring process.
        \item The error message includes details about the exception.
    \end{itemize}
\end{itemize}

\noindent The test cases for this module can be found \href{https://github.com/ssm-lab/capstone--source-code-optimizer/blob/new-poc/tests/api/test_refactoring.py}{here}.


% \subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}



% \subsection{Tests for Functional Requirements}

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \subsubsection{Module 1}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}

% \begin{enumerate}

% \item \textbf{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% \textbf{Initial State:} 
					
% \textbf{Input:} 
					
% \textbf{Output:} \wss{The expected result for the given inputs}

% \textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}

% \textbf{How test will be performed:} 
					
% \item \textbf{test-id2\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% \textbf{Initial State:} 
					
% \textbf{Input:} 
					
% \textbf{Output:} \wss{The expected result for the given inputs}

% \textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}

% \textbf{How test will be performed:} 

% \item \textbf{...\\}
    
% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Tests for Nonfunctional Requirements}

% \wss{If there is a module that needs to be independently assessed for
%   performance, those test cases can go here.  In some projects, planning for
%   nonfunctional tests of units will not be that relevant.}

% \wss{These tests may involve collecting performance data from previously
%   mentioned functional tests.}

% \subsubsection{Module 1}
		
% \begin{enumerate}

% \item \textbf{test-id1\\}

% Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
%   be automatic}
					
% \textbf{Initial State:} 
					
% Input/Condition: 
					
% Output/Result: 
					
% \textbf{How test will be performed:} 
					
% \item \textbf{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% \textbf{Initial State:} 
					
% \textbf{Input:} 
					
% Output: 
					
% \textbf{How test will be performed:} 

% \end{enumerate}

% \subsubsection{Module 2}

% ...

% \subsection{Traceability Between Test Cases and Modules}

% \wss{Provide evidence that all of the modules have been considered.}


\bibliography{../../refs/References}

\newpage

\begin{appendices}

\section{Appendix}

\wss{This is where you can place additional information.}

\subsection{Symbolic Parameters}

Not applicable at the moment.

\subsection{Usability Survey Questions} \label{A.2}

See the surveys folder under \texttt{docs/Extras/UsabilityTesting}.

\newpage{}
\section{Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\subsubsection*{Mya Hussain}
\begin{itemize}
  \item \textit{What went well while writing this deliverable?} \\ 
  
  Writing functional tests for the capstone project went surprisingly 
  smoothly. I found that having a clear understanding of the project's 
  requirements and functionalities made it easier to structure my tests 
  logically. The existing documentation provided a solid foundation, 
  allowing me to focus on creating relevant scenarios without needing 
  extensive revisions.Overall, I felt a sense of accomplishment as I was able to
  write robust tests that will contribute to the project's 
  success.

  \item \textit{What pain points did you experience during this deliverable, and how did you resolve them?}\\ 
  
  One challenge I faced was ensuring that each test was precise and 
  effectively communicated its purpose. At times, I found myself 
  overthinking the wording or structure, which slowed me down. To 
  tackle this, I started breaking down each test into simple components, 
  focusing on the core functionality rather than getting lost in the 
  details. I also struggled with organizing the tests logically to 
  create a seamless flow in the documentation. I resolved this by 
  grouping tests thematically, which made it easier to follow. Despite 
  the frustrations, I learned to embrace the process and appreciate the 
  importance of thorough documentation in building a robust project. 
  Balancing this deliverable and the POC was also challenging as there 
  wasnt much turnaroud time between the two and we hadn't coded anything 
  previously.

\end{itemize}

\subsubsection*{Sevhena Walker}
\begin{itemize}
  \item \textit{What went well while writing this deliverable?} \\
  
  I was responsible for writing system tests for the projects non-functional requirements and I found the process to be very useful for gaining a deep understanding of all the qualities a system should have. When you write a requirement, obviously, there is some thought put into it, but actually writing out the test really sheds light on all the facets that go into that requirement. I feel like I have even more to contribute to my team after this deliverable.

  \item \textit{What pain points did you experience during this deliverable, and how did you resolve them?}\\
  
  Writing out all those tests was extremely long, and I found myself re-writing tests more than once while pondering on the best way to test the requirements. Some tests needed to be combined due to a near identical testing process and some needed more depth. I also sometimes struggled with determining if some testing could even be feasibly done with our team's resources. To resolve this I held a discussion or 2 with my team so that we could have more brains working on the matter and to ensure that I wasn't making some important decisions unilaterally.
\end{itemize}

\subsubsection*{Nivetha Kuruparan}
\begin{itemize}
  \item \textit{What went well while writing this deliverable?} \\
  
    Working on the Verification and Validation (VnV) plan for the Source Code Optimizer project was a pretty smooth experience overall. I really enjoyed defining the roles in section 3.1, which helped clarify what everyone was responsible for. This not only made the team feel more involved but also kept us on track with our testing strategies.
    
    Diving into the Design Verification Plan in section 3.3 was another highlight for me. It helped me get a better grasp of the requirements from the SRS. I felt more confident knowing we had a solid verification approach that covered all the bases, including functional and non-functional requirements. The discussions about incorporating static verification techniques and the importance of regular peer reviews were eye-opening and really enhanced our strategy for maintaining code quality.

  \item \textit{What pain points did you experience during this deliverable, and how did you resolve them?}\\

  I struggled a bit with figuring out how to effectively integrate feedback mechanisms into our VnV plan. It was tough to think through how to keep the feedback loop going throughout development. I tackled this by setting up a clear process for documenting feedback during our code reviews and testing phases, which I included in section 3.4. This not only improved our documentation but also helped us stay committed to continuously improving as we moved forward.
  
\end{itemize}

\subsubsection*{Ayushi Amin}
\begin{itemize}
  \item \textit{What went well while writing this deliverable?} \\ 

  Writing this deliverable was a really crucial part of the process. It 
  helped me see the bigger picture of how we’re going to ensure everything 
  in the SRS gets tested properly. What went well was the clarity that came 
  from laying out the plan step by step. Even though we haven’t put it into 
  action yet, just knowing we have a solid structure in place gives me confidence.
  
  Another highlight was sharing our completed sections with Dr. Istvan. It 
  was great to get his feedback and know that he appreciated the level of 
  detail we included. Having that validation made me feel like we’re on the 
  right track. It also reminded me how important it is to be thorough from 
  the start, so we’re not scrambling later when we’re deep into testing.
  Having all the requiremnts and test cases mapped out helps me stress less
  as I know have an idea of what the proejct will look like and have these 
  documents top guide the process in case we get stuck or forget something.

  \item \textit{What pain points did you experience during this deliverable, and how did you resolve them?}\\ 
  
  One of the challenges was trying to anticipate potential gaps or issues 
  in our testing process while still being in the planning phase. Thinking 
  through how to cover both functional and non-functional requirements in
  the SRS in a comprehensive yet practical way was tricky. We resolved this
  by deciding to create a traceability matrix, which will help us ensure that 
  every requirement is accounted for once we move into the testing phase. Even 
  though the matrix isn’t done yet, just planning to use it gives a sense 
  of structure. 

  Another tough spot was figuring out how to handle usability and performance 
  testing in a way that doesn’t feel overly theoretical. Since we’re not at the 
  implementation stage, it’s hard to gauge what users will really need. To work 
  through this, I focused on drawing from what we know about our end-users and 
  aligning our plan with the goals outlined in the SRS. Keeping that user-centered 
  perspective helped ground the plan, making it feel more actionable even at this 
  early stage.
\end{itemize}

\subsubsection*{Tanveer Brar}
\begin{itemize}
    \item \textit{What went well while writing this deliverable?} \\

    Clearly pointing out the tools to use for various aspects of Automated Validation and Testing(such as unit test framework, linter) has created a well-defined plan for this verification. Now the project has a structured approach to validation. Knowing the tools before implementation will allow both code quality enforcement and the gathering of coverage metrics. For the Software Validation Plan, external data source(open source Python code bases for testing) has added confidence that the validation approach would align closely with real world scenarios.

    \item \textit{What pain points did you experience during this deliverable, and how did you resolve them?}\\

    One of the challenges was ensuring compatibility between different tools for automated testing and validation plan. For example, code coverage tool needs to be supported by the unit testing framework. To resolve this, I conducted research on all validation tools, to choose the ones that fit into the project's needs while being compatible with each other.

\end{itemize}

\subsubsection*{Group Reflection}
\begin{itemize}
  \item \textit{What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.\\} 
  
  Sevhena will need to deepen her understanding of test coordination and project 
  tracking using GitHub Issues. She’ll focus on creating detailed issue templates 
  for various testing stages, managing the workflow through Kanban boards, and using 
  labels and milestones effectively to track progress. Additionally, mastering test 
  case documentation and ensuring efficient communication through GitHub’s discussion 
  and comment features will be critical.

  Mya will enhance her skills in functional testing by learning to write comprehensive 
  test cases directly linked to GitHub Issues. She will leverage GitHub Actions to 
  automate repetitive functional tests and integrate them into the development workflow. 
  Familiarity with continuous integration pipelines and how they relate to functional 
  testing will help her verify that all functional requirements are met consistently.

  Ayushi will focus on integration testing by ensuring that the Python package, VSCode 
  plugin, and GitHub Action work together seamlessly. She’ll develop expertise in using 
  PyJoules to assess energy efficiency during integration tests and learn to create 
  automated workflows via GitHub Actions. Ensuring smooth integration of PyTorch models 
  and maintaining consistent coding standards with Pylint will be essential. She’ll 
  also manage dependencies and coordinate with the team using GitHub’s multi-repository 
  capabilities.

  Tanveer will deepen her knowledge of performance testing using PyJoules to monitor 
  and optimize energy consumption. She will also need to develop skills in security 
  testing, ensuring that the Python code adheres to best security practices, possibly 
  integrating tools like Bandit along with Pylint for static code analysis. Setting 
  up and maintaining performance benchmarks using GitHub Issues will ensure transparency 
  and continuous improvement.

  Nivetha will enhance her skills in usability and user experience testing, particularly 
  in evaluating the intuitiveness of the VSCode plugin interface. She will focus on 
  collecting and analyzing user feedback, linking it to GitHub Issues to drive interface 
  improvements. Documenting user experience testing and ensuring that the product’s UI 
  meets user expectations will be a significant part of her role. Using Pylint to maintain 
  consistent code quality in user-facing components will also be essential.

  Istvan will provide oversight by monitoring the team’s progress, using GitHub Insights 
  to ensure that testing processes meet industry standards. He will guide the team in 
  integrating PyJoules, Pylint, and PyTorch effectively into the V\&V workflow, offering 
  feedback and ensuring alignment with project goals.

  All group members will have to learn how to use pytests to perform test cases in this
  entire project.

  \item \textit{For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?\\} 
  

    \textbf{Sevhena Walker (Lead Tester)}
    \begin{itemize}
        \item \textbf{Knowledge Areas:} Test coordination, PyJoules, GitHub Actions, Pylint.
        \item \textbf{Approaches:}
        \begin{itemize}
            \item Online Courses and Tutorials: Enroll in courses focused on test automation, PyJoules, and GitHub Actions.
            \item Hands-on Practice: Apply knowledge directly by setting up test cases and automation workflows in the project.
        \end{itemize}
        \item \textbf{Preferred Approach:} Hands-on Practice
        \item \textbf{Reason:} This approach allows her to see immediate results and iterate quickly, building confidence in her coordination and automation skills.
    \end{itemize}

    \textbf{Mya Hussain (Functional Requirements Tester)}
    \begin{itemize}
        \item \textbf{Knowledge Areas:} PyTorch, functional testing, GitHub Actions, Pylint.
        \item \textbf{Approaches:}
        \begin{itemize}
            \item Technical Documentation and Community Forums: Study PyTorch documentation and participate in forums like Stack Overflow.
            \item Mentorship and Collaboration: Pair with experienced team members or mentors to get guidance and feedback on functional testing practices.
        \end{itemize}
        \item \textbf{Preferred Approach:} Technical Documentation and Community Forums
        \item \textbf{Reason:} It allows her to explore topics deeply and find solutions to specific issues, promoting self-sufficiency.
    \end{itemize}

    \textbf{Ayushi Amin (Integration Tester)}
    \begin{itemize}
        \item \textbf{Knowledge Areas:} PyJoules, integration testing, PyTorch, GitHub Actions.
        \item \textbf{Approaches:}
        \begin{itemize}
            \item Workshops and Webinars: Attend live or recorded sessions focused on energy-efficient software development and integration testing techniques.
            \item Project-Based Learning: Directly work on integrating components and iteratively improving based on project needs.
        \end{itemize}
        \item \textbf{Preferred Approach:} Project-Based Learning
        \item \textbf{Reason:} It aligns with her role's focus on real-world integration, providing relevant experience and immediate feedback.
    \end{itemize}

    \textbf{Tanveer Brar (Non-Functional Requirements Tester - Performance/Security)}
    \begin{itemize}
        \item \textbf{Knowledge Areas:} Performance testing with PyJoules, security testing, Pylint.
        \item \textbf{Approaches:}
        \begin{itemize}
            \item Specialized Training Programs: Join programs or bootcamps that focus on performance and security testing.
            \item Peer Learning: Collaborate with team members and participate in knowledge-sharing sessions.
        \end{itemize}
        \item \textbf{Preferred Approach:} Peer Learning
        \item \textbf{Reason:} It promotes team synergy and allows him to gain practical insights from those working on similar tasks.
    \end{itemize}

    \textbf{Nivetha Kuruparan (Non-Functional Requirements Tester - Usability/UI)}
    \begin{itemize}
        \item \textbf{Knowledge Areas:} Usability testing, user experience, GitHub Issues, Pylint.
        \item \textbf{Approaches:}
        \begin{itemize}
            \item User Feedback Analysis: Conduct regular user testing sessions and analyze feedback.
            \item Online UX/UI Design Courses: Enroll in courses that focus on usability principles and user experience design.
        \end{itemize}
        \item \textbf{Preferred Approach:} User Feedback Analysis
        \item \textbf{Reason:} This approach provides real-world insights into how the product is perceived and used, making adjustments more relevant.
    \end{itemize}

    \textbf{Istvan David (Supervisor)}
    \begin{itemize}
        \item \textbf{Knowledge Areas:} Supervising V\&V processes, providing feedback, ensuring industry standards.
        \item \textbf{Approaches:}
        \begin{itemize}
            \item Industry Conferences and Seminars: Attend events focused on software verification and validation trends.
            \item Continuous Professional Development: Engage in regular self-study and professional development activities.
        \end{itemize}
        \item \textbf{Preferred Approach:} Continuous Professional Development
        \item \textbf{Reason:} This method allows for a consistent update of skills and knowledge aligned with evolving industry standards.
    \end{itemize}

\end{itemize}

\end{appendices}

\end{document}
